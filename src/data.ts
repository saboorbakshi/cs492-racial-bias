export const interviewees = [
  {
    name: 'Jennifer Aguiar',
    role: 'Clinical Bioinformatician',
    src: 'jennifer-aguiar.jpeg',
    company: 'The Hospital for Sick Children',
    companySrc: 'the-hospital-for-sick-kids.jpeg',
    youtubeId: '2Fmyextb2Qg'
  },
  {
    name: 'Pedro Ballester',
    role: 'Machine Learning Specialist',
    src: 'pedro-ballester.jpeg',
    company: 'The Hospital for Sick Children',
    companySrc: 'the-hospital-for-sick-kids.jpeg',
    youtubeId: 'Up5Bgq2Vrxs'
  },
  {
    name: 'Scott Davidson',
    role: 'Senior Bioinformatician',
    src: 'scott-davidson.jpeg',
    company: 'The Hospital for Sick Children',
    companySrc: 'the-hospital-for-sick-kids.jpeg',
    youtubeId: 'CFe6isPF6ao'
  },
  {
    name: 'Dan Holtby',
    role: 'Computer Science Professor',
    src: 'dan-holtby.png',
    company: 'University of Waterloo',
    companySrc: 'university-of-waterloo.png',
    youtubeId: '0NFZOjxsusA'
  }
]

export const caseStudies = [
  {
    title: 'Transgender Patient’s Emergency Misdiagnosed by Algorithmic Bias',
    summary: [
      'A 32-year-old transgender man arrived at the emergency room with severe abdominal pain. Although he informed staff that he was transgender, the hospital’s electronic health record (EHR) system listed him as male (Ring, 2019). As a result, clinicians failed to consider pregnancy and misattributed his symptoms to factors like obesity. The patient was, in fact, pregnant and experiencing labour complications. Due to the delay in recognizing this, urgent care was postponed, and tragically, the baby was stillborn (Ring, 2019).',
      'This case highlights how rigid, binary classifications in health records — combined with provider assumptions — can lead to critical misdiagnoses. Standard pregnancy-related alerts were never triggered because the system registered the patient as male. This highlights the dangers of algorithmic bias and the need for more inclusive healthcare systems that reflect the realities of transgender and non-binary individuals in today’s day and age (Compton, 2019; Cirillo et al., 2020).'
    ],
    questions: [
      'Imagine you’re a physician treating a patient with severe abdominal pain and no visible injuries. What diagnoses would immediately come to mind, and what additional information would you ask for?',
      'What if you were told the patient was a woman?',
      'How might your diagnostic thinking change if you were told the patient was a man?',
      'How would learning that the patient is a transgender man affect your clinical reasoning, and what challenges might arise in ensuring they receive appropriate care?',
      'In your opinion, now knowing that the person was transgender and pregnant, what role did algorithmic bias, system design, or any other factors that you can think of, play in the AI’s failure to identify pregnancy risk in this case?',
      'What changes would you suggest — to electronic health records, triage systems, provider training, etc. — to prevent similar outcomes for transgender patients in the future?'
    ],
    discussion: [
      'One of the main issues that stood out in all the interviews conducted was how social stereotypes regarding gender shaped clinical reasoning. Most participants who were asked how they would manage a patient who has abdominal pain acknowledged that their diagnostic reasoning would differ depending on the patient’s gender or their perceived gender. As an example, Jennifer Aguiar explained how a female patient would require her to think of reproductive health problems like ovarian cysts or pregnancy in her differential list. However, she, Dan Holtby, and Scott Davidson noted that if the patient was considered a male, they would all disproportionately focus on gastrointestinal or urinary pathology. This does demonstrate that there is gender clinical heuristic bias that can result in cognitive error, particularly concerning transgender patients whose anatomy does not correspond to gender-based assumptions.',
      'All participants advocated for the need to capture gender identity as well as the sex designated at birth in health records. Aguiar pointed out that although there is a need to honour gender identity, critical clinically relevant information, such as a uterus or ovaries. should be freely available for use in clinical decision-making. Ballester specifically argued for the need to differentiate these two constructs in electronic health record (EHR) systems as more hospitals adopt artificial intelligence-driven alerts and triage systems. If a system only recognizes gender identity and not biological anatomy, crucial alerts—such as those related to pregnancy—can be missed, with life-threatening consequences.',
      'Algorithmic bias was another concern shared across the interviews. Aguiar invoked the “garbage in, garbage out” principle, stressing that if transgender individuals are not adequately represented in the datasets that train AI models, the resulting tools will fail to serve them. Ballester agreed and advocated for designing more sophisticated data pipelines that take into account variables like hormone therapy or surgical history. Without such nuance, AI systems risk reinforcing existing biases and becoming blind to outliers.',
      'Scott Davidson offered a slightly different perspective, attributing the failure less to systems and more to human error. He noted that in some cases, even when EHRs are limited, the necessary information is verbally communicated by the patient but not acted upon. This highlights the need for better training so that healthcare providers can interpret and apply contextual information more effectively.',
      'Ultimately, the interviews point to four clear action items: EHRs should include both sex assigned at birth and gender identity, AI systems must be trained on inclusive data, clinical education should prioritize transgender healthcare, and empathy must remain at the core of every patient interaction. Only through this multidimensional approach can healthcare become truly equitable.'
    ]
  },
  {
    title: 'Symptom Checker Underestimates a Woman’s Heart Attack',
    summary: [
      'An AI-powered symptom-checker app provided dangerously different recommendations for a man and a woman with identical symptoms. Both were 59-year-old smokers reporting sudden chest pain and nausea. The only variable was gender. The male patient was warned of potential heart issues, including unstable angina or a heart attack, and advised to seek emergency care. The female patient, however, was told her symptoms might be due to depression or anxiety, with no urgent care recommended (Trendall, 2024).',
      'This discrepancy highlights how AI tools can replicate and amplify gender bias, potentially leading to delayed treatment or misdiagnosis (Joshi, 2024). In this case, the AI appeared to rely on statistical trends that underrepresented heart attacks in women—overlooking the real and serious risk of cardiac events in female patients (Cirillo et al., 2020). Public concern grew after this example came to light, as it revealed how gender stereotypes, like “women are anxious and men have heart attacks”, can shape algorithmic decisions (Trendall, 2024).',
      'Ultimately, the case highlights the critical importance of designing healthcare AI systems that recognize and adjust for bias. Life-threatening conditions must be considered for all patients—regardless of gender—especially when symptoms are shared.'
    ],
    questions: [
      'Imagine a 59-year-old patient presents with sudden chest pain and nausea. Without knowing their gender, what would be your top differential diagnoses, and what would your next steps be?',
      'Would you take the problem seriously right away, or would you think that chest pain for a 59-year-old is likely not serious?',
      "Now imagine learning the patient is a woman, and you're told that statistically, women are less likely to have a heart attack. Would this influence your decisions? Should it?",
      'If the patient was a man, would your sense of urgency change, keeping in mind that statistically, men are more likely to have a heart attack?',
      'So overall, would your perspective and seriousness stay the same if they were a 59-year-old with chest pain regardless of their gender?',
      'Even if the statistical risk is lower in women, how would you balance data like this with the need to consider life-threatening conditions like heart attacks?',
      'How should an AI triage system handle gender-based statistical differences? Should it present all serious possibilities regardless of probability, especially in potentially fatal scenarios?',
      'Was the AI’s response defensible, given it followed statistical trends, or should its priority have been patient safety over data-driven averages? Where’s the line between data and ethical care? Do you believe it failed in its duty to provide safe recommendations?'
    ],
    discussion: [
      "Throughout all of the interviews, there was the shared belief that chest pain and nausea in a 59-year-old should always be treated seriously, regardless of gender. All four experts immediately suspected a cardiac issue based on the symptoms presented, indicating a strong baseline understanding that aligns with safe clinical practice. This uniformity contrasts sharply with the AI's gendered recommendations in the case study, highlighting a key flaw in the AI’s decision-making: its overreliance on statistical generalizations rather than patient-centered reasoning.",
      'Pedro Ballester and Jennifer Aguiar most clearly articulated the limitations of using population-level statistics to guide decisions for individual patients. Ballester noted that while gender-based prevalence may guide public health policy, applying such trends to individual care often results in error. Aguiar echoed this, stating that "an individual is not a statistic" and emphasizing the ethical obligation to consider rare but serious conditions like heart attacks in women. Both experts advocated for a shift in AI systems from probability-driven outputs toward risk-based frameworks that prioritize patient safety, particularly in potentially fatal scenarios.',
      'Meanwhile, Dan Holtby and Scott Davidson emphasized the importance of context in AI usage. Holtby argued that AI should always err on the side of caution, especially in public-facing tools, mentioning that "it should say this could be serious, so you should talk to a doctor." Davidson also stressed that AI’s role should be determined by its end-user. For doctors, comprehensive diagnostic possibilities make sense, whereas for patients, such tools must tread carefully to avoid both false reassurance and unwarranted panic.',
      'A consensus emerges around the need for transparency and breadth in AI outputs. All four experts favoured a model where AI systems list multiple diagnostic possibilities, clearly indicating their likelihood and severity, rather than fixating on the statistically most probable condition. Most importantly, they agreed that potentially fatal outcomes must never be excluded, even if they are statistically less common.',
      'The experts collectively rejected the AI’s actions in this case as ethically flawed. While they acknowledged that the AI was technically consistent with its training data, they all argued that the model ultimately failed in its duty to provide safe and equitable care. As Pedro Ballester concluded, "if we know it\'s wrong, you just need to fix it."',
      'These interviews powerfully depict that statistical data must be used carefully in AI healthcare systems. In life-or-death situations, algorithms must prioritize safety, fairness, and transparency over blind adherence to historical trends. To earn trust and ensure ethical care, AI systems must be designed not just to reflect medical data, but to actively challenge its biases.'
    ]
  },
  {
    title: 'Male Breast Cancer Patient Denied Treatment by Gendered Algorithm',
    summary: [
      'Raymond Johnson, a 26-year-old man from South Carolina, was denied Medicaid coverage for breast cancer treatment solely because of his gender. Although diagnosed with breast cancer, he was ruled ineligible for a federal Medicaid program that covered patients screened through certain government programs — programs that only screened women. As a result, the insurance algorithm automatically excluded men, leaving Raymond without coverage for the chemotherapy and surgery he urgently needed (Park, 2022).',
      'This case exemplifies how gender bias embedded in policy and algorithmic systems can have life-threatening consequences. The program failed to account for the fact that men can also develop breast cancer, effectively barring male patients from life-saving care (Joshi, 2024). Advocacy groups, including the ACLU, condemned the exclusion as discriminatory and pushed for reform (Park, 2022). The case prompted broader conversations about the need to design healthcare systems and policies that are inclusive of all genders and reflective of real patient populations.'
    ],
    questions: [
      'When a young patient (let’s say 25 years old) presents with a lump in their chest, what key factors guide your differential diagnosis and decision to recommend further testing?',
      'How might your diagnostic approach change based on if the patient is female? What would your decision lean more towards in terms of diagnosis?',
      "If they were male, would you consider the same things as women's chest pain or would your approach be quite different for male chest pain?",
      'Breast cancer is rare in men, less than 1% of all breast cancers occur in men, should that rarity affect your decision to test for it? How do you balance the risk of over-testing versus missing a critical diagnosis?',
      'What role do you think societal assumptions — like breast cancer being a “women’s disease” — played in the algorithm’s decision to deny coverage to Raymond?',
      'What changes would you suggest to healthcare algorithms or policies to prevent gender-based denial of coverage in similar cases? How can we ensure equal access to treatment for all?',
      'Should healthcare algorithms prioritize statistical likelihoods and trends or be designed to account for rare but serious conditions — especially when lives are at stake? How should they strike that balance?'
    ],
    discussion: [
      'Across the board, all interviewees acknowledged that even though male breast cancer is rare, less than 1% of all breast cancer cases, it should not be ruled out during diagnostic processes. Jennifer Aguiar emphasized the importance of initial imaging for any visible lump, regardless of gender, even though breast cancer would not be her first suspicion for a male patient. Her reasoning prioritized the visibility and seriousness of symptoms over statistical rarity, suggesting that algorithms, too, should adapt to this mindset. Scott Davidson shared a similar sentiment, asserting that if a patient presents a legitimate complaint, such as a lump, it must be taken seriously irrespective of gender. This reflects a broader principle of equitable care: statistical outliers may still represent real, life-threatening cases.',
      'Dan Holtby, while not a clinician, highlighted the potential role of Bayesian reasoning in diagnosis, supporting the idea of ruling out common conditions first. However, he strongly advocated that algorithms should never make final healthcare decisions. According to him, any automated decision must be subject to human review by a qualified doctor. This perspective aligns with broader ethical concerns around AI in healthcare: automation should support, not replace, clinical judgment, especially in life-or-death situations.',
      'Pedro Ballester provided a particularly insightful critique from a machine learning standpoint. He pointed out that statistical averages, while helpful at a population level, often fail when applied to individuals. He argued that algorithms must not be overly reliant on population-level data, and instead must be designed to consider edge cases. His comments speak directly to the core failure in Raymond’s case: the reliance on population trends to deny an individual’s valid, clinically diagnosed condition.',
      'A recurring theme in all responses was the importance of clinical diagnosis over automated judgment. Each interviewee advocated for healthcare systems to value the medical expertise of clinicians above algorithmic output. For instance, Jennifer suggested that algorithms should place higher weight on the input of trained professionals, especially when a definitive diagnosis is already made. Scott Davidson emphasized that algorithms only follow the logic embedded in them, so logic that is flawed or biased, reflects the shortcomings of their human creators. He said that this emphasizes the importance of real-time auditing and the inclusion of override mechanisms.',
      "Moreover, all interviewees acknowledged that societal assumptions—like breast cancer being a “women’s disease”—influenced not just public perception but the structure of algorithms and insurance policies themselves. The consequences of such assumptions are severe: exclusion from treatment, delayed care, and potentially avoidable deaths. Jennifer pointed out the flawed logic in encoding gender into decision-making when the diagnosis is already clear. Pedro and Scott both noted that systemic incentives, such as insurance companies' focus on cost reduction, often overshadow patient well-being, making such discriminatory policies harder to correct without external accountability.",
      'In conclusion, the responses from these professionals make it clear that healthcare algorithms must evolve to become both inclusive and context-aware. There is consensus that algorithms should never be the sole decision-makers in clinical settings. Instead, they should function as tools that support doctors, especially in diagnosing rare conditions. Policies must shift from a rigid interpretation of statistical norms to one that is flexible enough to accommodate the full spectrum of human health experiences. Overall, Raymond Johnson’s case is a call to action for healthcare systems to recalibrate both their technological frameworks and societal assumptions to uphold the ethical imperative of equal access to care.'
    ]
  },
  {
    title: 'Anthony Randall and Kidney Transplant Algorithm Bias',
    summary: [
      'Anthony Randall is a Black man from Los Angeles who was on dialysis, waiting for a kidney transplant for over five years (TheGrio, 2023). What he did not know is that an algorithm from the transplant system incorporated a race-based “modifier” that made Black patients’ kidney scores seem better than they were. This modifier caused Randall’s kidney disease to be less severe than it truly was, leading to his placement on the national transplant waiting list being significantly delayed (TheGrio Staff, 2023). In mid-2023, he filed a case against his hospital (Cedars Sinai Medical Center) and the United Network for Organ Sharing alleging that he was unfairly deprived of a fair chance to get the transplant because of the racially biased formula (Penn Medicine, 2024). It was no secret that the algorithm had a bias (Penn Medicine, 2024).',
      "The board of the transplant system understood the modifier was resulting in Black patients' illnesses being severely underestimated. By early 2023, all hospitals were directed to stop the usage of race adjustment and Black patients’ waiting times were to be changed to reflect the postponement (Penn Medicine, 2024). Randall claims that had these changes come sooner; he could have already had the kidney that he desperately needs. His case highlights how the goal of the clinical algorithm was good, but the execution was not due to the insertion of race which caused Black patients to not receive quality care in a timely manner."
    ],
    questions: [
      'Imagine you’re a healthcare provider evaluating a patient with kidney disease for transplant eligibility. What clinical factors would you typically consider when assessing disease severity and readiness for transplant?',
      'Have you heard of race-based modifiers in kidney function scoring (such as eGFR)? What do you think was the original intention behind including race in those calculations?',
      'Knowing now that this modifier made Black patients appear healthier than they were — delaying transplant eligibility — how do you think this affected patients like Anthony Randall?',
      'Should hospitals or national organizations be held accountable when algorithms known to be biased continue to be used? What obligations do they have once they become aware of harm?',
      'In Randall’s case, the board eventually mandated the removal of race adjustments, but it came years after the issue was known. Do you think that delay was acceptable? What should have been done differently?',
      'How can health systems design clinical algorithms that avoid reinforcing historical or systemic inequities — especially those tied to race?',
      'What steps, if any, should be taken to make things right for patients who were harmed by biased algorithms — such as adjusting wait times, issuing apologies, or offering compensation?'
    ],
    discussion: [
      'All four experts recognized that eligibility for transplants involves a combination of clinical urgency and donor compatibility. However, once the discussion shifted to the inclusion of race-based modifiers, concerns intensified. Dan Holtby highlighted that any inclusion of race must be grounded in “very sure statistics,” noting that inflating a patient’s numbers to appear healthier can be disastrously misleading. Jennifer Aguiar offered historical context, pointing to the use of spirometry in lung function assessments and how these practices are rooted in eugenics and outdated racial pseudoscience, which is very similar to this eGFR case. She strongly condemned the embedded race adjustments as “a horrible idea” and emphasized that biases often go unnoticed because they are baked into tools clinicians use unknowingly.',
      'Pedro Ballester emphasized the need for ongoing performance checks, pointing out that any algorithm, whether AI-driven or not, must be continually validated and recalibrated as demographic distributions change. He underlined the double harm done: worse medical outcomes for marginalized patients and a reinforced distrust in healthcare systems. Similarly, Scott Davidson asserted that even if algorithms begin with good intentions, the moment they are revealed to cause harm, immediate removal is necessary. He considered the continued use of such biased systems not just negligent but potentially malicious.',
      'A shared theme across all interviews is the need for accountability. Once bias is identified, healthcare institutions must act promptly. Whether through revising wait times, offering public apologies, or even compensation, restoring trust with affected communities is vital. Aguiar especially highlighted the importance of rebuilding trust with marginalized populations, arguing that it isn’t just morally necessary but scientifically beneficial to ensure inclusive data in medical research.',
      'The experts also stressed proactive strategies. Suggestions included diversifying training datasets, prospectively auditing algorithms, and ensuring transparency in how algorithms influence decisions. Davidson and Ballester emphasized testing with both historical and real-time data, while Aguiar stressed addressing systemic biases in the data we use before entrusting it to machine learning tools.',
      'Conclusively, these interviews illustrate a collective understanding that while clinical algorithms aim to improve decision-making, their misuse, particularly when embedding race without rigorous justification, can perpetuate systemic injustices. Ethical healthcare demands that such tools are scrutinized, regularly audited, and always serve equitable, patient-centered care.'
    ]
  },
  {
    title: 'Dr. Noha Aboelata and Pulse Oximeter Bias During COVID-19',
    summary: [
      'During the pandemic, race-related biases in medical technology confronted Dr. Noha Aboelata, a family physician and the Chief Executive Officer of Roots Community Health Center based in Oakland. In late 2020, one of her patients was an elderly African American gentleman who suffered from chronic lung illness (UCSF, 2022). One of the checks done previously, the pulse oxygenation check, revealed that his oxygen saturation levels were high. Even though the device showed a relatively normal oxygen level, Dr. Aboelata’s clinical instinct indicated the patient was much more distressed. She conducted an arterial blood gas test which confirmed her worst fears, the oxygen content in the patient’s blood was too low and he needed oxygen.',
      'Sometime later, she came across an article in the New England Journal of Medicine that confirmed her hunch; the oximeters were unable to register low oxygen levels in dark-skinned patients as compared to white patients, due to differences in how light is absorbed by the skin (Allen, 2024). She and her colleagues were outraged by a device that was supposed to help their patients but was grossly inaccurate for the Black population. Finally, her clinic participated in a class-action lawsuit against easier manufacturers and sellers of pulse oximeters for more detailed warnings and up-to-date devices (UCSF, 2022). She did not stand idle while demanding the FDA take pulse oximeter discrimination towards races very seriously.'
    ],
    questions: [
      "Imagine you're treating a patient with chronic lung disease during a respiratory pandemic. If a pulse oximeter shows normal oxygen saturation, but the patient appears visibly distressed, what would you do next?",
      'How much would you completely rely on tools like pulse oximeters in clinical decision-making? Do you think you would ever question or double-check the accuracy of a medical device?',
      'In this case, Dr. Aboelata’s clinical judgment overruled the device’s reading. What does this say about the limitations of relying too heavily on technology without understanding its biases?',
      'Pulse oximeters were found to overestimate oxygen levels in patients with darker skin. Why do you think this design flaw persisted for so long, despite the risk it posed to patients of colour?',
      'Do you believe the FDA and manufacturers have done enough to address this issue? What more should regulatory bodies be doing to ensure devices are accurate across diverse populations?',
      'How should future healthcare providers be trained to detect and challenge device-based bias in patient care?',
      'For communities that have been historically underserved or harmed by biased tools, how can the medical system begin to rebuild trust and ensure safer, more equitable care?'
    ],
    discussion: [
      'All four participants accentuated a critical theme: clinical judgment must take precedence over blind reliance on technology. Jennifer Aguiar passionately emphasized trusting the patient first, asserting that if someone appears distressed, that should supersede a device’s reading. She highlighted that tools can fail and are not immune to technical shortcomings or baked-in biases. Similarly, Scott Davidson affirmed that proper understanding of how a device works is crucial. His statement, “you need to know the ins and outs of the tools you use,” highlights the importance of provider education, not just in use but in function and limitations.',
      'Pedro Ballester provided a more systemic critique, pointing to the failures of regulatory bodies like the FDA, which often approve tools based on limited data. He notes that original testing often overlooks diverse populations, and ongoing evaluation of devices is rare. His suggestion to involve experts from other domains—technical fields like machine learning and engineering—in hospital decision-making reflects a proactive and cross-disciplinary solution to a deeply embedded issue.',
      'Dan Holtby recognized the racial bias but was less confident in proposing concrete actions. His acknowledgment that tools like pulse oximeters “shine light through the skin,” and therefore may be impacted by skin tone, shows an awareness of the technical root of the bias. However, his uncertainty around regulation and rebuilding trust reveals a broader issue: not all clinicians or researchers may feel equipped to challenge the systems they work within.',
      "Collectively, the responses reflect a consensus that medical devices must be critically evaluated for bias, especially those as ubiquitous as pulse oximeters. The professionals agree that testing across diverse demographics is not just necessary but urgent. Aguiar’s comment that “we’ve baked bias into technology” points to a disturbing reality, bias isn't always accidental, it's often institutionalized through omission.",
      'Rebuilding trust with historically underserved communities requires transparency, empathy, and accountability. As Scott Davidson noted, institutions must demonstrate that their tools work for everyone, not just a dominant subset. That starts with openly admitting the limitations of past tools, educating clinicians to question and investigate device outputs, and reforming regulatory standards to mandate inclusive testing.',
      'All in all, the interviewees reinforce the lesson exemplified by Dr. Aboelata’s case: technology should support, not replace human judgment, especially when lives hang in the balance.'
    ]
  },
  {
    title: 'Alex Morales and Smartwatch Blood-Oxygen Reading Bias',
    summary: [
      'Alex Morales, a New York resident, brought attention to a case of possible racial bias in the consumer health device, Apple Watch (Stempel, 2023). Morales, who has a darker complexion, bought an Apple Watch with the expectation that its blood oxygen sensor would accurately log his oxygen levels for fitness and health purposes. To his surprise, he later found out that the device’s oximeter may not work well with people from his demographic (Stempel, 2023).',
      'In late 2022, Morales initiated a class action lawsuit against Apple for allegedly containing a blood oxygen app that was racially discriminatory and did not function as promised for non-white customers. Supported in part by complaints of other studies claiming that more advanced pulse oximetry devices are “massively less useful on people with darker skin”, Morales asserted that Apple owed the public an explanation (Stempel, 2023; Nicholls, 2022). After all, paying smartphone users assumed that the device would be equal for all users, which is not the case. While the judge dismissed the case in 2023, it did start an important discussion regarding Apple products. Their case showed the world that there are, in fact, indirect biases in medical-grade equipment. This shows us that Alex Morales and the rest of the community are still subjected to discrimination based on race even in the technology they choose to use. Moreover, it demonstrates the keen eye for responsibility the tech industry has in these situations.'
    ],
    questions: [
      'If you purchase a health-monitoring device like a smartwatch, what level of accuracy and reliability would you expect — especially when it comes to tracking critical health metrics like blood oxygen levels?',
      'How would you feel if you discovered that your device didn’t work as accurately for people with your skin tone or demographic?',
      'Do you think companies like Apple have an obligation to disclose limitations in their health sensors, especially if those limitations affect certain racial or skin tone groups?',
      'Why do you think skin tone bias in oximeters and wearable health devices hasn’t received widespread attention until recently? What might be some of the barriers to addressing it?',
      'The court dismissed Morales’s case, but do you think tech companies should still be held accountable in other ways? What kind of responsibility should they carry when their products are shown to underperform for marginalized users?',
      'Should consumer tech devices with health features (like smartwatches) be subject to the same regulatory scrutiny as medical devices? Why or why not?',
      'How can the tech industry ensure that innovations in health and wellness tools are tested fairly across diverse populations? Who should be involved in that process?',
      'What does this case reveal about the intersection between technology, race, and access to accurate health information in the digital age? How can companies rebuild or maintain trust with users who may feel excluded or misled by biased product performance?'
    ],
    discussion: [
      'Across the interviews, a key theme that emerges is expectation versus reality. While professionals like Dan Holtby and Pedro Ballester expect some level of accuracy from devices like smartwatches—particularly for commonly measured health metrics such as heart rate and blood oxygen—both acknowledge that if a product fails to perform reliably across different demographics, that information must be disclosed transparently. Holtby emphasizes that companies should publish the accuracy rates and limitations of their sensors, especially if the shortcomings disproportionately affect certain groups. Ballester agrees, adding that burying such disclaimers in lengthy terms and conditions is not sufficient, essential limitations must be made visible and accessible to all consumers.',
      'In contrast, both Jennifer Aguiar and Scott Davidson express general skepticism toward the reliability of smartwatches for medical-grade monitoring. Aguiar frames these devices as offering only a ballpark estimate and asserts she would not rely on them for critical medical insight. Nonetheless, she emphasizes that if a device fails to serve a specific demographic, users deserve to know, especially if their safety or health decisions are at stake. Davidson, while similarly distrusting of smartwatch metrics, maintains a more detached view, arguing that unless a device is marketed as medical-grade, users should lower their expectations.',
      'A second major theme is representation and design bias. Both Ballester and Aguiar highlight how the lack of diversity in clinical trials and design teams leads to blind spots in product development. Ballester explains that product testing often reflects the demographics of those developing the technology, which are typically white, university-based populations, thus inadvertently excluding others. Aguiar goes further to point out the systemic burden on marginalized communities to advocate for their inclusion in systems that have historically overlooked them.',
      'Finally, corporate accountability and trust are central to this discussion. All four experts agree that companies like Apple have a moral obligation to either ensure equitable performance or disclose limitations. Holtby calls for regulatory oversight if companies advertise health benefits, while Aguiar advocates for apologies, reimbursement, and tangible changes to regain consumer trust. Ballester and Davidson offer more pragmatic views: Ballester suggests that alternate products be developed for underserved demographics, while Davidson believes accountability depends entirely on how the product is marketed.',
      'In sum, these interviews underscore the urgent need for transparency, inclusive design, and ethical responsibility in the health-tech industry. As wearable health devices become more integrated into personal health management, especially for underserved populations, companies must be held to higher standards—not just in performance, but in fairness.'
    ]
  }
]

export const conclusionSections = [
  {
    title: 'Why it matters?',
    content:
      "Everyone should have equal, unbiased access to healthcare, but right now, that’s not the case. Systemic disparities already exist, and if we’re not careful, AI could make them worse. In fact, by 2022, nearly 20% of U.S. hospitals had integrated some form of AI into their practices (Baten & Abdul, 2022). Although this is remarkable progress, it also raises red flags. We are now utilizing these technologies to make decisions concerning the diagnosis, treatment plans, and even who gets access to care. If the data that is fed to these algorithms is biased, the results will be biased as well. This implies that people who are already part of marginalized communities, such as racial minorities, transgender people, and others, may be left behind or at risk of being harmed by the very systems designed to assist them. That’s why we’re discussing far more than simply algorithms and data, this is about real situations where lives are helped or cut short, so we must ensure everyone gets the care they need and deserve."
  },
  {
    title: 'What we learnt?',
    content:
      "One of the biggest things we’ve learned is that bias in healthcare is not just a theoretical issue, it has serious, real-world consequences. For example, in one tragic case, a transgender man, assigned female at birth, was marked as male in the system. Although he had disclosed he was transgender, clinicians failed to consider pregnancy as a possibility. As a result, care was delayed, and the baby was stillborn. In another case, a Black man waited over five years for a kidney transplant because the algorithm used to assess kidney function artificially boosted scores for Black patients, making them seem healthier than they really were. These examples show that bias often doesn’t come from the AI itself, it comes from the data it’s trained on and the assumptions of the people building these systems. I’ve learned that fixing these systems after they’re deployed is extremely difficult. That’s why we need to design them responsibly from the ground up, this means collecting inclusive, representative data and ensuring diverse voices are at the table when these tools are being developed."
  },
  {
    title: 'What was challenging?',
    content:
      "One of the most difficult aspects of this project was the research involved, particularly collecting examples of biased healthcare data that resulted in negative consequences for someone's care. We were not searching for just numbers or broad patterns,  we wanted stories that depicted the very real ramifications of such biases on peoples’ lives. It was important to us that the impact of biased AI wasn't reduced to numbers, but instead framed through human experiences. In addition, the interviews posed another challenge. How could we frame questions that stimulate meaningful and thought-provoking interaction? We had to ensure that the questions weren’t too leading, vague or overly guiding, while also posing questions that encouraged deep reflection and discussion from our participants. Finding the right balance between being respectful, clear, and insightful required a lot of revision and consideration."
  },
  {
    title: 'If we had more time, what else might we have done?',
    content:
      "Given the opportunity, we would have liked to include additional interviewees from varying positions and perspectives within the industry, enabling a breadth of lived experiences and a richer understanding of the problem. The insights gained would be immeasurably deeper in quality regarding the effects of data bias in healthcare across diverse communities. Furthermore, the quiz section could have been changed to be more instructional and engaging. Imagine if participants could walk through case studies with contextual guides, explanations, and real-time feedback. This approach could have helped users not only test their understanding but also learn and reflect as they progressed."
  }
]

export const transcripts: Record<string, string[]> = {
  'dan-holtby': [
    'All right. Hello. Would you first like to start off by introducing yourself? You could tell us your name, did you have where you currently work and where your research interests lie.',
    "Sure. So I'm Dan Holtby. I have a PhD from the University of Waterloo, which is where I work as a teaching professor. So my, my research area before I went into pure teaching was sort of bioinformatics and distributed computing.",
    "Thank you for the introduction. So today's topic will be about how biased clinical research data affects healthcare practices. Our interview consists of six thick specific cases where biased clinical data led to AI systems making incorrect decisions which specifically stem from either gender or racial biases. The first three cases will focus on gender based biases, whereas the last three cases will focus on racial biases. And overall the interview is formatted in a way such that at the beginning of each of the six cases, I'll give you very little information about the case and try to bias you in some sort of way. And with this bias, you'll kind of understand the point of view that AI system made the way that the AI system made the biased prediction that it did. And afterwards, I'll give you the full details about the case and we'll analyze the impacts of the biased AI decision and how we can move forward to prevent these cases from ever happening again for future patients. And then finally, at the very end, after all six cases, I'll ask you some overarching questions about biased clinical data and how it affects marginalized communities and what you would suggest to include in similar systems if you were tasked to design one.",
    'Sure.',
    "We'll start for the first case. Let's imagine you're a physician and you're trying to treat a patient with severe abdominal pain, but they have no visible injuries. What diagnosis would maybe come to immediately in your mind and what additional information would you ask for? It's okay if you're not like super knowledgeable in this space. Just any general ideas are fine.",
    "Yeah, I mean, I'm not that kind of doctor. Like there's all kinds of things that it can be. Right. So, so I would assume the very first thing to do would be some sort of tests to rule things out. Right. But yeah, no clue, no worries.",
    "And then let's say I told you now that the patient was a woman. Is there anything maybe that you lean towards the diagnosis view?",
    "I mean that certainly might shift some, some probabilities around, but I wouldn't think so significantly.",
    "And then I guess if asked if for the same thing for a man, you probably wouldn't Be too certain.",
    'Yeah.',
    'And last question before we actually go into the case. What if the patient was transgender? Would you kind of treat them differently than you treat like a binary man or woman? Is there any sort of special precautions maybe you would take?',
    "I mean, for abdominal pain? I wouldn't think so.",
    "Okay. Okay. So now I'll summarize the case and kind of what the AI did and then we can talk about what happened.",
    'Sure.',
    "So. So there was a 32 year old transgender man that arrived at an emergency room with severe abdominal pain. He informed staff that he was transgender, but in the hospital's electronic health record system, they listed him as male. As a result, clinicians failed to consider pregnancy and they misattributed his symptoms for two factors like obesity. The patient was in fact pregnant and experiencing labor complications. Now, due to the delay in recognizing this, urgent care was postponed and tragically, the baby was stillborn. This case kind of highlights how if you have rigid binary classifications of health record, for example, like male or female, combined with assumptions from the clinicians themselves, it can lead to critical misdiagnosis. Standard pregnancy related alerts would never be triggered by an AI system because in the system itself, it's. The data is registering the patient as a male. And it kind of highlights the danger of algorithmic bias like not fully classified data and the need for more inclusive health, more inclusive healthcare systems that reflect the realities of transgender and non binary people in today's day and age. And so in your opinion, not knowing that the person was transgender, pregnant, what role do you think algorithmic biases play? Not fully labeled data, system design and other factors you can think of that could play into the AI's failure to identify pregnancy risk in this case.",
    "I mean, so the obvious one is that if gender is binary and they've just got the one label, one or the other, that rules out a lot of nuance and is overall inaccurate.",
    'What changes would you suggest then in that case to whether electronic health records or triage systems? For example, should we have affected sign of birth as well as a gender you currently conform to? Should we have provided that would.',
    'That would make the most sense? Yeah.',
    'Do you think, do you think anything also helped maybe if conditions were a bit more aware that transgender people, like transgender math, could still be pregnant or like any other things that give you other predict on this case, I mean.',
    'That, that pretty much sums it up.',
    'Okay.',
    "Is the system didn't let them put the relevant information in and so it, it just wouldn't come up.",
    "Sounds good. Now we'll move on to the second case, I'll kind of do similar thing. I'll give you like limited information and then just ask for your opinions. So imagine a 59 year old patient presents with sudden chest pain and some nausea without knowing their gender. What would be your top diagnosis for the. What would your next steps be? And just anything that you could think of there.",
    "So I mean that chest pain and nausea, I mean that sounds like heart attack, but also just like anxiety can, can cause the. That as well. I mean, I'm limited on what I know. Those are both, right?",
    "Yeah, those factors were part of the actual case itself. Now my next question to you Is, since they're 59 years old, would you take this patient, would you take the problem seriously right away or would you think maybe it's not that serious? Would the age play a factor in how seriously?",
    'Yes, certainly from my understanding at that age, a heart condition would be more likely, but I would think any severe sudden chest pain would be serious regardless of age.',
    "Okay, that's a good point. And now let's imagine that you learned that the patient is a woman and you're told that statistically women are less likely to have a heart attack than men. Would this like influence your decision in any way? Would you take the thing any less seriously? Or like, is there any. Would your decision or anything, your approach change knowing this information?",
    "That woman, I mean, I know heart attacks present differently in women, but not exactly what those differences are. So I wouldn't know.",
    'Okay. And if it was a man and you know that the men are more statistically likely to have a heart attack or you sense emergency change, like would you be more on edge if it was a man versus a woman with the same symptoms?',
    "I would. I wouldn't think so, but I, I don't know.",
    "Okay, so you'd be relatively the same. You'd be like indifferent between both of them. You would take it serious regardless then?",
    'Yeah, I would think so.',
    "Okay. And I'll summarize the case of what happened and then we can talk about the implications of it. There was an AI powered symptom checker app that provided dangerously different recommendations for a man and woman with identical symptoms. Both of them were 59 year old smokers reporting sudden chest pain and nausea. The only variable that was different between the two was gender. The male patient was warned of potential heart issues, including unstable angina and heart attack, and advised to seek emergency care. The female patient, however, was told that her symptoms may be due to depression or anxiety with no urgent care Recommended. This discrepancy highlights how AI tools can replicate and amplify gender biases, potentially leading to delayed treatment or misdiagnosis. In this case, the AI appeared to heavily rely on statistical trends that underrepresent heart attacks in women. And it overlooks the real two outcomes, the real and serious risk of cardiac events in female patients. Public concern grew over this example as it came to light and it revealed how gender stereotypes that women are more anxious or might have more panic attacks versus men have heart attacks usually. It could shape how these algorithmic decisions are made because if the person making it has a bias, then it could also affect the AI system itself and also just the way we code the probabilities and things like that. It might be focused more towards probabilities versus real dangerous outcomes that might be rare. Ultimately, this case highlights the critical importance of designing healthcare AI systems that recognize and adjust for biases. Life threatening conditions must be considered for all patients, regardless of gender, especially when symptoms are shared even among them. In your opinion, how do you think AI triage system should be able to handle these gender based statistics? Should it be able to present all possibilities regardless of the probability of each of the outcomes? Should I especially list fatal possibilities or should it just always just be given the most likely outcome?",
    "I mean if it's an online symptom checker app, it certainly should always be erring on the side of caution because it's not a doctor, so it should say this could be serious, so you should talk to a doctor regardless of, well, there's other things that aren't serious that it could be and they might be more likely. Right. And it's pretty like they all say that, like this is if they, they should anyway.",
    "Yeah, yeah, yeah. I also interviewed someone else and they kind of mentioned a similar thing where it might be slightly different versus like if you're a doctor using the AI thing to identify problems and then make a decision based on that versus a person just checking the app on their own. And then I kind of mentioned that like, you know, generally these days people believe anything. Like if you see something on TikTok, they believe it's real. So I think like the app, if it's just like a person using it, it should generally err on the side of caution, so it should warn them that maybe it's not likely that you have a heart attack, but it is possible. So that you should, you should go see a doctor. But in this case it didn't really do that. Kind of dismissed it and said it was probably just a panic attack which was like, could be extremely catastrophic depending on how severe the heart attack episode was.",
    'Yeah.',
    "And in this case, do you think the AI's response was defensible given it followed statistical trends or should it be given, should its priority have been patient safety over just data driven averages and statistics?",
    "I mean, I don't think it's reasonable for it to be giving one diagnosis anyway rather than laying out the possibilities. So if it lays out the possibilities, even if, well, based on gender, this one is less likely to be what it is. It shouldn't be hiding the number two option or whatever it should be. These are all of the things that it can be.",
    "Okay, that makes sense. So you're saying you should be giving like a more kind of transparent approach. Here are all the things maybe this is most likely, but there's also these possibilities.",
    'Yeah.',
    'And in this case, would you say, do you think we kind of failed in studio to provide a safe recommendation in this case the, the AI system?',
    'Oh, yeah, definitely.',
    "Okay. Because it kind of like depends on your perspective, you know, if you think about technically did exactly what it said. Like we trained it on statistical data. So like in a way you could say like it wasn't technically wrong in what it said, but maybe the way it was framed and the AI was trained.",
    'Sure.',
    'Less, less.',
    "There's, there's always what the, what the machine learning people trained it to do versus what the people wanted them to be doing. And that's true in whatever software field.",
    "Okay. And now we'll move on to the third case. So let's say this time a young patient, let's say around 25 years old, presents with a lump in their chest. What factors would kind of guide your decision and your decision to recommend further testing? Just like initial thoughts, you don't have to be super specific if you don't know. Sure.",
    "I mean, so it's, it's statistics again.",
    'Yeah.',
    "Like certainly women are much more likely to have breast cancer if there's a lump. But it's not impossible for men to either. But again, I don't know what the standard of care is if man has a lump, for example. Nor do I know what the other possibility assists. But I don't know much there.",
    'Yeah, that was a good answer though. I was going to ask you about feeble, Adele. So I guess feeble, you said that you probably err on the side of breast cancer. If it was a bad, would you possibly consider breast cancer? Would you consider other things more?',
    "I mean, again, it's Very unlikely. So by default that's not something I would think of necessarily just because it, without knowing more information, it's pretty rare. And the sort of, the standard is to rule out the common things first.",
    "Yeah, I get your train of thinking. So let's say I give you a little bit more information. Like you said, breast cancer in men is rare. Less than 1% of all breast cancers actually occur in men. So I know you mentioned that, you said that you should kind of test for common things first. But just knowing this fact, would this rarity kind of affect your decision for it? I guess you kind of answered it that you would first do general stuff. But would, how would you kind of balance the possibility of over testing versus missing a critical diagnosis? You just go through first like smaller, more common checks before going to these more advanced checks to actually check for breast cancer or do you have any thoughts of your approach there?",
    "Again, I don't, I've got no knowledge in the medical field, just you know, Bayesian statistics.",
    "Okay, no worries. And that's pretty much it for the diagnostic questions for this case. And I'll summarize the case and can talk about it.",
    'Yeah.',
    "So this case was in regards to Raymond Johnson. He was a 26 year old man from South Carolina and he was denied Medicaid coverage, which is his health insurance coverage for breast cancer treatment, solely because of his gender. Although he's diagnosed with breast cancer, he was ruled ineligible for a federal Medicare program that covered patients screened through certain government programs. And in this case it was specifically program screened for women. As a result, the insurance algorithm automatically excluded him, leaving Raymond without coverage for his chemotherapy and the surgery that he originally needed. This case kind of exemplifies how gender biases embedded in policy and algorithmic systems can have life threatening consequences. The program failed to account for the fact that men can also develop breast cancer, effectively barring male patients from life saving care that they might need. Advocacy groups including the ACLU condemned the conclusion as discriminatory and push for reform. And this case prompted broader conversations about the need to design health care systems and policies that are more inclusive of all genders and reflective of real patient populations concerns. And so now and ask you, what role do you think societal assumptions like breast cancer is like a woman's disease played in the algorithm's decision to deny Raymond.",
    "See, I've, I've got no basis to speculate what went on there. It's. But it's like.",
    "It'S kind of crazy like you know, the possibility is still.",
    "At least 1% like my, my understanding is, government or otherwise, that an insurance company can't have a purely algorithmic denial that a, that a doctor is supposed to review these decisions.",
    'In this case, would you say that maybe governments or like clinical facilities should have like an overruling voice when it comes to these insurance coverages being denied?',
    "Yeah, like certainly it should be about the, what doctors say. And if it's, if it's an actual diagnosis, it doesn't matter what the statistics are. If it's an actual diagnosis, it's an actual diagnosis.",
    "Yeah, it's kind of a lens into my next question. I was going to say, what changes would you suggest in healthcare algorithms or policies to prevent gender based denial of coverage in semiciscus? How can we ensure equal rights to treatment for all? So I guess you kind of mentioned that we could have a more government or hospital like health care intervention when these healthcare claims are being approved. Is there anything else you would like to add?",
    'Yeah, well, I mean, just that it. Just that it should be a doctor making that decision, not a computer.',
    'Yeah.',
    'And not a blanket policy.',
    "And that's also kind of like lends into my next question. Should healthcare algorithms prioritize statistical likelihoods and trends or should it be designed to account for these rare but serious conditions? Especially when there's lives at stakes. How can we strike this balance in these algorithms?",
    "I mean, that's not something I know.",
    "Do you have any suggestion maybe if you were tasked with making some sort of algorithm to approve people's healthcare claims, like how can you make it so that people can't just claim like you know, a disease that they're not possible they possibly can have versus like cases where these diseases are very rare for that demographic. But it is possible.",
    'Like you may.',
    'Serve like any general ideas, basically.',
    "I mean my idea is that an algorithm shouldn't be denying, period.",
    'Okay, so shall these always be like a human review?',
    'Yeah.',
    'Okay.',
    'Or, or AI triage it, but it ultimately has to be a doctor making healthcare decisions.',
    "Okay, we can move on to the next case now. Now we're on our last three cases. So these will be all about more of the racial biases now.",
    'Yep.',
    "So let's imagine you're a healthcare provider and you're evaluating a patient with kidney disease for transplant eligibility. What clinical factors would you consider when assessing disease severity and readiness for transplant? Just any sort of general ideas and you'd have to be superior.",
    'Sorry, for what? Just for what? Exactly.',
    "So a patient with kidney. You want to decide a patient with kidney disease. If they're eligible for transplant. And like think of it as maybe comparing four or five different people. Like how would you kind of decide who maybe would get the transplant first? Is there certain factors you would consider and just like any sort of general ideas you have.",
    "Presumably it's going to be making like the, the decision is going to be based off prognosis, like so I guess it would be two questions. First of all, how, how long can they wait if they're not the next one chosen? Are they going to die before, before another's available? And also how likely is, is the, is the donor to be accepted? Like what, what's the, what's the probable outcomes there?",
    'Yeah, exactly. Those two are like the main things that I would consider as well. Like the need for the patient, like do they need it direly or can they wait versus also how close can they match the donor? Like how likely is that transplant will actually be successful and they can do a work for them. Now my next question to you is, have you ever heard of race based modifiers in kidney scoring systems such as egfr? Example of this is for example, for black patients. When you have these kidney scoring systems, their scores are somewhat inflated. I guess the idea behind it is that black people generally have lower scores with this, like this kidney function system. So then they elevate their scores a little bit so that they look better than they actually are. And it then affects the eligibility for these black people then to get the kidney to receive the kidney transplant versus another person. What do you think the original intent behind this, including racing these calculations is? Do you think it makes sense? Do you agree with that?',
    "I mean it could make sense. It certainly doesn't sound good on the face of it though.",
    'Yeah.',
    "Like if it's, if it's based on whatever it is they're measuring, the, the measurement itself has different outcomes, then that should be compensated for. But you'd have to be very sure of the statistics behind that before actually doing that, I would think.",
    "Yeah, that makes sense. And I'll summarize the case and talk about what happened. This was a case of. The person's name was Anthony Randall. He's a black man from Los Angeles and he spent over five years in dialysis waiting for kidney transplant. He was unaware that there was a racially biased algorithm that was delaying his access to the transplant list. The algorithm used in kidney disease assessments applied a race based modifier that artificially improved kidney function scores for black patients. This meant that Randall's illness appeared less severe than it was and it delayed his eligibility for transplant. In 2023, Randall filed a lawsuit against the Cedars Cyanide Medical center and the United Network for Organ Sharing, arguing that the race adjustment in the algorithm unfairly deprived him of life saving care. Timely vaccinated care through the transplants. Though the Transplant System Board has acknowledged the problem and directed all hospitals to stop its use modifier earlier that year, Randall continues to argue that the change came too late. This case illustrates how clinical algorithms, even when well intended, can perpetuate racial inequities. Is built on flawed and outdated assumptions. It highlights the need for healthcare algorithms to be regularly audited for bias and systems bias effort systems to ensure that all patients, regardless of race, receive fair, accurate assessments based off their medical needs. And not knowing that this modifier made black patients to be healthier than they were delaying transplant eligibility. How do you think this affected patients like Anthony Randall?",
    "Certainly not in a good way. Right. Like with. I think it's obviously quite bad for anybody whose numbers are being artificially inflated to make them. To make them appear better than they actually are. Right. The. Yeah, but I don't know what. What else to say besides that. That sounds real bad.",
    "Yeah. I think the idea behind it was maybe it could, maybe it was probably not malicious. And they kind of wanted to make scores, you know, accounting for different racial demographics depending on how the score this stuff come out. But the effect was that it was definitely way worse than if there was no modifier because it just made unhealthy people seem healthy, that they have to wait five plus years just for treatment. So it ended up being way worse than its actual intent. Do you think hospitals or national organizations should be held accountable when algorithms are known to be biased but they continue to be used? What obligations do you think they have once they become aware of the harm? Because I think apparently for this one, it was known that the kidney function scoring system that they use is somewhat biased because people have been complaining about the inequities for a while. And I think there's been research related to it kind of proving that it is unfair. So it was kind of well known for some time that the algorithms were unfair, but they were still continued to be used. So then do you think there should be these, these hospitals and things should be held accountable for continuing to use systems even though they know it's. It's biased just because maybe makes it easy for them to score different people and like it just makes their system a lot more streamlined and easier. So then do you think they should be Held accountable for, you know, taking an easy route versus being held accountable and doing the right thing and getting rid of the race modifier earlier. Or do you think like they're kind of justified in a way or like.",
    "What are your opinions? I sort of think that once you're made aware of something, it changes from inadvertent to deliberate. So, so in that sense, if, if it ends up actually being like it changes it from, from negligence to malice.",
    "I would think yeah, there's actually.",
    "So, so certainly they would be in a lot more trouble if they've made that decision, I would think, or they ought to be.",
    "There's a similar example I gave for this question when I asked other interviewees as well, where this facial recognition technology, which is used in policing systems and it's been known for a while that these technologies are unfair to people of darker skin tones because if it's, if the facial recognition technology sees one darker skin tone person, it'll match it to like another darker skin tone person because it's hard for it to fully identify all the facial features. So it'll easily match to someone who's completely innocent. And it's been known that in the policing system that, that technology is flawed. But I think they prefer to use it because at least they could put someone behind bars whenever there's like a case. They have a, they have example, they have security footage to use their facial recognition technology. If it's this person, they can send them to jail immediately. Even if they're innocent or not. At the end of the day, if they are proven to be innocent, they could just blame the facial recognition technology for being wrong and just continue to use like a system that's already easy for them. So that's a good example where they kind of knew there's a bias, but they, they didn't stop using the system until there was enough backlash that, that they felt that they actually had to remove it because they preferred to be to, they prefer to keep an easier system. And that might be the case here as well, where they have this unified kind of score, kidney function scoring system. So instead of trying to scrap it entirely, they'll just continue using it until enough people complain and then they have to get rid of the race modifier and rebuild the entire algorithm to be more effective for all demographics.",
    "Yes. So something doesn't sound quite right there because if it was, if it was as simple as it, it takes race into account, you would just like put a default value there regardless. But if it, if it's a deeper Thing that the fee, the various kidney function things that they're measuring through blood tests and whatever. If those numbers themselves vary by race, that's not something that's easily pulled out and also is something that sounds far less deliberate.",
    "And in Randall's case, the board eventually mandated the removal of race adjustments, but it came years after the issue was known. Do you think this delay was acceptable? What do you think should have been done differently to keep these hospitals and organizations accountable earlier?",
    "See, I don't have enough information there.",
    "I think one of the interviews I interviewed, you kind of get an example where similar to if you find a bug in Microsoft and Microsoft's code, you'll say something to them like, you have 30 days to fix this bug, otherwise I'm going to go public with it. You could do something similar here where I found this bug in your system. If you don't do it, I'm going to make it publicly available. People will know that your system's algorithmically unfair. Maybe something like that could help where there's actual concrete proof or some sort of way to guaranteed, say that your algorithm is biased in some sort of way. Maybe that could help then keep these systems more fair and people are able to defy issues or something.",
    "It might, but I, but I really have no idea. If, just, if just if just pointing out the issue isn't enough, then probably, yeah, going to the media, that's sort of the standard approach anyway.",
    "How do you think health systems can design critical algorithms to avoid reinforcing historical or systemic inequalities, especially those tied to race? Any sort of general ideas that maybe they could try to follow in general when they're making AI to prevent these biases, maybe like having more diverse data or like giving you sort of other ideas.",
    'Sorry, run that by me again.',
    "I asked how can health systems design clinical algorithms to avoid reinforcing historical or systemic inequities, especially those type of race? Then I gave the example that, for example, maybe when they train on clinical data, they could try to have more diverse data sets, so people, different demographics, try to represent more underrepresented populations more when they're training, maybe having continuous training through applications. When you deploy your machine learning model and as you find more patients and you get more data across different demographics, you could update the machine learning parameters to then be more accurate towards a larger demographic of people. Do you have any other general maybe ideas you can think of that could help to prevent reinforcing these, like these biases towards marginalized communities in AI systems?",
    "So certainly More diverse training set would be beneficial there. But I mean a lot of it is going to come down to what's the, what's the source of this data? Because all they're, all, I mean all they're really doing is replicating what you've shown them. So you, the, like the number one thing is you have to show them things that are accurate.",
    "Yeah. And I'll ask you the last question for this case. What steps, if any, do you think should be taken to make things rights for patients that were harmed by these biased algorithms, such as adjusting their wait times, issuing apologies, offering some sort of compensation? Is there some sort of way that they can make right to these people that waited five plus years for kidney transplant just to find out that they were unfairly waiting for five years.",
    "Yeah. Outside my wheelhouse to say that. Like I wouldn't know. No.",
    "And we'll move on to the fifth case now. So let's imagine you're treating a patient with chronic lung disease during a respiratory pandemic. If a pulse 6 ometer shows normal oxygen saturation, but the patient appears to be visibly distressed, what next steps would you try to take?",
    'Sorry, what, what are the symptoms?',
    "So they're a patient with chronic lung disease and your pulse oximeter shows that they have a normal oxygen saturation, but you can tell that the patient appears visibly distressed. Maybe. What would you be in your next steps after you see that the pulse oximeter showing that they have no oxygen saturation, but to yourself you can see that they look visibly distressed and they don't look like the oxygen levels are normal. Any sort of like rough ideas? Okay. Like would you try to do other tests or.",
    "Well, certainly. Yeah. Like if they, if it, if it does look like they're struggling to breathe or they're showing other signs of respiratory distress, then, then you would have to, I would imagine, look, look into it further. And I already know that this is going to have to do with skin tone because they, they work by shining a light through your skin so the different skin tones are going to affect the reading. Yeah, I would guess.",
    "Yeah, you are right about that. Now, before we go into the cases, it's one more question. How much would you completely rely on tools like pulse exometers and clinical decision making, do you think if you use like this kind of tool or any other tools in general, and you see, for example, the patient looks different than what the, what your tools giving the result you get from the tool, would you ever double check the accuracy or like doubt the medical device itself? Or would you assume that like, the device is probably right and maybe your intuition is not?",
    'And I would generally. I would generally assume that the devices are accurate without. Without being told otherwise.',
    'Okay, but if you could tell, for example, the patient looks visibly distressed, would you still believe that maybe their oxygen levels are normal even though the equipment said it was normal?',
    "I mean, that depends if I would think there's another possibility besides oxygen levels being an issue.",
    'Okay.',
    "If there's several options and the sensor says that this is fine, then look at the other options. Certainly. But if there's, if there's not other options, then it's got to be the sensor that's the issue.",
    "Look. And now I'll summarize what actually happened in the case. During the pandemic, race related biases in medical technology confronted Dr. Noah Al Biloda. She's a family physician and the CEO of Roots Community Health center based in Oakland. In late 2020, one of her patients was an elderly African American gentleman who suffered from chronic lung illness. One of the checks done previously on a previous visit showed that the pulse oxygenation check revealed that his oxygen saturation levels were high and on this visit that she was conducting the device. The pulse 6 ometer showed that he had a relatively normal oxygen level. That the doctor's clinical instinct was to she could talk instinctively that the patient looked like they were much more distressed than what this device appeared to show. And so she conducted an arteral blood gas test which confirmed her worst fears at the oxygen content in the patient's blood was too low and that he needed oxygen. Sometime later, she came across an article from the New England Journal of Medicine that confirmed her hunch. The oximeters were unable to register low oxygen levels in dark skinned patients as compared to white patients. She and her colleagues were outraged by the fact that a device that they used was supposed to help the patients, but it was grossly inaccurate for the black population. Finally, her clinic participated in a class action lawsuit against manufacturers and sellers of pulse oximeters for more detailed warnings and up to date devices. She didn't stand idle while demanding the FDA to take pulse examiner discrimination towards race seriously. So in this case, the doctor's clinical judgment overruled the device's reading. What do you think this says about the limitation of relying too heavily on technology or AI without fully understanding its underlying biases?",
    "Well, it definitely shows that you can't always trust the device. Like I'm not. Certainly the connection to AI would be if you're getting measurements from these devices, they might not if they're not accurate, then the model you're training won't. Won't be either.",
    'And these pulse oximeters were found to overestimate oxygen level in patients with darker skin. Why do you think this design flaw is able to persist for so long despite the risk it posed to patients of color?',
    'I got no idea.',
    "No. I guess I could possibly, if I had to think, I would say probably. The data that they trained these Pulse 6 operators on weren't super diverse. They didn't test it on people maybe of darker skin tones as much, and thus maybe like people like normal hospital workers and stuff that were using these devices, trusted that it was fine, and most of the time that probably was fine. But then in the cases where you could visibly see that they're different from the actual reading is those are the only extreme cases where you'd actually notice these. So maybe that's why it was so hard to find concrete examples, that it wasn't fair against people of darker skin tones.",
    'Yeah.',
    'Do you, do you believe the FDA and manufacturers have done enough to address this issue? What more should regulatory bodies be doing to ensure that devices are accurate across diverse populations? Do you think they should do, like, maybe yearly checks or like they should be checking the algorithms behind these things that they do? Or.',
    "Yeah, I don't know.",
    'Or maybe this should be some sort of issues, come up with some sort of proof. Maybe the FDA should try to keep these companies more accountable to address the issues as fast as possible.',
    "Yeah, like, certainly there, there should be some sort of requirement to train on diverse data, but I wouldn't know how to enforce and guarantee that kind of thing either.",
    'Yeah. And last question for this case, for communities that have been historically underserved or harmed by stools, how can the medical system begin to rebuild trust and ensure safer, more equitable care to these impacted communities?',
    "Yeah, like, I'm not sure about that.",
    'No worries. And we can move on to the last case now.',
    'Okay.',
    'And so my first question to you is, if you were to purchase a health monitoring device, like a smartwatch, like, for example, an Apple watch, what level of accuracy and reliability do you expect from these kind of devices, especially when it comes to tracking critical health metrics like blood oxygen levels, heart rates, and other things like that?',
    "I mean, I, I don't know what kind of accuracy I would expect. I would expect it to be published. What the accuracies of the, of the measurements actually are.",
    "Would you expect them to be, like, mostly reliable? Like, do you expect your every heart rate to be correct? On the watch Generally, yeah. Okay. And how would you feel if you discover that like the smartwatch didn't work accurately for people of your skin tone or demographic.",
    "Aggravated certainly. Especially if that wasn't disclosed.",
    "Yeah, I think in this case it wasn't disclosed, but I'll go into the case now. So this was in regards to Alex Morales, which was a New York resident and he brought attention to a case of possible racial bias in consumer health device Apple Watch. Morales, who has a Skinner who is a darker complexion, bought an Apple Watch with the expectation that it's blood oxygen sensor would accurately log his oxygen levels for fitness and health purposes. To his surprise, he later found out that the device's oximeter may not work well for people his demographic. In late 2022aMorales initiated a class action lawsuit against Apple for allegedly containing a blood oxygen app that was racially discriminatory and did not function as promised for non white customers. Supported in part by complaints of other studies claiming that more advanced pulse oximetry devices are mass less useful on people with darker skin. Morales asserted that Apple owed the public explanation. After all, paying smartphone users assumed that the device would be equal for all users, which was not the case. While the judge dismissed the case in 2023, it did start an important discussion r[] products. This case showed that there are in fact indirect biases in medical grade equipment. And this shows us that Alex Murray. This shows that Alex Morales and the rest of the community are still subjected to discrimination based on race, even on technology that they choose themselves. Moreover, it demonstrates that a keen eye for responsibility in the tech industry is kind of needed in these situations. So now I'd like to ask you, do you think companies like Apple should have an obligation to disclose the limitations of their health sensors, especially if those limitations affect certain racial or skin tone groups?",
    "Yes, certainly. Like if they, if they know it doesn't work, then they should say that.",
    "Yeah. Because even like the post accommodator devices I mentioned to you, those don't work for the darker skin tone people. So an Apple Watch is going to be like a smaller version of that. So I guess they should have made it clear that it's probably very unlikely to work properly for darker skin tone people.",
    'Yeah, for sure.',
    "Why do you think bias and biases in oximeters and wearable health devices haven't received widespread attention until recently? What might be some of the barriers addressing it? Okay, maybe we just like lack of solid like clinical proof that it doesn't work against.",
    "Yeah, so it would, it would be as could be as simple as they, they, they didn't test it at all. So, so until, until major events where it's like like the, the case here where do a doctor notices that it's very obviously wrong that it, it simply didn't come up.",
    'So in this case the court dismissed Morales Morales case. But do you think tech companies should still be accountable in other ways? What kind of responsibility should they carry when their products are shown to underperform for marginalized communities?',
    'So if like and I know.',
    'Lots.',
    "Of countries don't let Apple have those features turned on because they're like if you're going to be presenting medical data you have to be certified as a medical device which costs money and they don't want to do it. But in some countries you can just do whatever you don't need to be certified. So that's what they're doing. Certainly I think either they shouldn't be allowed at all to be selling a medical device without it certified as a medical device or if they are allowed they should certainly disclose that it's not certified as accurate and shouldn't be relied on for any health decisions.",
    "I think that's a good point.",
    "And if it's going to be certified, whether it's an Apple watch or an actual medical device that they would use in a hospital, then it should be tested on as a diversity diverse a group as can be managed certainly any anything where you're it's operating based on shine like a camp, like a sensor, like a light sensor on skin. And to to not test it on anyone with a dark skin tone is nonsense to me.",
    "It kind of relates to your sponsors now. But should consumer tech devices with health features like smart watches be subject to the same regulatory scrutiny as medical devices? Why or why that I guess you could have answered it. You said like if they're going to advertise as medical devices then for sure they should be held up to the same standards. But if they're not then it should be clearly stated that it's not like medically accurate device. Right?",
    'Yeah, for sure.',
    "And I guess this is also related to your response to but how can the tech industry ensure that innovations and health and wellness tools are tested fairly across diverse populations. We should be involved in that process. And I guess you said that if it's medical grade equipment it should be monitored by regulatory body and they can make sure that's also fairly tested across diverse populations.",
    "Yeah but that's, oh no, that's, that's it is it?",
    'Okay. And this is the last question for this case, what does this case reveal about the intersection between technology race and access to accurate health information in the digital age? How can companies rebuild or maintain trust with users who may feel excluded or misled by biased product performance?',
    "1, 1 I would think a good place to start would be an apology, but on top of that, like an explanation for what went wrong and what you're going to do now to fix what went wrong. Right. Not just, well, we didn't mean to do this, but what are you going to do to fix it and what are you going to do to avoid that happening again?",
    "That makes sense. And then now all the cases are done, I'm just going to ask you some overall overarching questions, just kind of reflecting on everything and then we will be done. The first question I have is, have you ever encountered or learned about intensive biases, whether it be in the clinical setting or anywhere else, and how do you think it affects the decisions that AI makes? This could either be like maybe real life examples that you see bias in AI that has drastic outcomes or anything you've seen online, just like anything you've seen in person or anything you've seen online or anything that you know.",
    "So not nothing I've directly experienced. I do know a friend of mine, when she took her kids to the doctor, the sort of, the receptionist was usually dismissive of whatever she said but if her husband said it would, would believe it immediately. But yeah, nothing I've personally experienced and I've, I have certainly read about bias like not, not, not AI training, just Dr. Training like the, like for, for a long time. I know that in the, like the textbook that they were using to teach doctors in Canada said, said something about like black people feel less pain and that this was just a medical fact that they were given. Completely untrue.",
    "Okay. I guess that would also like overarching effect into AI as well. If these are what doctors or programmers are taught, then this is what they'll also include in the AI systems and then the AI systems will perpetuate those biases further.",
    'Yes.',
    "My next question to you is how can we ensure that AI systems in healthcare reflect diversity and complexity of human identity, including transgender and non binary people. I guess one of the things we talked about was like, you know how you can have a sex at birth and then also the gender you currently conform to? Yeah, that's one thing that could definitely help. Is there any other things that you could think of?",
    "So I mean the hardest part of AI training is always getting good Training data. But. And it's, it's far from specific in the medical field. It's far from being specific to AI as well. Like a lot of the time because of the, the risk to, like an unborn child and not knowing if a woman is pregnant necessarily. They simply do not do any testing on women whatsoever. Which there was, which led to, you know, bad outcomes because drugs that work for men don't necessarily work for women. And even drugs used to treat diseases or symptoms that are specific to women were never tested on women because it's safer to test them on men. And you don't. Because you don't have to worry about if it has. That causes birth defects.",
    "Yeah, that's a good point.",
    "But, but certainly if, if we're just putting numbers in then, then that's, that's not relevant here. But it's just, it's sort of a universal issue, I think is, but if we're, if we're doing like putting numbers in to a computer, there's at least, it's much easier to do than if you're actually doing tests on somebody. Then I would think it's easier to get a more diverse data set that way. But I suppose these numbers are all coming from tests that have been administered, so maybe not.",
    "Yeah, but it is true that we. I've done research when I was doing this project that a lot of times things are not tested on diverse demographics and so that they're not interested on the targeted demographic. Like you mentioned, some drugs that are for females, the tests are mostly males. And then it might not work as well when in practice when females are actually using it. And since they were part of the training set or maybe they're not continuously training, they can't really improve their, their algorithm or medical practices any better.",
    'Yes.',
    "My next question to you is it's kind of related to what we talked about before, but if you're designing an AI tool, how would you balance giving statistically accurate information but also not downplaying rare but deadly possibilities? I guess you kind of mention it like the kind of a list of possibilities and you also mentioning what's likely or sorting about most likely to least likely, but then also making sure to tell them that you should probably see a doctor regardless. Or.",
    "Yeah, like, I mean, I would know. Like generally I don't think a computer should be giving automated advice like that. Like if it's saying here are the possibilities, it should always come with a disclaimer that it's just, it's bare. It's, it's basically reading it an encyclopedia for you. But only a doctor can really make an assessment based on, on, on this what it's telling you. And certainly anytime there is a, a reasonable possibility that it's something serious, it should tell you to see a doctor right away, regardless if that's like the most likely thing. It's just about is it possible.",
    'What kind of training or human oversight do you think should go into building and testing these AI systems that are for health care to avoid embedded gender or race or any other sort of biase?',
    "So how to, how to prevent these biases? I mean the, so the, the tricky part, if you simply don't tell it the these things, you don't tell it race, you don't tell it gender. Then because the, because of a different sort of bias that the, these, these attributes like these features do influence others. You're frozen. Oh, there you go. So these, these attributes do affect other things. And so ideally during machine learning it would be saying it would be using this to compensate for bias rather than as a source for it. But, but it also, it also introduces the, the, the real risk that the exact opposite of what you would hope would happen happens that it fixates on this as the signal. So it says oh well this is a woman, so it can't be this because that's what was in the training data. So can you have a feature that it, that it can't use? You'd have to, you'd have to talk to someone who knows more about machine learning. But I'm pretty sure you can do something like that and if not that probably make a good paper is can you have features that are that it, that it cannot and maybe just regularization would prevent it from fixating on these things. And you can probably wait the regularization to make it say let's never emphasize this but it can be used to change the, the emphasis on other things. But then when you do this, you run the risk of the machine ending up doing basically the same thing that doctors had done with that kidney function test. They're like oh well for this race these numbers are different. So we have to compensate it so that it's fair. But if your statistics for saying we have to to make it fair are wrong, then you're, you're doing the complete opposite. Yeah. So it to me sounds like a very hard problem. Yeah, that's a good response. And all the university I did my undergrad at CS students had to take an ethics course where we talked not, not, not much about medical. It was all over the place. There was legal and ethical considerations and medical stuff came up briefly. I think more universities should do that. UW used to part of the, one of the first year programming courses. The old version of CS136, I think had an ethics component to it, but when it was redesigned to do C instead of Java, they dropped it.",
    "Okay, yeah, this, the interview that we're doing is for the ethics course. And it's definitely like a fun course. It's like a seminar, good to discuss. I do like a nice change compared to traditional CS courses.",
    'Is it a required course now or.',
    "No, it's not required.",
    'Yeah. All right, well, it should be.',
    "Yeah, it's a nice course. And now my next question is, what do you think it feels like for a patient to be denied care not because of medical necessity, but because of how an algorithm classifies you? How can future healthcare prevent.",
    "I mean, outrageous, right? Like that you not, not even a doctor. It's not a person at all. It's just the computer says no. Right. It's. That's, that's like a classic dystopian novel sort of short story sort of thing.",
    'And maybe how do you think future healthcare professionals can be trained to try to recognize and challenge the biases that appear, like these ones in medical systems or software, so that, you know, when they see like, for example, a black patient being denied for like five years, like, how can, how can we draw the line so that professionals can try to recognize and challenge these biases based off their own judgment?',
    "I don't know. I'm better with computers than people. Right? Like, I've, I, I mean, I think part of medical training should be devices aren't, aren't perfect if, if something, if something seems wrong. Never trust one test, Never trust one device. And anyone who's worked with computers knows not to trust them.",
    "It's pretty funny. I talked to someone who's from SickKids and they're more on the machine learning side and he says he said a different thing than most people where he's like, he trusts machines more than humans because he said like, humans will also have their own biases, sometimes even more. Yeah, sometimes he usually trusts a machine more than a human's events.",
    "Yeah, and that's certainly true. And, and it can be bad. Like the, the heart attack example. Actual doctors can be a lot worse than the, than the program was in that, in that case, like if I forget the breakdown, like, but a lot of doctors will not consider fibromyalgia in men, even though It's. It's not. If it's not like the breast cancer case where it's 1%, it's like it happens twice as often in women as it does in men, but that's not. Well, it'll never happen. But a lot of doctors get the impression that it's less likely, so you won't see it. So look for other things first. Even though the difference is not that dramatic. I wasn't. I wasn't speaking bias about bias. I. I just meant that people who program computers make mistakes a lot. So when you're trusting a computer, you're trusting a person at the end of the day.",
    "Yeah, that is a good point. Yeah. That's all my questions for you for today. I really appreciate your time. Yeah, thank you for taking the time to share your opinions on this topic of, like, bias clinical research data affecting healthcare practices. I think we're super knowledgeable about it, but I still appreciate you participating and answering. Do you have any closing remarks you want to say or anything?",
    'Nothing that I can think of.',
    "Okay, that sounds good. That's pretty much it.",
    'All right, well.'
  ],
  'jennifer-aguiar': [
    'Okay, so, hi Jen, would you first like to start off by introducing yourself? You can tell us your name, the degree you have, where you currently work, and where your research interests lie.',
    'Yeah. And hi Jessica. Thanks for chatting with me. So my name is Jennifer Aguiar. I have my PhD in biology from the University of Waterloo where I was focusing on using bioinformatic tools and techniques to study various respiratory diseases. And I am now working as a bioinformatician at the Hospital for Sick Children in their department of Pediatric Laboratory medicine. And there I am focused on pediatric cancers as well as Mendelian rare diseases.',
    "Thank you for the introduction. So today's topic overall will be about bias, clinical research data, and how it affects healthcare practices. Our interview will consist of six specific cases where clinical data led to AI systems making incorrect decisions. And they specifically stem from either gender or racial biases. The first three cases will focus on gender based biases, whereas the last two cases will focus on racial biases. And overall, the interview is kind of formatted in such a way that at the beginning of each of the six cases I'll give you very little information about the case and to try to bias you in some sort of way. And this will kind of give you the understanding or the point of view that the AI had the limited information it has. And then you could understand the bias that it came to and the bias like how the biased AI decision came about. And then we can analyze after, once we identify the bias, I'll summarize the entire case with all the details with you, and then we can talk about the impact that the biased AI decision had and how we can move forward to try to prevent this bias or this decision from happening again for future patients. And then at the very end of the six cases, I'll ask you just some overarching questions based off just all the cases and just talking about the whole idea of these biases in general and how it affects marginalized communities and what you would suggest or include in similar systems if you were test to test to design one.",
    'Okay, sounds good.',
    "So the first case I'll talk about, ask my first question. So imagine you're a physician and you're treating a patient with severe abdominal pain, but you see no visible issues. What diagnosis or what kind of things would come to your mind first or additional information that you would seek. Just having this limited information.",
    "So I guess just to clarify this limited information. So I'm. Yeah, I think if I'm. If I don't know what the sex of this patient Is I not? I guess I would probably err on the side of quote, unquote caution and assume that it is something to do with the gastrointestinal tract, something along that line. So I'd probably be going down that route. But in my mind I can already there's like a red flag there of like.",
    "And then now I'll give you a bit more information. That's. What if the patient was a woman? Is there any sort of symptom that you kind of go towards them?",
    "Yeah. So I would say that if I knew that the patient was a woman or diagnosed female, diagnosed assigned female at birth, I would, I wouldn't necessarily all of a sudden rule out any sort of like gastrointestinal ailment. Like, I would certainly want to still investigate that. But I would all of a sudden now be bringing into it might have something to do with her ovaries. Like, I'm thinking maybe if she's having abdominal pain, she might be having some sort of like ovarian cyst or perhaps it's not even that serious. And it' um, but depending on the age of the patient, like whether she's pre her menstrual cycle, whether she's of childbearing age, whether she's like post menopausal, there could be various things going on there. But yeah, I would definitely start bringing the, the more like female centered anatomy into the equation.",
    'Yeah, that makes sense. And then how would your thinking change if you were told the patient was a man?',
    'If the patient was a man and they were being. Or they were coming to me with abdominal pains, I suppose I would probably stick with my original first line of questioning or first line of interrogation, which would be the kind of like gastrointestinal something to do with the GI tracked. That would certainly, I think, be my, my first go to still.',
    'Okay, yeah, that makes sense. And then now, last question before I actually tell you about the case. What if you, what if you knew the patient was a transgender person? How would that change your clinical reasoning? Would there be some challenges that arise? Would you approach things differently.',
    "When you say a trans person? Do we know if they're female to male or male to female or.",
    "Because they're transgender man in this case?",
    "Okay, so if they're a transgender man, I'm going to just assume that they were assigned female at birth. So I would then again start bringing in. It is possible that there are still like the, the female sex organs or things like ovaries that may be coming into play. Might depend on whether this person is, if they've had any sort of like bottom surgery, like if they're pre op, post op, what sort of potential, like hormone replacement therapy they might be on, like, all of that could be affecting what is going on there. But if I'm just going, if I don't know any of that, and I'm just assuming, like assigned female at birth, trans man, I'm going to assume that they still have intact ovaries and uterus and things like that. And so I would still probably investigate that hopefully a bit more sensitively given that that might cause some sort of gender dysphoria for the person. But, yeah, I would, I would start bringing that back into the equation now.",
    "Okay, yeah, that makes sense. Yeah. You gave like, out of all the people I interviewed, I think you're the most clinical, so you give the most, like, in depth, accurate answer. I think out of everyone. Now actually summarize the case and what happened and we'll talk about the implications of it and maybe how we can prevent it from happening in the future. But the case is there was a 32 year old transgender man that arrived at an emergency room with severe abdominal pain. Although he informed staff that he was transgender, the hospital's electronic health record system had him listed as male. As a result, clinicians failed to consider pregnancy and misattributed his symptoms to factors like obesity. The patient was in fact pregnant and experiencing labor complications. But due to the delay in the clinicians recognizing this, urgent care was postponed and the baby was tragically stillborn. This case highlights how rigid binary classification health records like male or female, combined with assumptions from the clinicians themselves could lead to critical misdiagnosis. And standard pregnancy related alerts would never be triggered from, for example, AI systems, because all the AI system knows is that the gender is male and they don't know if there's a possibility that they're transgender or any other information. And so then standard pregnancy related would have never been triggered. And this kind of highlights the dangers of algorithmic biases and the need for more inclusive healthcare systems that reflect the reality of transgender and non binary individuals. It's in today's day and age. So then my question to you is that in your opinion, now, knowing that the person was transgender and pregnant, what role do you think these like, biases, system designs and other factors you could think of played in the failure to identify pregnancy risk in this case?",
    "Yeah. So I think it's funny because when the, when you immediately started talking about, okay, now I'm going To tell you the case, my brain kind of went, huh, I guess, like, if we're talking about like women or somebody who had like a uterus, like, I wonder, I guess they could be pregnancy. But without any other information, I was just like abdominal pain. Like, it's not my first thought. But yeah, I think that's very interesting because on the one hand I'm thinking that's quite good of the hospital or this healthcare provider that they are using the gender marker that this person has requested, like that is going with their gender identity now. So I'm glad that they're saying like this person is a male because, because they are. But the fact that they're, you know, in a medical record, I think you do want a bit more detail. You do want some way to say, like, yes, they are male, but there is this other information of assigned female birth. They have transitioned. That is, that is massively relevant because, yeah, like you said, I think an AI is going to not like it's only going to be able to based on the information that you've given it and based on, you know, all the past literature and everybody kind of only now starting to come to terms with certain things. Like, I imagine that the AI is basing this on a. There's a gender binary. There's male, there's female. Males don't get pregnant. So not even considering it as a possibility. So that is, yeah, I think a very interesting case. And I think it's. It kind of makes me think of the. My, my sort of overarching thought about a lot of this is just like the, it's the garbage in, garbage out. Like if you, if you don't tell the AI certain information, like, it's not going to know.",
    "That's kind of how it is for a lot of these cases where I think the AI wasn't given good enough data and thus it could make good enough of a decision. Another next question I have for you is what changes would you suggest, whether it be to electronic health records, triage systems, or even clinician training to prevent similar outcomes for transgender patients happening in the future? For example, an idea could be, you have two genders. One would be a sex stand of birth versus the gender you currently identify with. Maybe that way you have more information that an AI system or even just a clinician right away knows to consider from both perspectives. But do you have any other maybe suggestions or maybe clinician training? Because I guess in this case they did the person to identify that they are transgender, but still the clinician maybe didn't take the problem as seriously. Is there any suggestions you have?",
    "Yeah, so I think that one, I think the suggestion of either having a sort of category or an area in a patient's chart where there's like sex assigned at birth and then a category in their chart that is their, their gender, or even having like, I know it might be one of the doctor's offices I go to, where they ask if you're comfortable, would you like to fill out? And you say that you, whether you are woman, trans woman, like, like CIS woman, trans woman, CIS man, trans man, non binary. Like they have several options for you and, and then there's just a lot more for the individual to choose from, which is great in terms of like an inclusivity standpoint. But then if that's the information you're feeding to the AI, it's a, there's a lot more for it to, to, you know, digest and learn from. So I think that's definitely one of them, but then I think the other is, yeah, some of it falls, I would say falls to the clinician because whether or not the AI has the correct data, ultimately the AI is not the doctor here. And so that clinician should have been taking that information into account and that should have broadened their horizons of what type of issues could have been possible. It, like, I'm not saying that this clinician needed to immediately go, ah, okay, so you're probably pregnant, like, but I would say they should have at least inquired as to whether that could be a possibility. Is this person sexually active? Have they recently had intercourse? Like, is there any reason that they would potentially think that that is a possibility themselves? Like, I would at least just be a bit more curious, I guess, as a clinician, ask a few more questions rather than just going down the, oh, no, that's not it.",
    "Right, Yeah. I think it's also a good point that you mentioned of like all those different gender identities that could be good on one hand, but on the other hand you have to have enough people in your data set that are representative of each of the different genders. So that, that way the AI can actually make connections between different genders and different conditions. So it's kind of like a balance between having a few groups that you can have a lot of data for or a lot of different groups where you have a lot less data for each of them.",
    "And then, yeah, that's definitely the sort of paucity of data, I'd say even, you know, just between CIS men and CIS women or different, like Racialized groups. Like we, we definitely already have a lack of data on a lot of individuals and groups. I mean so. No, that is a good, a good point as well.",
    "And that's it for the first case. So move on to the second one now. So let's imagine a 59 year old patient presents with sudden chest pain and some nausea without knowing their gender. What would be your next, what would be your top diagnosis and what would your next steps be?",
    "I. Without knowing their gender, again, it's not like I can already feel the biases in myself because without knowing their gender I'm immediately going to go to a more, a more male diagnosis which would be, I would, I would probably think maybe they might be having a heart attack.",
    "That's a good guess. And then now for the next question. Let's say like regard, you know, they're 59 years old. Would you take the problem seriously right away knowing that it's chest pain and 59 year old or would you think it's not that serious or would you be thoughts in terms of severity?",
    "That's, that's tough. I think not being a clinician, I, I can't say how I would respond for sure. I want to believe that I would be taking it seriously regardless of age. It's not as though, you know, cardiovascular disease is only an affliction of the elderly or it's only an affliction of young folks. So I want to believe that I'd be taking it seriously regardless, but very hard to say.",
    "Yeah. And now let's imagine that, you know, the patient is woman is a woman and you're told statistically women are less likely to have heart attack than men. Would this influence your decision in any way? Do you think it should influence your decision?",
    "I, I don't necessarily believe it should influence my decision because while, you know, trends and statistics are very helpful, we obviously like human beings, want to find patterns in things. I would say this individual, I need to kind of, to the best of my ability, treat them as an individual, as a person and they are not a statistic. And so just because perhaps broadly speaking women might be more like less likely to suffer from cardiovascular disease or heart attacks than men, then it's not impossible. So I wouldn't want to just rule it out myself. Knowing some of what I do know, I, I would probably, if I knew it was a woman, start inquiring about other symptoms because often heart attacks can present differently in women. They'll like. I think the stereotypical for man is yeah, the chest pain the, the left arm pain, all of that. A lot of times women get back pain for, like, no reason. So I'd maybe start inquiring about other symptoms. They might have to help me decide whether or not I should be ruling out a heart attack or not. But I, I don't think I should just be automatically ruling it out because, oh, it's less likely. It's still possible.",
    'Yeah. I think even when I was talking to Pedro about this question, he said, like, the statistics, but, like, statistics, if you apply them directly to, like one person, he seems like 99 of the times wrong because. Yeah, just like a general statistic. So based off the actual symptoms that you see, you can actually make a much better diagnosis than just statistics, which is kind of what AI always does.',
    "Yeah, exactly. Which again, it's like, it's what it, it's what we're sort of training it to do and asking it to do. We're not asking it to sit in the room and like, learn about this person.",
    "Yeah. And now kind of the opposite side. So let's say the patient was a man, and you know that men are statistically more likely to have heart attacks than women. Would this change your sense of urgency in any way? Would you, like, treat, like, if it was a man versus a woman with the same symptoms, would you treat the man, like, man's case more seriously or, like, would you have a more sense of urgency? Would it be kind of the same?",
    "I think it would. I think it would be the same. Or at least I would very much hope it would be if I was in that position. Because I think oftentimes a lot of where our biases come from is that we, we do treat certain people's ailments with more seriousness and more urgency than others. I would say, again, like, because cardiovascular disease and heart attacks can be more prevalent in men. Like, again, I would, I would want to be exploring that route, but. But on the other hand, I'm. I'm thinking, you know, mental health is often, like, not talked about as much with men. It's under diagnosed. This person may very well be having a panic attack and they just don't know because they don't talk about mental health and anxiety. So they're. It's like the reverse coin of. For the woman, I wouldn't want to rule out a heart attack, but I don't. I'd obviously explore other things for this man. Again, like, yeah, it might be a heart attack, but I also maybe want to explore some other things, options for them as well. It's a, I think again just the, you kind of have to go in with the, for the last, for the last one I was sort of like oh, I feel like you have to treat them as an individual and as a person. Which of course, but also part of me is like with more of the stats brain. I feel like you, you have to treat them with like what you're seeing. So like just like raw facts, like take the, try to take the biases out of the equation. Like what have they told you that they are feeling? Go with that.",
    "And now I'll summarize the case and then we can talk about what happened. So the case was that there was an AI powered symptom checker app and it provided HSC different recommendations for a man versus a woman with identical symptoms. Both of them were 59 year old smokers reporting sudden chest pain and nausea. The only variable that was different between them was gender. The male patient was warned of potential heart issues including unstable angina or heart attack and advised to seek emergency care, whereas a female patient was told that her symptoms might be due to depression or anxiety. With no urgent care recommended. The discrepancy in the AI highlights how AI tools can replicate and amplify gender biases, potentially leading to delayed treatment or misdiagnosis. In this case, the AI appeared to rely solely or mostly on statistical trends that underrepresent heart attacks in women, overlooking the real and serious risk of cardiac events in female patients in this case. The public concern grew after this example came to light as it revealed how gender stereotypes that women are anxious or men have heart attacks more can shape algorithmic decisions ultimately. Case Topics the critical importance of designing healthcare AI systems that recognize and adjust for biases and life threatening conditions should always be considered regardless of gender, especially when the symptoms are shared between different demographics. And so my question to you is how should AI triage systems handle gender based statistics, statistical differences and should it present like for example, should it present all possibilities whatever you ask it, like oh I have chest pain and nausea. Should it list all possibilities of different things that you could get? Should it just give you the most statistically highest chance, like the most statistically probable thing that you probably get? Or should it give like an entire possibility and especially like highlight that there is extreme like even though something is very unlikely, it is still possible if it's something that's like extreme, like for example like a life or death situation, should it always be like telling you this should give you all possibilities or should it just give you the Most probable possibility. What are your thoughts on that?",
    "I guess I do have one question. Is it when you're like a healthcare triaging, the AI is giving out these possibilities, the person who's reading them, is that the clinician or is that the patient? I'm going to assume clinician.",
    'Yeah, goes in there.',
    "So I was going to say in that case I'd probably go with all the possibilities even if they are high risk. If it was the patient, I was going to say, ah, we're getting into WebMD territory where you're going to tell somebody they have a headache and that means they've got brain cancer. Like you don't want to freak the, freak the patients out. But I think assuming and trusting that the clinicians are competent and are going to you follow up with whatever the AI is spitting out at them and they're not just going to like take it as fact. As long as we're going to apply some critical thinking, I would probably suggest that the AI offer up all of the possibilities and be like, okay, like you know, it's a very slim chance that it's this, but if it is this, that's a really high risk. So we're telling you. And then here's what it maybe most likely is and, and maybe there is, you know, based on the current research at least like maybe there is a, a slight difference of like oh, you know, men potentially like have a higher incidence of cardiovascular disease. So maybe it makes mention of that. But I think the AI should be sort of in this instance pulling the, the taking in the information, pulling all of the sort of like available resources. But then it, I think it should be giving as much as possible and it should be up to the clinician to then you know, make the judgment call with their own rationale. That would be my, my lean.",
    "Okay, yeah, that was a good point too Also that if it's like a patient facing app, then you shouldn't just give like a bunch of things and like they also get scared from all the things and then.",
    "Yeah, yeah. And I think as well like there, there's so many things like, like even like when it comes to like cancer diagnoses, like a lot of times if people get, are diagnosed with cancer, the first thing they'll look up is the survival like rate because they want to know like what is like am I going to survive x many years? But like when you look up a survival like a survival curve, like that's not like quite what it is telling you, but that's what like the patient's gonna hear and be like, okay, well in five years or in 10 years, like this is what's going to happen. So it's, it's also understandably, a lot of patients don't necessarily have like the medical literacy to be understanding this information. So I think if it's patient facing, it might need to be a little more cautious. But for a doctor who knows this stuff, I think it's okay to be like, here is the kind of worst case, slimmest chance scenario, but there's a possibility it could be this. And you can decide for yourself if that's worth investigating or not.",
    "And my last question for this case is, do you think the AI's response in this case was defensible given it followed statistical trends, or should it have prioritized patient safety over data driven averages? Where can we draw like a line between data, statistical averages and ethical care? Do you believe it failed in its duty to provide safe recommendations in this case?",
    "It's hard to say in the sense of does an AI have a duty to do anything? Like I, I am especially, I know AI is certainly a very powerful tool, but I always lean a little more heavily on. It's up to the, like, the, the, the, the responsibility of care, I think lies with the clinician more than it does the AI. I think, I think ideally in say a more kind of research setting, perhaps statistics is more useful. I would say in a clinical setting, you always want to err on like the side of caution, patient safety, that type of thing. And so I, I guess in that sense, like, yes, the, the outcome or like what the AI did was not ideal. I, I wouldn't, it's one of those, I wouldn't say it failed more that the, we have failed to train it.",
    "And we'll move on to the third case now. So let's say a young patient comes to you and let's say they're around 25 years old and they present with the lump in their chest. What key factors would guide your diagnosis and decision to recommend further testing?",
    "I mean, I'd immediately be wondering again the sex of this patient. You said they were young, but I don't, I don't think I heard you say sex though.",
    'Yeah, just young and lump in their chest.',
    "Yeah. I guess what I would be most curious about what the sex of this patient is. Not that, not that being male, fully 100, like precludes one from developing breast cancer, but it's significantly less likely. I would also be curious about the, when you say age like, and they're young. I. Like, how young are we talking? Like, are they kind of like 25 years? Oh, you said 25 years. Okay, so they're post. Post p. Okay. Yeah. I think I would be asking about sex to start before kind of moving into. No, I don't. I think, again, regardless of. Because regardless of whether it's a, you know, breast tissue or whether it's, like, muscle, there's a. There's an obvious physical lump that you would probably just, regardless, want to get, like, an ultrasound or a CT scan. I suppose if they don't have breasts, I wouldn't necessarily send them for, like, a mammogram, but like, an ultrasound or a ct, I feel like would be useful regardless.",
    "Okay, that makes sense. And then now let's say I told you that the patient was a female, and how would your decision change or what you. What would you lead more towards in terms of diagnosis?",
    "I would definitely be. I don't know if I would be able to diagnose right away, but I would be immediately, like, alarm bells ringing of potential breast cancer or some sort of tumor and would be sending them for a mammogram to. To investigate further.",
    "And then now let's say they were male. Would you consider the same things as you would when you were considering a woman's chest pain, or would your approach be different for male chest pain and the lump in their chest?",
    "I think it would be different both for. Like I said, like, there is the statistical. Like, I believe the incidence of breast cancer in men is only like, 2.2percent maybe, but also, I think just like, the practical of the way that mammograms work. I. I don't. I'm not sure if it would be physically possible to have somebody who doesn't have breasts have a mammogram, but I would still send them for some type of imaging, probably just not that one.",
    "Okay, so it sounds like either way, you kind of have a similar approach and you would take the both seriously. And for the last question, I'll give you the statistic that you're talking about. Breast cancer in men is rare. It's less than 1% of all breast cancers occur in men. And my question is, should that really affect your decision to test for it? How would you balance the risk of over testing versus missing a critical diagnosis?",
    "Yeah, so I think, for me, I think that's where it comes to. It reminds me of, like, you know, subreddits where people say, like, need more information. I wouldn't be like, yeah, I Probably wouldn't immediately go, ah, yes, it's probably breast cancer for this young gentleman. I wouldn't be ruling it out, but I, it would not be my first thought. But I don't think it needs to be my first thought for me to want to send them for imaging because I think because there's something so visibly physical with this one, I'm like, regardless of whether I think that's a, like breast tumor or a different type of tumor or just some cyst or something, there's an obvious lump. And so I'm like, you still probably want to get that checked out regardless of what you think it is. And then if you do get it imaged, then okay, if I'm, you know, if this is one of the 1%, I think the imaging would at least like start to send us down that path. So I think probably because there's something so visible about it that I would, I would be sending them for some sort of like imaging regardless. But no, yeah, like I, I, I will admit that it would not be my first thought. Yeah, of oh yes, this person probably has breast cancer. Yeah, I think I would just be like, that seems like a weird lump. We should probably get that looked at regardless.",
    "Yep, I think those are good points. And now I'll summarize the case. So this case was in regards to Raymond Johnson, which was a 26 year old man from South Carolina. He was denied Medicaid coverage, which is health insurance coverage for breast cancer treatment, solely because of his gender. Although diagnosed with breast cancer, he was ruled ineligible for a federal Medihat program that covered patients screened through certain government programs. In this case it was programs that only screen women. As a result, the insurance algorithm automatically excluded men, leaving Raymond without coverage for chemotherapy and surgery that he needed. Originally, this case exemplifies how gender biases embedded in policy and algorithmic systems can have life threatening consequences. The program failed to account for the fact that men can also develop breast cancer, effectively barring males from life saving care that they need. Advocacy groups, including the ACLU condemned the exclusion as discriminatory and have pushed for reform. The case prompted broader conversations about the need to design healthcare systems and policies that are inclusive of all genders and reflective of real patient populations problems. Now my question to you is, in this case, what kind of role did societal assumptions like breast cancer being like a woman's disease played in the algorithm's decision to deny Raymond for coverage?",
    "Yeah, that one's like very shocking to me because I thought this was going to be an oh like we, we missed the diagnosis because they were a man and we just assumed that men can't get breast cancer, which would already be bad. But the idea that they would be diagnosed with breast cancer by a clinician and then it would just be oh, but we won't treat you for it because you're not a woman is shocking to me. Yeah, no, that's, that's. I, I think that's one of those things where I'm not entirely in terms of, I'm not entirely sure why a, maybe not why AI is being used, but like why that is information that the AI is being given. Surely the, the gender of this patient doesn't matter. If I, if I, the clinician saying this person has breast cancer, you should just be treating them for breast cancer. Their sex or gender is irrelevant at this point. I have told you that they have breast cancer and therefore need treatment for that. So I feel like that information. I understand maybe there'd be, or maybe there'd be some instances where you would need the information about sex or gender when you're coming, when it comes to policy or insurance and the American healthcare system. But that one, I'm just like, huh. It just seems like it should be this is what they have treat them for it. Not. Yeah, but they couldn't possibly have it because so we won't treat them for it. That seems backwards.",
    'Kind of like a tactic insurance might use to not have to like pay for the medical treatment then because.',
    "Yeah, I'd be curious if like there's any. Because obviously I think the American healthcare system is certainly something. So I would be curious if there would be any sort of similar kind of instances like in Canada, like if like private health insurance companies would have the same issue or you know, whether you would still get the treatment because like publicly funded type of thing. I'd be interested about that. But yeah, no, that's, that's quite bad.",
    "Uh huh. And then my next question to you is what changes would you suggest in healthcare algorithms or policies to try to prevent these gender based denial of coverage in similar cases? How could we make sure that there's equal treatment for everyone?",
    "I don't in terms of like actually implementing this like for the AI? I will not pretend to know how this would work, but I would like my thought would be one, it needs more andor better information. Like it, it should be aware that there is 1% of breast cancer cases that are in men. So like they should like there, there shouldn't be this presumption that it would be impossible for a man to have breast cancer and therefore be denied coverage. And then I, yeah, so I guess it'd be that. And then I, I wanted to put a stronger emphasis on the diagnosis from the clinician. Like, I think to me that should be, I don't want to say like clinicians can't make mistakes or be wrong in their diagnoses, but I, I would, I would be kind of weighting that more heavily of, well, if like X, Y, Z, you know, imaging or test or whatever and like the clinician has taken all that information and said this person has this disease, I would weight that more strongly. And, and maybe they're, you know, other characteristics would come into play a little less. But I guess, you know, this also, this also involves massive sort of systemic policy changes because government bureaucracy and also just like insurance companies, they're, they're there to make money. I'm sure they're not, they're not looking for more opportunities to pay out for people. So I think it would involve like, even if we've got very good AI tools, again, it's like the. But are we gonna like, use them properly? We'll see.",
    "And then the last question, which is kind of related to what we talked about, but should healthcare algorithms prioritize statistical likelihoods and trends or should they be designed to account for like these rare but serious conditions? Especially when there's lots of stakes. How can we try to like strike this balance in these system, these AI systems?",
    "Yeah, I think this reminds me of another sort of US healthcare system based snafu where yeah, like, trends are useful. I understand why they are there and why we would want AI or like any algorithms that we are using to help with that. Because in terms of figuring out, you know, populations that might be most at risk and potentially need more screening or things like that, like, I think that's very useful. I understand like why at, you know, certain ages they tell women like every two years or three years or whatever to be getting like screenings and things for breast cancer at certain ages. Like I, and they're not necessarily telling men that. Like, I get why that is happening, but to assume that people, you know, that these rare cases don't exist seems crazy. And it like, it makes me think of the US health care system. They. I don't know which commercial algorithm it is, but there is a commercial algorithm that they were using to again, like notice trends and guide health decisions and determine which, you know, groups were most at risk for certain things. And basically it determined that. I suspect it's probably all people of color. But they, they honed in on black people, were less sick than white people for various different ailments. And it's because they were using amount of like, money spent on, you know, given populations in like, healthcare treatment. They were using that as like a proxy for sickness. Because I guess the idea is just, you know, the more money you're having to spend on treating somebody, the more sick they probably are. When really it turns out that it's just that the American health care system doesn't spend that much money treating people of color. And so, like, people of color aren't being treated. And now AI is thinking, oh, like. And so like, they're just like, healthier, they're just not as sick, which is. So then they're not.",
    "They're.",
    "No, they were no longer being included in like, high risk categories. They're like, no, those people are so healthy. So it makes me think of that where I'm like, yeah, like, sometimes trends can be helpful. And sometimes I'm like, depends what you're basing those terms off of. And also, even if they're like, super accurate, like, yes, women are more likely to get breast cancer. That is correct. It shouldn't be. It should be like, perhaps it should be like a more instead of less situation. Like, it should be okay that maybe we're screening women more, but that doesn't mean that we should say, but so men couldn't possibly, like, the rare cases still exist.",
    "Yeah, that was a good example you brought up too, where like, in those cases it would actually like, ruin like systemic inequities even more because people that already can't pay enough for healthcare are being less prioritized. Like, it's.",
    "That'd be like, yeah, exactly.",
    'Yeah.',
    "So, yeah, before it was like, we know that you're sick and you just, we won't treat you or like, you can't afford to be treated. Now it's, you couldn't afford to be treated, so we're not even gonna actually recognize that you're sick. And it's like, okay, great, excellent.",
    "Yeah. And that's it for the gender cases, the gender bias cases. So now we're going to talk about the last three racial bias cases now. And the first question I have for you for case number four now is imagine your healthcare provider evaluating a patient with kidney disease for transplant eligibility. What clinical factors would you consider when assessing disease severity and readiness for transplant? You can kind of think of it maybe as you have like, several different patients that need a kidney transplant. How would you kind of decide which one needs it the most? What kind of factors would you consider?",
    "Oh, that's a good question. I feel like I don't know as much about the kidneys in terms of like what would preclude somebody.",
    "Any general ideas? Okay, sorry. Any ideas are okay. Like you'd have to be super knowledgeable.",
    "Yeah, I think, I suppose I would be looking into, you know, things like age. Like it is horrible, but I'm like if it's, you know, a 25 year old person who needs a new kidney versus, you know, a 90 year old person, the, the sort of cost per use, for lack of a better term is going to be better with the 25 year old patient. Not to mention they would probably handle the transplant a lot better. I guess I would need to know about like blood type. My hope would be that there are enough kidneys from enough donors that that wouldn't matter, but I know that's not usually the case. So it might just be that like somebody gets bumped on the list because there's just not a suitable kidney for them. I don't think I know what sort of lifestyle factors I would or should be looking for in terms of kidney function. I suppose I would probably want to know like if they or like their family perhaps has a history of kidney disease because again, when it comes to transplants it's horrible. But you're kind of thinking like, am I going to give you this kidney and are you going to immediately ruin it? Like is kind of the same thought process as to why alcoholics don't get liver transplants type of thing. So if there was some sort of, if there was some sort of disease that like some sort of like renal disease, that means that like this kidney is either like not going to take very well or that it's, you know, probably going to be damaged quite quickly after transplant, then maybe that would be a prohibiting factor. But yeah, that's tough.",
    "Yeah, those are good points. A couple other points I think some other, other interviewees came up with was like the severity of how bad like the person needs it. Like for example, someone's kidney is like close to failure, then maybe we want to give them a transplant first. Then another thing was also how eligible you are. Does your kidney match the donor's kidney? Will your body be able to accept it? And just other things like that. Now my next question to you is, have you heard of race based modifiers in kidney scoring systems such as egfr? The idea behind it I think is primarily for black People, for example, these kidney scoring systems will make them, their scores look healthier than they actually are because I guess generally their scores are lower with these systems. So then they have a modification for certain races that makes the kidney scores look better than they actually are. Like do you think this is a good idea or no?",
    "And I, I was going to say that is, I didn't, I didn't know that they did that for kidneys. I, if, if you'll allow a tangent, I do know that they do that for lungs. As I mentioned, my PhD was all like kind of like respiratory research and things like that. And one of the primary ways that diseases like respiratory diseases like COPD and things like that are diagnosed is with a spirometer which you blow into and it kind of measures your lung capacity. And if your lung capacity is below a certain level, you probably are sick or there's something wrong with your lungs that is meaning that they don't have as high of a capacity as they should. But back in, you know, the 1700, late late 1700s, American Founding Father and like just prolific slaveholder Thomas Jefferson, he decided that he looked into it and claimed that slaves had a reduced lung capacity compared to white people. Then in the late 1800s another like slaveholder turned doctor quantified this at 20% that he said that slaves or just black people in general had 20% less like decreased lung capacity than white people. And then like the 1920s came along and there's all the eugenics and this started making it into clinical handbooks to the point where now like modern day if you're using a spirometer it has a race based like yeah, like correction on it. And so basically it just decides that specifically black people, I think some other racialized groups have like a different sort of like correction put on them. But specifically black people like their basically said that like their lung capacities should just normally be 20 less than white people. And so it just corrects that. And even like the most well meaning like researchers or clinicians may not even know this is happening because it's like baked into the software. And so if your lung capacity is like 20% less, which probably means you're sick, but if you're a black person then it's like oh no, that's just normal for you and so you're not sick and so you're not going to end up getting treatment for copd, which is I think especially tufts given like the primary risk factors for COPD are like smoking, air pollution, things like that. And a lot a Lot of, like, there's a lot of, you know, climate, racism and socioeconomic reasons as to why people of color are probably exposed to these pollutants, like at a higher level. And so they're probably more likely to get copd, but then also less likely to be diagnosed and treated for it because they've got, they're just told like, oh, that 20% decrease in lung capacity is just like, that's par for the course for you. So if they're doing the same things with kidneys, I will, I can say, no, that's a horrible idea.",
    "Yeah, yeah. Now I'll actually summarize the case and talk about it. But yeah, it's kind of like a similar thing to what you just said.",
    'Oh, that poor person.',
    "So in this case, the person's name was Anthony Randell. He was a black man from Los Angeles who was on dialysis waiting for kidney transplant for over five years. What he didn't know was that the algorithm that was used for the kidney transplant system incorporated a race based modifier that made black patients kidney scores seem a lot better than they actually were. This modifier caused Randall's kidney disease to be perceived as less severe than it truly was, leading to his placement in the national transplant waiting list to be significantly delayed. In mid-2023, he filed a case against the hospital, which was named the Cedars Sinai Medical Center. And he also filed another one against the United Network for Organ Sharing. And he alleged that they were unfairly depriving him of a fair chance to get a transplant because of racially based formula. And apparently it wasn't a secret that the algorithm did have a bias, because I think a lot of these cases did come up. People have done research into the, into the actual scoring system behind it, and they were able to tell that there was in fact this adjustment factor for people of different races, specifically for black people. And so, yeah, these cases were coming up and the board of the transplant system understood that the modifier was resulting in black patients illnesses being severely underestimated. And by early 2023, all hospitals were directed to stop the use of this race adjustment for black patients waiting times. And their waiting times were changed then to be reflective of the postponed changes. Randall claims that these changes should have came sooner and he could have already had his kidney that he desperately needed. And this case highlights how the Google clinical algorithms are good. They try to do the scoring system to see who needs it, but the execution was not done well because of this. The insertion of race, which caused black patients to receive to not receive their quality care in a timely manner. So this is like a crazy case. Like it took five plus years for him to even get it just because the score was making him seem a lot healthier than he really was. Knowing that this modifier made black patients appear healthier than they were and delaying his transplant eligibility. How do you think this affected, you know, patients like Athena?",
    "Yeah. Oh gosh, that's horrible. Yeah, so that's really like you said, I think it's tough because in that case, like the, that the AI was sort of doing what it was designed to do of trying to prioritize patients. And again, like, I get why that happens. You don't want it to be like a first come, first serve if there's some like this is why like emergency rooms triage, like if somebody comes in with a gunshot wound, like if you've got like a cold, like they should probably be seen before you. But it's the garbage in, garbage out of like, well, how did it decide that somebody was worse? Based on very, very poor, like biased data that we gave it. And so I'm happy to hear that it, it sounds as though you said the this like, correct. This race based correction has been like people been told to stop using it for kidneys. I'll, I, I, I mean, I hope that it's the same for the like spirometers for lung disease. I, as of 2022 or 3 it was not so like, you know, maybe in the last two years it's changed but.",
    'And like this one was in 2023 as well.',
    "Yeah, because I think it was like, it's something that maybe some people knew about and there was like research into it. But then it was more around Covid where like again, people were using spirometers a lot and people were like, hang on, wait a minute. And a lot of like articles are coming out eyes on it. So hopefully a similar thing happens if it hasn't already. But yeah, that's a very, just that kind of gets to the crux of like it. Part of me is like, it doesn't, it does matter. But part of me is it matters less right now at least of how good can we make AI and like the models and stuff. I'm just like, I think we got to look back at ourselves and be like, what, what, what are our own biases? What are we feeding it? And like changing those up before we kind of rely too heavily on AI which will do exactly what we want it to do. Which is say that for some Reason black people's kidneys are different or whatever it was trying to suggest.",
    "Yeah. My next question to you. Should hospitals and national organizations be held accountable when algorithms are known to be biased, but they continue to use them? What obligations do you think they have once they become aware of them? And I could give you an example that I gave to the other interviews as well. Another. This was an example, like the one that we just talked about was they know that there was this bias towards black people, but they kept using the technology. Another example was facial recognition technology used in policing systems. It was known after some time that the facial recognition technology doesn't work as accurately for people of darker skin tones because shadows and things mess it up easily. So then whenever you have, like, a photo, it'll match it to, like, another black guy. That's not really the same. You can tell yourself that they're not the same, but it will match them. And the police will then, you know, incarcerate that person. And I guess at the end of the day, even if they were proven innocent, they could just say, oh, well, the AI system did it. It's not our fault. And then they have an excuse there. And so, like, it's beneficial for them to have it because then they have an excuse to blame it on someone. And also just makes things more efficient. As soon as they identify this person, put them in jail, we're done. The case is closed. Like, so. In that case, even though it was known that there was these biases, I think they preferred to use it because it was just efficient and easy for them, and they didn't really stop it until there was enough backlash. And that's kind of similar to the analogy here where this scoring system probably made it easy. They just use the scoring system. They know who goes first, who goes second, who goes third in the kidney transplant chain. But then once enough backlash came, then they realized that we should really make a change rather than just take the easy way out. So I was just asking about, do you think they should be held accountable for these kind of things where they can't take the easy way out? Or maybe there might be even, like, a malicious intent behind it. Maybe they don't want black people to get treatment as fast as other people. And so then my question to you is, like, what obligations do they have to take care of the problem once they become aware of its harm?",
    "Yeah. So I think that's a really good example that you brought up, and I think it illustrates my feelings on it, which are. I think they. Yeah, the. Again, the, the burden of the responsibility of care and the, and the, the burden of responsibility falls on. It would be like the clinicians or the, the hospital or the, in the case of like the policing would be like the police services. Because I, I do not think using AI as a, as a scapegoat or an excuse is acceptable. And yeah, I think it's unfortunate because I think yes, there are ways in which AI can probably make a lot of people's lives and like the work that they're doing better, easier, like you said, like more efficient. But I guess I'm always like, but easy like more efficient or like easier for who? Because like yeah, sure, this cop didn't need to you know, spend too much extra time trying to like, like tell apart two different people to make sure that he was incarcerating the correct person. Great, he gets to like knock off early and go home. But like that's not better for the person who has now been wrongly incarcerated. So it's like, yeah, it's more efficient but in like a, it's like that the half assed done way. And I think it's tough as well because I think this is why I'm like, I think we need to be like looking at ourselves. I don't think when something goes wrong with AI, I don't think we can go like, oh well, I was just. The eyes fall. I'm like, no, because in that best case situation you're still responsible for like double checking its work basically. Like you're still like the autonomous person in the room. But then I think like the unfortunate thing is I think sometimes people will also just go with it because the AI, we have sort of biased the AI and so then the AI is giving us biases back. Like you said, it may not necessarily be like, like overtly malicious, but I think unfortunately a lot of people have this sort of like implicit, like deep within them bias of what type of people deserve treatment, deserve to be healthy. And if the AI is saying, okay, this black person, they don't need a kidney as quickly, but this like white person does, great, that works well for me because that's kind of what my preference was anyway. Like it's, that's kind of what it, we've biased it and so when it feeds us back our biases, we're obviously going to go, yeah, that makes sense. So, and, and that's not on the AI, that's on us. So I think we have to take responsibility for that. Yeah. So in like the best case of take responsibility of like you should be checking its work. In the worst case, take responsibility for if it messed up, it's because you, you kind of did that to it. So yeah, I definitely think they like for the. That's why I'm sort of both happy that this race based correction seems to be, have been removed for the kidney function scoring. But I don't know if it's been fully removed for like spirometers. And in my mind that is like shocking because I'm like, okay, if you didn't. Because it's like baked into the algorithm and space and baked into the software, maybe you didn't know that it was doing it. Now that you do, I'm like, yeah, but like you said, I think sometimes as well, even like, well meaning people might be like, but we don't have another method. And so unless. But I'm like there has to at least be a way of like okay, fine, blow into the spirometer and then manually do a calculation to like add another 20% or whatever. Like there has to be, there has to be a way that you can like at least start to remedy this. You can't just throw up your hands and be like, oh well.",
    "Yeah, but I guess it also kind of sucks for the hospital people because if they're using like technology from another company, at the end of the day they don't know the underlying like background things like they can't fix it.",
    "Yeah, yeah, it is very tough. Yeah, I guess that's maybe a good point of when I, it's like the human responsibility but then it's okay logistically. Is it the hospital or is it like these third party like. And then obviously it's much more difficult to hold a third party that's got their black box technology. Like, it's, it's much harder to hold them accountable. So yeah, in terms of how people would be held to account, I think that's very difficult. But yeah, I, I would at least, I would at least as the clinician, even if I can't stop using these tools, even if I'm not like, I would at least be trying to do my like level best to be finding out as much information as I could or correcting where I can or like advocating for corrections where I can. I think it's just probably like just turning a blind eye and being like, oh well, this isn't my problem and I don't think it's the right way to handle it.",
    "Yeah, that's a good point. And in Randall's case, the board eventually Mandated the removal of race adjustments, but it came years after the issue was known. Do you think the delay was acceptable? Could we have done something differently to hold them accountable earlier?",
    "I mean, I don't think the delay is acceptable. Whether or not we could have done something different, I'm not sure in the sense of these, like I said, it's very difficult to hold companies or like these long held systems, like bureaucracies accountable. And even when you do, it's a very slow process. There's a lot of litigation. So I'm like, like, I don't think that's acceptable. I do think that that's too like too long of a turnaround time. But I don't know what we would have done differently apart from like, we gotta be like overhauling the system here.",
    'Yeah. And how can a health system design clinical algorithms to avoid reinforcing, you know, historical systemic inequities, especially those that are tied to race. Is there any sort of general ideas you have, like we make these AI systems. Is there something we can do to try to prevent these systems. Sorry. To prevent these biases?',
    "Yeah, unfortunately I think that's if I've been saying like the garbage in, garbage out. Like the obvious solution is, okay, we have to give it like good data or like unbiased data. But that is easier said than done. I think the, given our history as, as a species, we don't necessarily have the best track record of being unbiased and you know, things, things like that. So I, I, I think it still comes back to, we need to, we need to go back and be like, okay, where are our current biases? Like what data do we already have and how is it maybe wrong? And then be committed to like doing better moving forward for like the like new data that we are creating. I suppose in the meantime, if we're, because obviously like one of the other issues is like there being a paucity of data and we don't want to just like lose all the like health data that we have to like be using. But at least going back, knowing where those biases are and then you know, like say for the, I think you said it was egk, like the egfk, the kidney function. Yeah. You know, maybe figuring out, okay, what is that? Like like correction. Can we like train AI to you know, take that correction off or like ignore it or something if we want to keep using that data because it's maybe otherwise good and helpful and there's just like a part of it that's bad, like maybe we can, as long as we're aware of it, train AI and like model it. Better to not take those biases into account. But ultimately it's one of a. It feels like that we need to clean up our side of the street before we, you know. Yeah, hand it over to AI.",
    "Yeah, it's a very good point. And now the last question for this case I have is what steps, if any, should be taken to make things right for these patients that were harmed by biased algorithms, such as like adjusting wait times, issuing apologies, offering some sort of compensation. Do you think there's something that we could do to, you know, regain the trust of these people in the healthcare system?",
    "Yeah, I think, I think that's a very good point. Like the regaining of trust. I think that is one of the, that is a very big issue in like the medical landscape is that for a variety of reasons, all of them very valid marginalized communities do not necessarily have a lot of trust in healthcare and like the healthcare systems. So I think rebuilding that trust is very important. I do think like, sort of personal and, or public apologies can go a long way, assuming that they are backed up with, you know, like, like the ensuring now that like this, this correction is not being used like some sort of tangible. Okay. And we are changing, like we are going to like, put like our words into action. I think that's very important. I think compensation is honestly probably fair. I don't know how you would quite calculate that out. And in certain, like for this one, it took obviously a long time, longer than it should have, but this individual at least did end up getting their kidney. There are, I'm confident, cases where the, the outcome is that somebody died and I don't know how you compensate somebody for that. So it, it's tough. But I do think, you know, depending on like the, the situation and severity, like perhaps conversation is important there. But yeah, I do think that there's, there's work to be done to sort of regain people's trust in that sense, which would, you know, even, even if it weren't absolutely the right thing to do, it also behooves us as like the, you know, the healthcare individuals, like the, the medical researchers, because, like, if we're concerned about bias in our data and we want to make sure, like we're doing, you know, the equity, inclusion, all of that nice stuff. I, I suspect it's difficult when people are like, well, I don't want to, I'm scared to be included because I don't trust you. So having that trust and like that two way communication, I think would then allow us to further improve our medical research because we would have the trust of people that we want to include in our medical research to then make our medical research more, you know, inclusive. So it's like this long kind of, you know, domino effect.",
    "But yeah, those are all great points. We'll move on to the fifth case now. Almost done. So this is the second last case now.",
    "It's all very interesting. So I do see why it takes people a long time.",
    "Yeah, all of them took like over an hour. So now let's imagine that you're treating a patient with chronic lung disease during a respiratory pandemic. If a pulse oximeter shows normal oxygen saturation, but the patient appears visibly distressed, what would your next steps be?",
    "So the, whatever tool I'm using, it's saying their oxygen is normal, but they clearly look distressed.",
    "Yes. And it's like a pulse exometer. It's like those things that you put on their finger.",
    'Oh, yeah, yeah.',
    "There's no sensors.",
    "Yeah, yeah. I would definitely be basing it off of the, the patient's distress, I think. Yeah, I think if they're visibly distressed or like struggling to breathe, that's, you know, the pulsometer can go out the window.",
    'So then would you try then using other tools and other things to be able to assess oxygen levels?',
    "I'm trying to think because my, my gut reaction is like, no, I'd probably just give them oxygen. But I'm trying to think, is there any reason why, like that would be dangerous to do if they don't have deficient oxygen? And if there was, then yes, I suppose I would need to like, have, you know, some sort of orthogonal method to test them though. I mean, as I've mentioned with the spirometer, I don't know how, how much trust I have in the, all of these, but yeah, I think I would if it, if it seemed medically necessary to, before administering oxygen, I would probably use some sort of method. Like I wouldn't just base it off the pulsometer, but if not, and it's just based on patient distress and I'm allowed to, within my judgment, give them oxygen chin, because they seem to be struggling to breathe. I'd probably just do that.",
    "Okay, yeah, that's a good response. And now the next question I have for you is how much would you like completely rely on these tools like pulse exometers and your clinical decision making? I guess you kind of answer this, but do you think you would ever double check the accuracy of the device, Would you ever doubt it? Would you think it's 100% right? Even if they're distressed, do you think it would still possibly be right or would you be on edge, like not fully trusting it, using your own intuition more?",
    "I mean, I would definitely be questioning it. And I would want to double check because my, my inclination is to trust the patient and trust like what they are experiencing in their own bodies. So I would want to, yes, double check these methods because I'm like, even if it's like an, even if there's no weird, you know, like the kidney thing, even if there's no like weird corrections on them and it's perfectly fine, technology fails like it, you know, there are, you know, wires get crossed, frayed, like there are reasons why things may not work. And so I think I would err on the side of assuming the patient is, knows what they're talking about when they're experiencing distress and would kind of want to double, triple check the, the methods that I'm using. If they somehow all are like saying, you know, one thing, maybe that's when we have to start looking at other avenues. But I would, I would err on the side of trust the patient first and yeah, double check the technology because technology can fail.",
    "Yeah, yeah, I like that response. Now I'll actually talk about what the case was. So this was during the pandemic. There was race related biases in medical technology that confronted Dr. Noah Albor Albert. She was a family physician and CEO of Roots Community Health center based in Oakland. In late 2020, one of her patients was an elderly African American gentleman who suffered from chronic illness, chronic lung illness. And one of the checks that she's done in a previous visit showed that through a pulse oxygenation check that his oxygen saturation levels were high then. Now on this visit, she used the pulse oximeter and it showed that his blood oxygen level was at a normal level. But the doctor's clinical instinct indicated that the patient was much more distressed than what the pulse oximeter showed. She conducted an aerial blood gas test which confirmed reverse fears that the oxygen content in the patient's blood was too low and that the he needed oxygen. And sometime later, she came across an article in the New England Journal of Medicine that confirmed her hunch that these oximeters were unable to register low oxygen levels in darker skin patients as compared to white patients. She and her colleagues were outraged by the fact that a device that they use that was supposed to help patients was grossly inaccurate for the black population. And finally she and her clinic participated in a class action lawsuit against the manufacturers and sellers of pulse oximeters for not having detailed warnings or up to date devices. And she didn't stand idle while demanding the FDA to take this pulse oximeter discrimination towards races very seriously. This is kind of like an example where she took her own intuition, similar to how you said you would probably take action on the patient's discomfort, but it's like something that's been going on for years. People of darker skin tones. I guess the pulse 6ometers sensor doesn't work as accurately on people, darker skin tones. And so it wasn't giving good blood oxygen reading. So it's probably affected like a lot of patients over the years. But only the doctors that kind of want to like challenge the, their equipment will be the ones that actually try to go further, do more tests and try to make sure that the patient is okay.",
    "Yeah, no, I would say kudos to that doctor for kind of not only following their intuition for like that, you know, the acute issue of like this patient is in distress, but then like the, yeah, the class action lawsuit, the doing their research, the following up. Because obviously, like, yeah, we've talked a lot. I released, I've talked a lot about like how, you know, this is all like, terrible for like the patients who are involved and like, there's a lot of accountability that needs to go on, like hospitals or companies or whatever. But there are also like, yeah, instances where I'm like, oh, like, yeah, a lot of empathy for those doctors who are trying to do their level best to treat their patients, think they're doing all the right things and then find out like, oh, I've just been given like tools that are not equipped to help certain people. And like, that's kind of scary.",
    "Yeah. And in this case, the doctor's clinical judgment overruled, the device's reading. What do you think this says about like the limitations of relying too heavily on technology without understanding, you know, the, the, the algorithms behind it or the biases behind it.",
    "Yeah, I think that's like, I think you said it there like that we shouldn't be relying too, too heavily on technology. And like this is a more like a pulmometer is obviously, I think, a more common form of technology. Like people have encountered them. It's not like the more I think for a lot of people, like nebulous ideas of like AI and things like that. And so when you get to like these more complex ideas like using a technology, not fully understanding it, and then just like Taking its word as gospel is dangerous. I wouldn't even like with like Google how they like it gives you like the AI summary at the top of all of your searches. Now there will be times like I will be searching something for, you know, work. I'm looking for papers, but before I get to the papers, I see what it's written and there are times where I'm like, that's just wrong. I know that's just wrong. But you know, if somebody doesn't know, if they're, if they're not in that sphere, then you might read that and go, oh, okay, like, great, log that away is a truth. So yeah, I think it's one of those things where again I, there are a lot of ways where I think technology and AI can be very helpful. I do think it's, it's not worth being afraid and therefore not implementing them. Part of me is like, it's happening, it's going to be happening. But yeah, being mindful about biases, being mindful about ethics, and also remembering that like this is a tool but ultimately you're as like the say, clinician or whoever's using the technology, you're still meant to be like the brain in the room. So you know, you can't just kind of throw up your hands and be like, well, the AI said. Because the AI is not this patient's doctor.",
    "Yeah. And you know, pulse exometers are kind of like a call in piece of equipment. Like every hospital has it. Usually when you have a visit, you'll use it every single time. Even though they're so common. We, they were found to overestimate oxygen levels in turkey skin patients. Do you think that could be still so common? And this design flock kind of stayed for so long despite the risk it posed to patients of color.",
    "Yeah, there are certain, there are certain tools that we use like this, the pulsometer, but also like the, like, yeah, like the aspirometers I was talking about where I'm like, we, they're so common. We use them all the time and it's either we don't realize that they have these, these shortcomings, these like limitations, or we just, you know, how bad could they be? And it's like the best option we have. Like there, like there could be a lot of reasons why. But yeah, the idea that we're still used using them is, is alarming. And I think I, I would have the same feeling of that doctor where I'm like, this is outrageous that I'm using a tool that I'm expecting to do one thing and it's perhaps not, and that's. That could be seriously impacting people. And I think that goes for, like, the technologies and then also, like. But like, it's similar just like people's own, like, subconscious biases. Like, they're. They're deeply ingrained, but they just, like, linger, unfortunately. And so then when it's in technology that now it's like, well, we've built it in and, you know, how do you. How do you slowly start to remove that or replace it with something better like it. It can be. Can be tricky. Things can. Things can really easily get kind of baked in, and then that's just how it stays, like, status quo.",
    'Do you believe the FDA and manufacturers have done enough to address this issue? What more do you think regulatory body should be doing to ensure devices like these are accurate across diverse populations? Any sort of ideas you have there?',
    "I think, I mean, I'm gonna say since they're still in use and it seems like at least this one doctor had no idea that there was this limitation, I'm gonna say they're probably not doing enough to, I would say, at minimum, warn people of the limitation and like, be very clear that this tool can do this. But, you know, it might not be as accurate with these populations or like, it mention the limitations so that people can be making like, their best judgment or their best, like, their decision with their eyes open. But also it's. And this, this goes for, I think, all kind of medical research going forward. Like, it. The medical research, medical tools and devices, technologies, like, they need to be tested on more than just white male bodies. Like, they're like, before we're putting like, this tool on the market, this. That's now going to be in like, every hospital and every doctor's office. It should be tested on a wide variety of people. And like a wide, like, in this case, it'd be like a wide, like, variety of skin tones to see if it actually works. Because I feel like if you can catch it early enough and then, you know, you're not just like, sending it out there to do its harm. And hopefully there are ways to, you know, I, again, I don't pretend to know, like, all the ins and outs of the tech that goes behind those readings, but there's got to be ways to somehow help them work better for everyone. But yeah, like, we have to be testing on more people, more different types of people.",
    "Yeah, that's actually a good point you brought up, because when I was doing research for this project, we found out that a lot of times for actual clinical research that goes behind these tools or like pills or like anything in general is usually predominantly just white males compared to other people of other demographics. And even I think some medicine that's geared towards women will be mostly tested on males because they won't have, they won't have like birth complications. Like, you don't have to worry about, like, usually when you choose women, you have to make sure that they're not pregnant for some of the things. So if you just choose all men, it's easier to do. But is it really that accurate for a woman's body? That's the question.",
    "Yeah, I think, I don't remember which scientist said that it was. They chose to use the, I mean, they didn't even touch the whole race thing. I think they just decided, of course we would use white people, but they decided to use male bodies because women's bodies and all of their hormones and stuff were just. It made the environment too complicated. And I'm like, right, but if that's the environment it has to work in. Like, it's like when we were talking about Fusion Validator and we're like, we need to like stress test it. We gotta like see like how far until it breaks. Don't be like, oh, well, it works perfectly fine in like this cool, clean, like, like whatever, perfect environment. If that's not the environment it's gonna be like used in. But they're like, women's hormones are just like too complicated. And so like we're gonna test in men's bodies that I guess are void of hormones apparently. Like it doesn't even.",
    "That's crazy. Yeah. How should future healthcare providers be trained to detect and challenge device based bias in healthcare? You think like, maybe when they're retrained for like hospital use, for example, that they should always have like their own intuition versus what a tool might tell them. They should always like, not just like completely rely on these equipment. Or maybe even if there's like biases already known, should they be aware of them? Like for example, the pulse examiner, should they be told right away that for black patients it will look healthier than it probably actually is? Like, is there some sort of way that we could train these professionals to be able to always be prepared for biases that may arise in some of these tools?",
    "I think there's a lot that we could do to help these physicians be, you know, be able to practice their best possible medicine. Like, I think if there are biases or Limitations in the tools that they're going to be using, they should be made aware of them. But I also understand that there are certain times where you don't know what you don't know. There might be tools that have these baked in biases from long past research that we don't know about and so you can't relay that information to people. So in that case, I think in terms of training, ensuring that the as much as possible, they know that these tools are to be used to help, they're to be used to gather information. But in terms of decision making, that needs to still fall to them and using their best judgment, if they're ever concerned, make sure that they know that it's okay to be asking questions using orthogonal methods like double checking type of thing. And ultimately I think making sure that health care is patient centered and patient focused, like making sure that what that patient is telling you is always kind of like front of mind and what is driving you to do something. I think, and that one can be, I think it sounds like, oh, well, yes, of course. But again, like, like it's not just the technology. I think we ourselves obviously have a lot of baked in biases and it can take training and it can take practice to get people to kind of like get out of those. Like even things like often, you know, from long, long ago, like black people, people of color have been described as having like higher pain thresholds. So then their pain is often not believed or when they say they're in pain, people don't believe them. And now, even though I wouldn't say I've run into any doctors who have that explicit like bias, like they're not saying out loud, I think sometimes it's still in there. And so there'll be lots of stories of, you know, black women, like I think even Serena Williams, like, who's famous and wealthy and all this, but like was just post, post having given birth, was saying she was in pain and everyone's like, meh, like no, you're fine. And then it turned out she was hemorrhaging and thankfully somebody listened to her and she's fine.",
    'But okay.',
    "Yeah, I think like kind of putting into practice and being like, okay, when patients tell you how they're feeling, listen to them and, and having that be kind of your, your guiding principle.",
    "Yeah, I think that's a good point about like how like a lot of times we'll have like underlying biases, like deep rooted in us that we won't even realize that the bias is just like Your way of thinking. And that could really affect, you know, but in the case of medical, in the medical field, how they administer health care to patients.",
    "Yeah, so I think, like, making sure people know about the limitations of the technology, but also just making sure people are as much as they can. It's hard to like, root out the biases you are like, subconscious, but, like, as much as you can, just like listening to patients and making sure that they are centered in whatever you are doing.",
    "All right, and then now we're on the last case for this one. I want you to imagine that you're purchasing a health monitoring device, like a smartwatch. It could be like an Apple watch, for example. What level of accuracy and reliability do you expect from like these kind of devices, especially when it comes to tracking stuff like critical health metrics, like your blood oxygen levels, your pulse, your heart rate, like just basic metrics that you know, you want to monitor on a day to day basis. Maybe especially if you have a condition, it's important to check your vitals every day. How, how reliably reliable would you expect these metrics to be?",
    "Maybe I'm just like, cynical. If I had a condition that I needed to be checking my, like, vitals, I would not be trusting Apple to be the one to give me that information. Like, I have a smartwatch and I'm using it to count my steps. And I know that it's probably not accurate entirely, but it's giving me a ballpark. I can just tell if I've been like, sedentary or if I actually like, walked around in the day. I think if I needed to know like, accurately what my heart rate was, I'm, I don't know if I would trust Apple to be doing that for me. I would hope that because they are advertising that they can, that there are rules and regulations in place that mean that they need to be accurate. But I have a healthy level of skepticism with them where I'm like, I don't know. Probably not.",
    "Okay, so like, in general for smart watches, you wouldn't 100% trust, like these critical metrics that they would give you. You kind of think of them as a ballpark, but you wouldn't like, rely on them?",
    "Yeah, I, yeah, I would think of them as a ballpark. Like, I'm sure I'm confident it can tell whether my heart rate is like 100, like I just exercised or whether it's 60, like I'm resting. Like, I'm sure it can tell that difference. But like, I don't, I Don't know if I'm confident in, like, it telling me what my, like, blood oxygen level is or what my, like, you know. Yeah, I'm not. I would want. I would definitely be. If I saw something and I was like, oh, my heart rate's been, like, elevated for a very long time. Like, again, I think I would be like, okay, maybe I should, like, go see a doctor. Or maybe I should. There might be another tool that I can use. But I wouldn't just necessarily be like, oh, Apple says that I'm sick.",
    "Okay. And how would you feel if you discovered that your smartwatch didn't work accurately for people of your skin tone or demographic?",
    "That would definitely still bother me. I think even if I'm coming from the, Like, I don't trust it to work and I'm skeptical and whatever. I guess I'm coming from the. I don't trust it to work for anybody. So to find out that, oh, it does work for some people, just not me would be. Not. No, that would be bad. I would be very frustrated by that, especially because then I'd be thinking, okay, but what if I was one of the people who really trusted it and was using it for, you know, xyz important thing. Well, okay, now great. I. Now I know it hasn't been working amazing, like, so I think that there would be a lot of anger and frustration there.",
    "Yeah. And I'll summarize the case that's related to the smartwatch. So the person in question was Alex Morales, a New York resident. And he brought to attention to a case of possible racial bias in consumable health device Apple Watch. Morales, who had a darker skin complexion on an Apple Watch with the expectation that its blood oxygen sensor would accurately lock in oxygen levels for fitness and health purposes. To his surprise, he later found out that the device's oximeter may not work well with people from his demographic. In late 2022, Morales initiated a class action lawsuit against Apple for allegedly containing a blood oxygen app that was racially discriminatory and did not function as promised for non white customers. Supported in part by complaints of other studies claiming that more advanced false oximetry devices are massively less useful on people with darker skin. Morales asserted that Apple owed the public an explanation. After all, paying smartphone users assumed that the device would be equal for all users, which was not the case. While the judge dismissed the case in 2023, it did start an important discussion r[] products. And this case showed that. This case showed that the world that there are in fact indirect Biases in medical grade equipment like Pulse 6 ometers similar to the previous case. This shows us that Alex Morales and the rest of the community are still subjected to discrimination, discrimination based on race, even in technology that they choose to use themselves already. It demonstrates that a keen eye for responsibility in tech industry is needed in these kind of situations. And so my question to you is, do you think companies like Apple have an obligation to kind of disclose the limitations to their health sensors, especially if those limitations affect certain ratio or demographic skin tones or in general, do you think they should be disclosing these limitations so that people are actually aware prior to buying these in the first place?",
    "Yeah, I think definitely. And it's funny because like this whole time we've been talking about like just the, how it's important like medically and how like there's a safety aspect to it of having a technology and finding out that it doesn't work for your skin tone or like whatever demographic that you're in, like that's, that's probably like the, the worst thing, the top thing. But it's funny that I'm just like, oh yeah. And I bet people would also just be incredibly frustrated to know they paid money for a product that doesn't work or at least not for them. So I think both from a kind of safety health perspective and then also from a. Just as a consumer, yes, I do think you need to tell me if I'm about to buy a product that won't work for me and if by not working for me that could have like serious or like, like medically related consequences.",
    "And why do you think skin tone bias and exometers and wearable health devices haven't been, haven't received widespread attention until recently. What do you think some of the barriers might be to addressing it?",
    "Yeah, it's funny because like it seems like all no 2 of the 3 cases here that have to do with race have to do with something that can't like read past or see past certain skin tones. So it does seem very prevalent in like a lot of different technologies, which is interesting. But yeah, I, why it hasn't been addressed until now. I think there part of it could be a people don't know or people kind of don't want to know, like it doesn't affect you. So meh, like moving on. I think it is also that it's not quite a stigma but that feeling of like, like people, if you're from a marginalized community, I can imagine that for again a lot of very wide Ranging and valid reasons. You have been made to feel like you don't matter or that like, you're like, people aren't going to listen to you anyway, so. Or like if, if you do bring it up, you're going to be like the insert stereotype here. Like, isn't. You're going to be seen as just causing problems. And like, and so I can imagine that there are a lot of people who are sort of just like, well, like, that's going to be a lot of like, spotlight on me or like eyes on me if I raise this almost like the kind of bystander effect, but like within your own kind of body or your own like, like demographic where you're like, it's going to be like such a hassle and like, no one's going to believe me. It's going to be like this uphill struggle to get justice. And so like, I don't know, I guess I won't mention it kind of thing. But it could also be, I think as well, like, that these people don't know, like, for all. For what I've said previously of like there being some mistrust for like the health care community. I think there's also a lot of people, they're like, doctors know best, they're well trained, they got like MDs, they are smart and they know what they're doing and I trust them. And so if they're using a device, you're kind of just assuming, assuming like that it is working. And so you like, if you're part of that community where it's not working for you, you might not even know. Especially if it's not one of these, like, kind of case studies where like, something's gone wrong and it's blown up in your face. Like, if it's just a routine thing and nothing's really come of it, like, it's. There's no reason for you to like, be suspicious that it's not working. You might just be like, yep, nope, seems fine kind of thing. Like, if you told me right now that blood pressure cuffs don't work very well on women, like, I wouldn't, I would be surprised because I'm like, I use them all the time and it like, but it's never come up because I don't have high blood pressure. So I'm like, I use them all the time and it's always said it's fine and I feel fine. Moving on. So it's possible they don't work, but I have no idea. So, yeah, I think it's like that kind of mixture of, like, people might not know, either on the patient side or the clinician side, people might not know. And then the, when you do, it's like, okay, how much do I want to rock the boat to bring this up and, or do I just wait for hopefully somebody else to bring it up? Or how important is it to me? Like, it can be tough. We, we do unfortunately put a lot of the burden of advocating, like, on the people who are like, the most marginalized and vulnerable. We're just like, well, if you want better, you're gonna really have to fight for it. And that's tough.",
    "Yeah, that's a good point in this case. The court dismissed Morales's case. But do you think tech companies should be held accountable in other ways? What kind of responsibility should they carry when their products are shown to underperform for marginalized users?",
    "I do think that that company should be held more accountable. And again, this is my cynical realm. Like, are they going to be. But no, I do think that they should be held more accountable. And again, even if we're not going from the moral, like, medical elements, I'm just like, you're a company who is designing a product and you are selling it in this capitalist society we live in. And I think it's fair that consumers expect that it work. So even from just a, like, you know, purely capitalistic sense, I'm like, I do think they should be held accountable if their product doesn't work work. But yeah, I do think they should also be held accountable if it's, if they're releasing a product that is like, possibly dangerous kind of thing. I feel like there are some areas like, you know, like the kind of wellness community, not super well regulated, you can be getting, you know, supplements for anything. I feel like tech is kind of like that where it's like, it's sometimes regulated, but there's a lot of, like, areas where it gets really blurry and not very well regulated. And it probably should be.",
    'Do you think consumer tech devices with health features like smartwatches should be subject to the same regulatory scrutiny as medical devices? Why? And why not?',
    "That's really tricky because on the one hand, if they are, if they are purporting to be a medical device, in a sense, if they are purporting to be giving the same type of information that you'd be getting from a medical device, I think it is fair that a consumer would think that that be accurate because they're going to be using it probably for the same reason that they like that they would if they were getting it from a medical device. Like, what they're doing with that information is probably the same. And so I do think that if you give. If you're saying that you are able to measure certain medical metrics and you are offering those up to patients or not patients, customers, who. People, and they're using them for, you know, probably medical reasons or at least like, you know, they're informing their medical decisions with them, I think it's fair to expect that they be tested. But then it's tough because on the other hand, like, you know, Apple's gonna be like, we're just a tech company. We're not doctors. Like, if you've got a medical thing, you should be going to your doctor. Don't come to Apple like that. It's your choice to buy an Apple watch. And, like, while that's quite, like, callous, I. A little bit, I do see it where they're like, we never pretended to be doctors. If you. If you want to be, like, properly monitoring, like, your blood O2 levels, you should be, like, getting actual medical equipment. So it's tough. It's. It's that balance where I'm like, how much of that is, like, on. Like, if you're ever. If you're offering up a product that is in the medical realm and. And we're like, people are gonna expect that it works a certain way. You should make sure it does, versus, like, how much of the due diligence has to go on, like, the person to, you know, use their best judgment. When buying and using products that aren't, like, from licensed medical places. That one's, like, a real gray area for me where I'm like, I want to say that they should be regulated like medical equipment if they are offering a medical service, but I feel like there'll be implications of that that might be difficult to get around. And in. In a small sense, I do see their point of, like, but we're not a medical company. So, like, if you're looking for actual medical things, like, don't come to us. That's not really on us that you did that, so. Oh, I don't know.",
    'Yeah. How do you think the tech industry can ensure that innovation and health and wellness tools are tested freely across diverse populations? Who do you think should be involved in the process?',
    "Oh, yeah. Again, it, like, it comes down to just, like, the. The bare bones of, like, your experimental design. Like, when you're selecting your cohort, make sure there are people of different skin colors or make sure there are different genders and sexes and stuff in there. Like just being mindful up front of your, your study design, I think goes a long way because like, if, if like the study itself is like a house, I'm like, that's your foundation. So like make sure that that's solid before you start like building rickety scaffolding on top of it. Who should be involved? I, again, I think this is where a lot of like the DEI and like affirmative action and things like that come into play. Because again, for, you know, maybe not malicious reasons but in like, you know, just privilege and biases there. If like everybody, if all the top researchers and all the people who are involved in like making this technology are all white men, if it's not malicious, it might just be like they don't have this experience. Like they've got blinders on, they've got their own like, privilege. That means that they're not necessarily thinking about, oh, we should double check that this works on all skin colors. So I think having people of like marginalized communities at the table to bring their expertise, not just their like technical expertise, which they have obviously, but then they're like their lived expertise of, hey, what about this? That maybe people haven't thought of, I think is really important because I think when it comes to, you know, getting your diverse cohorts and like coming up with a really well rounded design, it might need some people with that, you know, lived experience to nudge us in that direction. So I think the people who are in these like, leadership positions or these, like, heading up these projects, there should be some diversity there.",
    'Yeah, those are good points. And the last question I have for this case is what does this case reveal about the intersection between technology, race, access to accurate health information in the digital age? How should companies rebuild or maintain trust with users who feel excluded or misled by biased product performance?',
    "Yeah, no, that's actually, that's like, made me think like, that's also a very good point of like the access piece part because whilst companies might be going, well, if you've got medical concerns, go to a proper doctor. Awful lot of people who don't have doctors or who cannot afford doctors. And even though, you know, an Apple smartwatch might be expensive, it's probably less expensive than like certain medical, you know, apparatus that you'd be getting, especially if you're, you know, somewhere in the States where it's, you know, privatized healthcare. So, yeah, that's tough. Where we might have this situation where You've got populations who, because they've been marginalized, are probably likely to have less access to, you know, a doctor or like, medical type, like the sanctioned medical technology. So they're going the route of tech and then finding out, okay, well, the tech doesn't work for people like me. Yeah, it so just that intersectionality thing where it's just like biases and like racism, it just begets more of that where now these people are sort of, I think, at a loss of like, well, things don't work for me. And I, like the medical system doesn't work for me. But now, like, the tech that I'm buying also doesn't work for me. Like, I can imagine that you're. You eventually get to this place where you're like, okay, well, like, nothing seems to be working for me. And then you asked your last question there.",
    'It was about how companies can rebuild or maintain trust with users who feel excluded or misled by biased product performance.',
    "I think it's probably similar in the sense of, like the earlier when you had asked about, like, medical, like healthcare professionals doing that. I think it's similar in the sense of acknowledging that there's been a problem, acknowledging that like you made a mistake or the product that you're putting out was working. Like, acknowledging that and kind of being upfront and apologizing for that I think is important. But then following that up with action, like showing that you are, like, working to address these issues, making sure, like, that the technology you're coming out with in the future has the ability to actually, like, work properly for all the, like, different skin types. I like, I would even maybe go so far as, like, especially if this is like, kind of the only reason this person bought this watch and it doesn't work for them. Like, like reimbursing them for that product, I think would be the polite, the morally correct thing to do. And then, yeah, making sure that you are, like, putting your words into action and making sure that, like, the things that you are putting out now in the future are more mindful and will actually work. Because it's like you just don't want people to go, oh, sorry, like, here's a 200 credit to Apple. And then, you know, the next product you buy also doesn't work. Like there needs to actually be, like, real change.",
    "Yeah, that was a good point. And then now we're done all the cases. Now we'll just ask you a couple of final overarching questions about just reflecting over everything. The first question I have is, have you ever encountered or learned about any personal bias in your work or maybe that you've seen through the Internet anywhere? And how do you think this affects healthcare decisions?",
    "Yeah, so, yeah, like I said, my, my graduate studies, like my thesis was all on respiratory medicine. And while my, the research that I was actually like doing was all, you know, lung cells in petri dishes, like, we weren't at kind of like clinical trials or anything, I don't think there was that as much of that chance for bias there. But in terms of the research that I was like taking in and that was then informing what I was doing. Yeah, a lot of it was definitely. And like, I, I think I had a whole chapter of this on my thesis of like, we can't just have like white male bodies because it's. A lot of what I was looking at was tobacco smoke, cannabis smoke, and then, you know, we were going to do cannabis vaping, but then the pandemic happened. So we're like, covet also affects the lungs, we'll just wedge that in. But a lot of these things, like especially tobacco smoke and cannabis smoke, a lot of the studies are on men because, like, I think some people will be like, oh, it's because, like, men like smoke more. And I'm like, do they? Or is it just that, like, you know, it's the kind of default body to be working with and, and like the, it's the kind of the go to. Or like, even if there's like the kind of social students of like, oh, like people who smoke, like, that's like a guy thing. And like, you know, smoking weed's a guy thing and not. So then like, there's this whole other half of the population that's not really being included. And then when we were looking into things like the effects of like cannabis on the lungs, like hormones definitely play a role in it and like the various different cannabinoids, like, they're also kind of like getting involved. So like, the results were certainly different between men and women when we were looking into any studies that actually like included women. So yeah, I think there's definitely biases like that that I've kind of come across especially as well. Like, it's better now. Like, it's getting better. A lot of granting agencies, like the Canadian Institute for Health Research, cihr, big granting agency, they explicitly make sure that you in your grant application include how you're going to take into account sex, gender, if applicable, like, like race, things like that. They explicitly ask you about that and you need to like, Outline in your project. Like, and, like, make sure you're saying, like, yeah, if it's a. If you're actually going to be doing human trials, like, we're going to have, like, X many men and X many women. And like, so, like, that's good. But, like, the further back you go and research, like, it's all on just white men. Like, there's not. There's not a woman or a person of color to be found. So that can be certainly very difficult. I think for the research I'm doing now, it's like, say, like, the rare disease stuff, it's tough because they're rare. So I was like, we're kind of. I don't want to say there's no bias, but I'm like, we're taking whoever we can get. Because I'm like, they're so rare in terms of, like, the. There's so, like, the pool is already so few. Like, if we were only selecting men or we were only selecting white people, like, we'd have, like, no patience, basically. Like, so I. I guess it being a. Working with rare diseases kind of forces you to, like, just you. You gotta make sure you're including everybody, which is a good thing. But, yeah, no, I think it's. It's tough. I like. And then even, like, in a personal realm, like, it kind of reminds me of the first story where somebody I know went in to their doctor's office because they were complaining of, like, an abdominal situation. And it was almost the. It was almost the reverse where, like, it's a good thing. The doctor at least considered ovaries, but they became very dismissive and they're. It's just your ovaries. It's probably like, I don't know, like, PMS or something. Go home. No, no. It turned out that, like, their appendix was rupturing and they had to get it taken out. And it was just like, oh, okay. Like, would a female doctor have, like, you know, maybe thought about ovaries but then seen. Yeah, that's probably not it. And like, you know, looked further and gotten me into a hospital before my appendix burst, maybe? Hard to say. So I think both in, like, the research I'm in, but also just, like, lived experience. Like, yeah, there's medical biases everywhere. Which is always why I'm like, I know people are like, oh, I'm so excited about AI And I'm like, I am. But, like, I'm so nervous about what we're giving it because, like, science has done a lot of amazing things and there's also some bad science out there.",
    "Yeah, it's kind of trying to find a balance between good AI, bad AI, convincing AI.",
    "I know, but then I think part of it's also, I'm just like, yes, we want to make sure that the AI that we're programming is good, it's robust, it's ethical, all these different things. But part of me is gotta turn the lens on ourselves a bit and be like, we should be mindful of what we're giving it and not pretend that we're perfect. And if it messes up, it's obviously the AI's fault. It might be that we're giving it some real questionable data based on some real questionable thoughts that probably many of us are still holding.",
    'The next question I have for you is how can we ensure that AI systems in healthcare reflect the diversity and complexity of human identity, including transgender and non binary people?',
    "Yeah, I think that all it still just comes back to like the making sure that we as people and like researchers and clinicians are doing the work to include those people, like all of those marginalized folks in our studies. Making sure that we have good, robust data to then feed AI, I think is step number one. And then as well, like, you know, research takes a long time. It's probable that AI in medicine is going to be implemented before we can go back and like undo all of our wrongs in research. So I think it's also just being really mindful that we do have these biases, that the research that we have given AI is going to have biases and just not kind of leaning back in our chairs and letting AI take the wheel, so to speak, like making sure that we are still using our judgment and our discernment and not just taking AI as like gospel.",
    'Yeah. If you were designing an AI tool, how would you balance giving statistically accurate information but also not downplaying rare but deadly possibilities?',
    "Hmm. Yeah, it's really tough because I'm like part of the, part of the obvious, like a lot of the reason that people want to be bringing in AI is that it, it makes things like quicker, easier, like faster. Like you'll be able to like diagnose faster, you'll be able to treat faster like this, like efficiency is what it's meant to be good at or why we want it, especially with like healthcare being so like imitated and like, you know, we're trying to free up clinicians to do as much as they can, like with a little time that they have in a day. So you want those trends and you want those stats to make it so that I can be as fast and efficient as possible. But like you said, we don't want to be losing out on those rare niche possibilities that are still real and valid and should still be taken seriously. So I don't. All I can sort of think right now is almost like, almost like we were talking about earlier, like a, like a, I'm thinking of like a flagging system where like the AI's kind of gone through, it's like distilled all this information and then it's kind of flagging in terms of like most statistically likely. But also maybe like here, here's like the most like serious concerning and kind of just like have those like quick flags that then like a clinician can look at and be like, okay, it's most likely this, but maybe I should check this because if it is this, that it would be really serious, like kind of thing. And then again, put that in the clinician's hands to then use their training and knowledge and experience to do with that information what they will in term. Yeah, it's so tough. Yeah, it's like the, similar to like pipelines that we use. Like, it's like that trade off between like specificity and sensitivity. Like, you can have 100% sensitivity if you, you don't care that you've got like, you're never going to miss anything if you don't filter out anything. So you'll catch all of the false, like, you'll catch all the true positives if you're willing to accept every false positive. And obviously you want to like, minimize. Like, you want to make sure you're like minimizing false positives and increasing true positives and like all that. And, but there's, there's usually like a, a point where you're like, okay, now it's like, when would sensitivity push like specificity the wrong way kind of thing? Like, when do we lose specificity because we're trying to be too sensitive to too many things. And I think that's the same here where you're like, okay, we want to be quick and efficient, but we also need to be accurate and we also need to be cautious. And now where's the balance between those things of like, if you're super, super cautious, you're probably not going to be that quick. But if you're like super quick, you might not be that accurate or cautious. And then you're like, okay, now where's like the sweet spot where it's still pretty quick. But we're not like just fully negating true positives that we probably should have been paying attention to. I think that's in terms of the actual kind of coding and training of it. Maybe I'm naive because I haven't had to do any kind of training of any sort of machine learning models, but I suspect that would not be as difficult as figuring out what you yourself or you as the researchers figuring out what the sweet spot is. I think once you know it, it should hopefully be okay to provide to the algorithm. I think it's figuring out what it is, like how much risk you're willing to take basically.",
    'Yeah, I like your point about like you mentioned how you should have kind of two possibilities, maybe one the most likely and one the most fatal or like most serious. And that way a clinician knows they should probably treat for this, but maybe also check for this just in case. That was like a, a nice point that I think.',
    "Yeah. And maybe it's maybe the most serious one is like a. You might just want to like ask a couple follow up questions just to really rule this one out. It's probably this, but just FYI, this is a possibility, a slim possibility, but maybe you want to ask a couple follow up questions.",
    "The next question I have for you is what kind of training or human oversight do you think should go into building and testing these AIs, like whether they're simple injectors or any sort of AI system to avoid any sort of gender or racially spice.",
    "I'm sort of imagining like, and maybe it's top of mind because. Do you still get emails from SickKids? Yeah. Did you see the like one about sky yesterday, like to kids AI?",
    'Yeah.',
    "So, because that's, I think that's top of mind. It sounded like they almost had like a three pronged approach and one of those prongs was like the sky service where they're like, we have experts here. If you want to implement AI, come talk to us. And we will, we have people who will be able to help you do that. Like, well, but also ethically and making sure that we're like HIPAA compliant and private and like all these different things. And so I'm imagining like there being some sort of on like every research team that or like any sort of like team that's trying to make AI. I feel like there should be like, I say this because I am one, but the science nerds, like the like computer people who are like doing the, the actual like coding but there needs to Be like that. Almost like a review board to be like, okay, with our expertise? Is the tool you made actually giving results that makes, like, biological sense that are, to the best of their ability, void of these, you know, biases? Or at least, like, makes them very clear. Because, like, like I said, like, with the sensitivity, specificity, it's not as though, you know, every pipeline that we operate has, like, 100% specificity and 100% sensitivity. That's often very difficult to achieve, if not impossible. But the limitations or, like, the what the, you know, false positive rate is, the what the false negative rate is. If there are certain gaps that, like, certain things that we're not able to detect, we make those really clear. They're huge disclaimers to anybody who is, like, using our tests. And I think that would be the thing where it's like, okay, we need to make sure that we've got some sort of advisory board that is, like, testing and making sure that, like, to the best of the ability of, like, them as experts, we're not including all these biases. And if there is a limitation, like, because maybe there's biases, because, again, like, the research that we've done in the past, like, is full of biases. Maybe there's not better research to be giving. But we've decided that the benefit of the tool outweighs the risk. But you then need to have a huge disclaimer about the risk or the limitations there. Like, if for whatever reason, which I doubt, there's no possible way to make one of those Pulse readers work for different skin colors, I strongly do not believe that's the case. I believe that we could do it. But if for some reason it's impossible, then that needs to be very, very explicitly stated of. This will not work, or at least it will not work as well for these. We can't just, like, pretend that the biases aren't there.",
    "That makes sense. At least being transparent and acknowledging those biases so that the healthcare professionals or even the normal people that are using these things will have an idea that this isn't 100% accurate. So just take it as a ballpark.",
    "Exactly. I think that's. I feel like it's like the PAN Canada, like, AI for health kind of thing. Like, basically the government of Canada have put out their kind of guidelines for AI in health. And I think one of them was transparency. And like, obviously other ones like that we've talked about, like, diversity and making sure there's minimal bias and ethics and stuff like that. But one of Them is like transparency, I think, because AI is like the hot thing and therefore the hot thing, like, makes money because people are interested in it. I think that that initial desire to make something that is proprietary or that's in a black box and like, that's yours, that you can make money off of it, I think, in my personal opinion, is a bit at odds with like, health care. I think when it comes to health care, there needs to be transparency and that's not the place to be like, in a big, like, I understand that, like, yes, in order to keep the lights on and keep making these discoveries, like, you need, like, companies need to make money and blah, blah, blah. But I think having these sort of like, proprietary black box, like, so that we can maximize profits at the risk of. Okay, well, there's not transparency. So the people using this product might not know if they're using it right. Or like, might not be aware if there are limitations, I think is an issue. So I think, yeah, transparency regulations.",
    "And your idea of like having an auditing kind of board is also sort like having someone after like all the developments done, maybe checking ethically doesn't make sense. Does it also make sense, like, biologically or whatever other cases and just having like a fresh pair of eyes? They're kind of like also working towards the same goal to help, like, verify that, you know, you didn't introduce any biases that you or your team thought of because then you have like another team also checking it out and making sure that it's good to go.",
    "Exactly. I feel like I'm a big proponent of like, multidisciplinary teams. Like, like, when. Even when it comes to like, you know, pipeline design, I may not, like, I think I could, I can tell you some things, but I may not be the best person to tell you, like, the best optimizations, like, for XYZ pipeline and like the, like the, the best coding practices for this particular pipeline. But I can certainly tell you that, like, if the results coming out of it are biologically, like, if they make sense kind of thing. And so I think you need those different people. Like, you need the coding experts, you also need the biology experts and like the clinicians and things, but you also need people who understand, like, legal and ethics. And then you also need people who, you know, whether it be through research experience or lived experience, like, have things like biases like diversity, having that front of mind so that this is something that they can kind of like check and yeah, audit before we just send it out in the world is I Think a good idea.",
    "My next question for you is, what do you think it feels like when a patient's denied care not because of medical necessity, but because of an algorithm classifies you as that way? And how can future healthcare professionals be trained to recognize and challenge biases like these in medical systems or software and just like, not just keep doing what, what's been done to actually, you know, think and challenge that Maybe some of the things that some of the tools or the processes that they're doing might inherently be bad.",
    "Yeah, I think that's, I think it's like it'd have to come from both, like the individual, like clinician type things, but it would also need to come from like the, the environment that they're working in, basically, or like the, the company or the organization that they're working with. I think organizations can do a lot by, you know, promoting dei, making sure that there's equity and inclusion within the workplace, ensuring that, like, employees have to like, do like, DEI training, like, making sure that that's in place and then, you know, because hopefully clinicians or like, whoever the employee is is going to be coming like, in good faith with the hope that they're trying to learn and get better. But like, even if they're coming in with like, I don't have any biases, like, at least make them take the training and they can, you know, just like, I think sometimes it's like knowing what the limitations are, Knowing what your limitations are is, does a lot. But unless you're kind of like prodded to like, you're like, a lot of people aren't going to look at that, like, they're not going to kind of interrogate their own biases. So you kind of need to give them the opportunity to do that through like, mandatory practices and then. Yeah, just sort of. Yeah, I think a lot of it comes like from like the system or like the organization down and like fostering and that, that environment where like, that's just sort of like, just like how you'd have like best coding practices. Like, you know, you're going to comment your code, you're going to like, do proper, like, like commits like to your repos and stuff. I think making sure that the employer is fostering these best practices when it comes to working with patients, especially patients of marginalized communities, and implementing these good listening skills and interrogating biases. If they're offering that training and setting up those best practices and modeling that, then I think the employees are going to be following suit and that'll Just be better for everybody and then it's just kind of like a lived practice thing.",
    "And now the final question I have for you was if you were on a team creating a new clinical algorithm, what are some like safeguards or must haves that you'd recommend to ensure that the AI doesn't cause harm to any demographic group, even unintentionally.",
    "So these be safeguards like as I'm making the tool or if. I'm sorry, if I'm using it already.",
    "If you're making one.",
    "If I'm making one, yeah. I think I would probably take almost like the CIHR model where I'm like from the beginning, like I'm imagining like when you sort of map out a pipeline and you're kind of like here's like the architecture, like the workflow flow doing that as well for like the design. When it comes to like the kind of what data is coming in and what you're, what you're expecting to kind of get out type of thing, I think starting with that fundamental and like making sure that you're bringing in like a wide variety of data would be like my, that's number one. And if you're, if we're not doing that like then we can't move forward with building this tool. But then yeah, I think it's also, I don't know exactly at what points, but there would. I think there's probably just like some stop gaps. Like I suppose often with AI like the, the idea is like it's kind of a, you don't see the middle. It's just like you put something in, you get something out. You're not seeing kind of like the steps, but like as you're building it, if you're able to kind of like put these stop gaps in and be like, okay, from like step one to step two, like did anything weird happen here? Yes. No. And like make sure that you're kind of, you know what is happening and like what the, like what the algorithm is doing. Because I think, yeah, sim, like it would be the exact same for like the, the clinicians using it. Where I'm like, you want to know what the tool is doing. You shouldn't just accept that like, oh, it's like a black box. Like you shouldn't just accept like that. I guess that's just like what this tool does. And so I think for you as like the developer making sure that you know what's happening at like each step and why and if you think that's fine or not would be important. And then, yeah, just lots of rigorous testing. I like, at the end, I think on like a wide variety of people, I think sometimes again, because it's probably not like super well regulated, we're able to just make these tools and then just put them out there. And I don't know how often like how well they've been tested or like on what demographics. So I think having a sort of, I'm imagining almost a bit like, like a, A bit like a drug trial where you're like, okay, we've made the drug, but now we've got like phase one testing, phase two testing. I don't know if it would need to be like as rigorous, more or less. But having that sort of planned approach, lots of diverse data, checking it as you go, and then having like maybe some phases of testing and auditing to be like, okay, is everything looking like we'd expect? And then kind of then when you're feeling confident being like, okay, we can release it to the world.",
    "Yeah, that makes sense. It's like a really good, like holistic, detailed view taking things step by step, it really improving and just making sure you know that what's going on at all times and you're not introducing biases because you're taking things step by step and making sure that you understand the way the AI is kind of thinking.",
    "Yeah, I do think that'll also take like the sort of the industry and like society incentivizing that. Because again, if we're going to go with like the kind of capitalistic, like, like the move fast, break things kind of vibe because that like just get it out there and that's not even like a tech, like a tech exclusive thing. Like even in the research community, like there's the, like the getting scooped where you're like, you want to get your research out there and publish before somebody else who's in like a similar field to you hones in on the same thing maybe and publishes first. But sometimes you don't want to be rushing your research. And so it's like, yeah, I think we also just need to be incentivizing people to take it slow and go the iterative route, especially when it comes to health and not incentivize. We'll just like get it out quick and be the first one. Because then you get to be the first one and you get to be like the, the innovator and you get to be. Because like, I think innovation is important, but I also think especially when it comes to health, maybe taking a measured approach is helpful. It's just we don't really incentivize people to do that.",
    'Yeah, that was a good point. Like we should be invent incentivizing more of good practice versus capitalization, especially in healthcare.',
    "Yeah, exactly. There are certain things like that, I guess, COD speed, like just make. Try to make your stuff quick and make money. But I was like, I don't think healthcare should be in the place for that.",
    'Yeah.',
    'Yeah.',
    "And that's all my questions for you for today. I really appreciate you taking the time to come and share your opinion on these topics. Do you have any questions for me or any closing remarks you want to make before the end?",
    "No, I think. Thank you very much for including me. This has been a very interesting conversation. I've really enjoyed it and I hope some of what I've been able to provide has been helpful. But yeah, no, I think this has been great. And you're clearly, I think, very well versed in the topic. And so I'm confident you'll be able to put together. I assume this is for like a project of sorts. I'm sure they'll be able to distill all of these interviews into something very insightful. So, yeah, thank you for including me.",
    'Thank you.',
    'Awesome. I will chat to you later.'
  ],
  'pedro-ballester': [
    'Okay, so hello Pedro. Would you like to start off first by introducing yourself? Please tell us your name, what degree you have, where you currently work and where your research interests lie.',
    "Oh, sounds good. Yeah. I'm Pedro. I'm a machine learning specialist at SickKids. I have a background in computer science and a PhD in neuroscience. And right now I work on cancer subtyping and classification of existing subtypes based on ML.",
    "Sounds good. Thank you for introducing yourself. Today's topic will be about how biased clinical research data affects healthcare practices. Our interview consists of specifically main 6 main specific cases where gender, where bias clinical data led to AI systems making incorrect decisions and it specifically stemmed from gender and racial biases. The first three cases will focus on gender based biases, whereas the last three cases will focus on racial based biases. Overall, the interview is going to be formatted in such a way that at the beginning of the six cases I'll give you very little information and then I'll kind of try to put you in like the spot of the AI and you can try to guess what the problem is and you can then get a sense for how the AI kind of made a bias, the bias prediction that it did. And afterwards I'll give you the full details about the case, analyze the impacts of it and then maybe how we can move forward to prevent the AI for making the same mistake again. And then finally at the end, after all the six cases, I will be just asking some overarching questions about just you know, reflecting questions about how clinical data affects marginalized communities and what we could, what you suggest that we could do. And if you're tasked with like making some sort of similar system, what would you try to include if you were designing one? Sounds great. I'll start off with the first case. I want you to imagine you're a physician treating a patient with severe abdominal pain and you see no visible injuries. What kind of diagnosis would immediately come to your mind? What additional information would you ask for? This is kind of like an open ended question. You could say whatever. If you don't have too much of an answer, that's okay too.",
    'Are we thinking specifically about a person diagnosing or a model diagnosing that person coming in?',
    "Yeah, I think you think you're just a physician and then like you're trying to diagnose someone who has severe Donald pain but you can't see anything visibly like an injuries and just think of it like this perspective, maybe your physician will like the first steps be what do you think the problem Might be.",
    "Yeah. No, no, makes sense. I mean, of course the main thing that will come to mind is there's some non visible issue going on, something that an X ray or an MRI or a CT scan could help identify. But I'll say abdominal, it's probably very, very broad, so I wouldn't go as far. Yeah.",
    'And what if I told you that they were a woman? Would you have any sort of like diagnosis you would have been towards or anything you can do there?',
    'Yeah, I mean, I guess in that case you probably need to consider pregnancy as well. Right.',
    'And then if it were a man, would your diagnostic thinking change compared to a woman? Would you consider other kind of maybe general diagnoses?',
    "Great question. It's also so far out of my field.",
    "Yeah, no worries. It's just like diagnostic questions you could answer with whatever.",
    'Yeah, of course. Not that it comes to mind maybe, but that applies to both just understanding the nature of their work a little bit more about what they do. But yeah.',
    'Okay. And then how would your decision be impacted if you learned that they were a transgender person? Would your clinical reasoning be different? Would you try to tackle the problem in different ways or would it be kind of similar to how you would treat like a binary man or woman?',
    "Yeah, I guess in this case here it's, it's more so about the possibility of pregnancy that would go around the decision of asking more about that, regardless of like, identity, I think. Yeah.",
    "Yeah. So you had the right idea. The. The case is about pregnancy. And I'll summarize the case now and then we'll talk about the impacts of the case. So there was a 32 year old transgender man that arrived at an emergency room and he had severe abdominal pain and he informed the staff that he was transgender. But the hospital's electronic health record in the system list to him as a male. As a result, the clinicians failed to consider pregnancy and misattributed his symptoms to factors like obesity. And the patient was in fact pregnant and experiencing labor complications. And due to the, due to the delay in the clinicians recognizing this, urgent care was postponed and tragically the baby was stillborn. And so this case kind of highlights how binary classifications and health workers, whether like male or female, combined with assumptions from clinicians themselves, can lead to misdiagnosis. And standard pregnancy related alerts would never usually be triggered by an AI system because in system it's registered as male. And this kind of highlights the dangers of algorithmic bias and the need for more inclusive healthcare systems that reflect the reality of transgender and non binary people in today's day and age. So in your opinion, now, knowing that the person was transgender and pregnant, what role do you think these biases, the system design, or any other factors you could think of played into the AI's failure to identify pregnancy risk in this case?",
    "Yeah, that was a great point that I think there's a lot of work happening now on figuring out better ways to classify that in the systems. And I think there's already some hospitals. If not. Yeah, I don't know how far the progress has been. And that separates no sex at birth from other gender and sex related fields so that you can make informed decisions like that. And I think then we just need to figure out how the systems built on top also get to change with the changing landscape of the variables that we use.",
    'Yeah, that makes sense. And then what changes would you suggest to, like, electronic health records, triage systems, maybe training for these clinicians to prevent similar outcomes for transgender patients in the future?',
    "Yeah, I mean, for certain, like the automated systems need to account that for that. It's crazy that they didn't. And that's again, I think the main thing is to separate those fields and to have a little bit more of a complex pipeline. The overhead is not super complicated anyways. It's just about figuring out exactly, you know, how to flag the right things. So I think it's just a blatant mistake, an issue that is just as kind of an obvious fix in this case. Yeah.",
    "Okay. Yeah, we'll move on to the next case now. So imagine a 59 year old patient presents with sudden chest pain and maybe a bit of nausea without knowing the gender. What would your talk diagnosis be? What would your next steps be.",
    "On chest pain and nausea? I think cardiac issues for sure come to mind for the first part. Then you have to ask what were they doing at the time when the pain started? The nausea component? I'm not too sure about the differentials here, but yeah, that's what comes to mind.",
    'Okay, yeah, that makes sense. And then would you take the problem seriously right away, or would you think that chest pain for a 59 year old is likely not that serious?',
    "I know that's serious for sure.",
    "And then now let's say you learned that the patient is a woman and you're told that statistically women are a lot less likely to have heart attack compared to men. Would this influence your decision or like the things you do in any way? Do you think it should have? Whose decisions?",
    "That's, that's, that's a great question because there's. The stats sets for this are interesting because they sometimes they, they don't reflect the patient. Right. So if you build a system that's purely based on prevalence, you lose a lot of deep individual information. Right. So from a global health perspective, in terms of thinking about policies, then you can think of if there's a, you know, if males experience X disease more than females and females experience Y disease more than males, you can think about targeting those of like, you know, outreach programs, things like that. But I think from an individual perspective, once they're in the hospital, those tests, they kind of like are almost irrelevant because at the individual level they don't really matter that much. And you just need to think about the actual symptoms that are currently going on.",
    "Yeah, I like that point. And then now let's say that they were a man and you know that men are statistically more likely to have heart attack than women. Would you have a change in your sense of urgency? Like, would you be more alert if it was a man and you know that men have statistically high chance of having a heart attack, or do you think it would be the same as whether it was a man or woman and they come with chest pain to you?",
    "Yeah, I think realistically it does affect care. Not saying that it doesn't. I think it does affect the way people conceptualize a case, but I think in this particular case it should not affect the case. Yeah.",
    "Okay, now I'll summarize the case and then we can talk about the impacts of it. So there's an AI power, an AI powered symptom checker app that provided dangerously different recommendations for a man and a woman with identical symptoms. Both were 59 year old smokers and they reported sudden chest pain and nausea. The only variable that was different between them was gender. The male patient was warned of potential heart issues, including unstable angina and heart attack, and advice to seek emergency care. The female patient, however, was told that her symptoms might be due to depression or anxiety and so no urgent care is recommended. This discrepancy highlights how AI tools can replicate and amplify gender biases, potentially leading to delayed treatment or misdiagnosis. In this case, the AI appeared to rely heavily on statistical trends and underrepresented heart attacks in women and overlooked the real serious risk of cardiac events in female patients. And public concern grew after this example came to light as it revealed how gender stereotypes that women are anxious and men have heart attacks can shape algorithmic decisions. Ultimately, the case highlights the critical Importance of designing healthcare AI systems that recognize and adjust for biases. Life threatening conditions should always be considered for all patients, regardless of gender, especially when symptoms are even shared among the different demographics. So how should an AI triage system be able to, able to handle gender based statistical differences? Should it present maybe like all serious possibilities regardless of probability, like even though women are less likely, should still present that as a possibility, especially when it could be related to fatal scenarios? Do you think maybe could be a better way to represent these statistical differences for different demographics of people?",
    "Yeah, yeah. Well, right now it's certainly the case that the prevalence and incidence of diagnosis, even if they were problematic to begin with, they're going to be inherited by the models because they're trained on top of data that's flawed to begin with. I think from what I've seen in recent research, and I think it does make sense is they kind of have to calibrate the models across, you know, gender, ethnicity, sex and all these other variables to try to be as fair as possible and somewhat ignore the baseline statistics that you have in the training data to try to overcome some of these historical biases. Of course that comes with caveats, right? Some, some of these could be relevant, but I think if you, as long as you prioritize what the most urgent things and most high risk scenarios could be and you go from like a risk mitigation for the, for the patient perspective, I think you should probably be on the safe side.",
    "Okay, yeah, that makes sense. And in your opinion, do you think the AI's response in this case was defensible given that it just follows statistical trends? Or should priority have been given to patient safety over just data driven averages? Where's the line we can draw between data and ethical care? Do you think it failed in its duty to kind of provide safe recommendations? Like, what's your overall thoughts on just the overall implications of its decision?",
    "Yeah, I mean the AI, its output is basically what it was trained on and in that sense it was an issue that was carried on. So I think that now that we know that this is an issue and the AI is basically allowing us to see that this is consistently an issue and it has been consistently issued in the past, it's just time for us to fix it. And I think defensible as in it was already being done like that and it continues to be done like that, I don't think that's a great defense. If we know it's wrong, you just need to fix it. But it is difficult, you know, from a Technical perspective, it is difficult, but at the same time, it's important. So it is what it is. Right.",
    "Okay. And now we'll move on to our third case, which is the last one for gender biases. So let's say a young patient around 25 years old presents a lump in their chest. What key factors will kind of guide you for your diagnosis or decision to recommend further testing?",
    'Can you repeat the case, sir?',
    "So let's say a young patient, maybe around 25 years old, presents with a lump in their chest. What key factors will kind of guide your diagnosis? What kind of further testing, maybe, would you think of.",
    "Yeah, I mean, I'm guessing if you found the lump already, unless it was visible somehow, but you should probably already do a CT scan and mri and depending on the edges, do a biopsy and see what's happening there.",
    'And how might your approach change if you knew that the patient was female? Would you have any sort of diagnosis that you lean more towards that you think it might be, versus the mom?',
    "Well, in that case, I'm guessing we're probably talking about the chance for breast cancer. I mean, that's certainly something to consider, but at the same time, if you have a lump, you need to check it out anyways, so I don't know if that would affect care in any way.",
    "Okay. And if there were a male, would you consider the same. Similar things as women's chest pain, or would your approach be, like, different. Would you consider generally different things? It's just a general question. You don't have to be, like, super specific.",
    "I mean. I mean, I think here there's, like, probably the distinction between sex and gender, but generally you would consider cancer as a possibility in both cases, maybe the subtype could be potentially different. And then here, sex and gender might play a role. What are the subtypes? But not knowing anything else about the patient, I would say yes.",
    'So would you still consider breast cancer if they were male?',
    "It depends if it's a blur or not. And. But also. Yeah, I don't. Yeah, I. I don't know. I don't know. I don't think I know enough about all that to make any.",
    'From opinion.',
    'Yeah.',
    "Okay. And then now I'll tell you that breast cancer is possible. Men. Like, it's rare. It's less than 1% of all breast cancers occur in men. Should that rarity affect your.",
    "That's crazy.",
    "Yeah. Like, biological men, like, not. Not transgender. It's like, less than 1% of all breast cancer cases are in men. So now, knowing this Would this rarity affect your decision and the things you test? How would you balance the risk of over testing versus missing a critical diagnosis? Any thoughts there?",
    "Yeah, well, I think it does affect in the sense of again the urgency. So you're going to look at the more urgent and obvious ones first. But then at some point that's still an urgent one. It might be, you know, lower on the differential in terms of probability. But if it's still worsened, it should still like be discarded. Right. Or rather if it's still, if it's still serious and harmful. Right. So I think it would maybe potentially move down the line in terms of priority, but it should still be assessed.",
    'Okay.',
    'Yeah. Okay.',
    "And then I'll summarize the case and we'll talk about what happened. So this is a case of Raymond Johnson. He was a 26 year old man from South Carolina and he was denied coverage from his health care health care insurance for breast cancer treatment solely because of his gender. Although diagnosed with breast cancer, he was ruled ineligible for a federal medicad program that covered patients screened through certain government programs. And the government program in this case only screened women. As a result, the insurance algorithm automatically excluded men, leaving Raymond without coverage for chemotherapy and the surgery that he urgently needed. This case kind of exemplifies how gender biases embedded in policy and algorithmic systems can have life threatening consequences. That's crazy. Yeah. And the program failed to account for the fact that men can also develop breast cancers, although extremely low chance, effectively barring males from life saving care that they need. Advocacy groups including the ACLU condemn exclusion of discriminatory and push for reform. The case prompted broader conversations about the need to redesign healthcare systems and policies that are inclusive of all genders and reflective of real patient populations concerns. So yeah, that was like a crazy case. And then I want to ask you now, what role do you think like societal assumptions played in this case? For example, like breast cancer is like a woman's disease. It's like pretty well known. I guess it's like pretty not well known that men can also have breast cancer. And like how overall, how do you like to think this kind of narrative played in the algorithm decision to deny Riemann?",
    "Yeah, I mean it's crazy. I didn't know that, you know, considering I work at a, at a cancer research lab. But it's wild. Yeah, the insurance thing is a whole other conversation. Insurance is beyond broken anyways. And of course we need to if we're going to have people making decisions like that, creating systems like that. It needs to have some sort of accountability. And the people that actually make the decisions need to be very knowledgeable in their fields. And I think that doesn't happen quite often, especially in insurance. They just kind of make decisions and then that's that.",
    "Yeah, yeah, it's kind of like they try to benefit themselves the most, I guess.",
    "Yeah, exactly. They're like, whatever.",
    "Yeah, it's crazy. So maybe what changes would you suggest to healthcare algorithms or like these policy algorithms to prevent gender based denial of coverage in similar cases? How can we ensure that all people, different genders, have equal access to treatment that they need?",
    "Yeah, I mean, I think one of the things is like for sure, if you're gonna have an automatic fail on, let's say on a disease, in this case breast cancer, based on sex or gender, you need a very, very strong rationale for that. And I'm sure if you actually had talked to a doctor that works with these cases, they would tell you that it could happen. So as long as you have the checks and balances in the insurance to make sure that their decisions based on whether it's again, all the social demographics, they're making any decision based on that, it needs to have an absolute, solid and irrefutable reason for it, you know.",
    "Yeah, that makes sense. And then should healthcare systems prioritize statistical likelihoods and trends or should they be designed to account for like these rare but serious conditions? Especially when there's lives of stakes. How should we kind of strike this balance of statistical likelihoods and rare but serious cases?",
    "Yeah, I'm biased because I work in machine learning and the whole point of machine learning is to get out of statistical averages and go into the individual, especially in healthcare. So I would strongly argue against statistical averages. Generally speaking, if you take an average and you apply to an individual, you're wrong. That's the case 99 of the time, although. 99, you know, average. But it's, it's. Yeah, no, we should absolutely carry the just decision based on sets. Stats are useful for outreach or maybe public programs and stuff for, you know, talking about risks and stuff like that. And maybe, you know, to help you understand what you should eat and shouldn't eat in terms of like long term health and all of that. But when somebody's in a clinic, those things kind of like go out of the window.",
    "Okay. And that's all for the gender cases. Now we'll move on to the, the racial bias cases. So for this is the fourth case now, I want you to imagine you're a healthcare Provider evaluating a patient with kidney disease for transplant eligibility. What clinical factors would you consider when assessing disease severity or readiness for transplant? It's okay if you're not like super knowledgeable in this space, just any sort of rough ideas.",
    'Eligibility for transplant. So, okay, yeah, so I think the decisions of a lot of it is based on lifestyle and age. Right. I think those are the main factors for eligibility, but I could be mistaken.',
    "Yeah. I think Scott also mentioned when I talked to him that it also matters like how close your kidney matches to other people's kidneys and a couple other factors.",
    "Yeah, that's fair. Yeah.",
    "Now the next question I have for you is, have you ever heard of race based modifiers in kidney function scoring such as egfr? What do you think the original attention behind this including race in these calculations is? And I'll give you a little bit more background about it from the case.",
    "I mean without any background, I would say that it, if it's being applied, it should be based on the chance of not rejecting the transplant. If there's any other reason then that wouldn't be a good answer, I don't think.",
    "Okay, so I'll summarize the case now and talk about what that scoring, the race based score does and then we can talk about the implications of it. So this case was in regards to Anthony Randall. He was a black man from Los Angeles who was on dialysis waiting for a kidney transplant for over five years. What he didn't know was that there's an algorithm for the transplant system that incorporated a race based modifier that made black patients kidney scores seem way better than they actually were. This modifier caused Randall's kidney disease to be considered less severe than it truly was, but leading to his placement on the national transplant waiting list to be significantly delayed like I mentioned, for five years.",
    "Oh, that's what you mean.",
    "So in mid 2023 he filed a case against the hospital and the hospital was called Cedars Sinai Medical center. And he also filed it against the United Network for Organ Sharing alleging that he was unfairly deprived of a fair chance to get a transplant because of a racially biased formula. And apparently it was no secret that the algorithm have a bias because I think people investigated and researched this because I think it was a kind of a well known issue. And the board, the board of the transplant system understood the modifier was resulting in black patients illnesses being severely underestimated. And by early 2023, all hospitals were directed to stop the usage of race adjustment in black patients waiting times and we're and were to be changed and reflected how healthy the kidneys were would be reassessed to reflect the postponement. Randall claims that these changes should have came sooner. He could have already had the kidney that he desperately needed. This case highlights how the goal clinical algorithms are good, but the execution was not due to the insertion of a race based modifier which caused black patients to not receive the quality care in a timely manner. I don't know that exactly the full details put the reasoning behind it, but I think it was just because maybe I guess in this case black patients may typically get lower scores than they're supposed to for this. That's why they try to make it a little bit better so that it kind of matches the other races as well. But in reality what ended up happening is it made it way worse for black patients because then they'd have to wait five plus years, for example, and it was just not a good decision at all. And after some time it was kind of well known because I think as these black patients wait for years and years, people started researching to these kind of things. And it wasn't until he kind of brought up this case that it was taken eventually actually seriously by the companies that are using these technologies.",
    "That's crazy. I mean, I understand the rationale. I think it makes sense to try to induce fairness through variables, right? Because if you just, you know, if you have an algorithm that's trained on a particular population applied to another, they tend to fail. So you kind of have to try to adjust it to every place that you go to and the diversity there. But I think that was a lack in the research, right, like the research and the, the ongoing checks and balances that any algorithm should have, regardless if it's AI based or not. I don't even think this one is, is AI based. I don't know about it, but you need to actually check what's going on over time, prospectively. But yeah.",
    'Yeah. And knowing now that this modifier made black patients appear healthier than they were deleting, delaying their transplant eligibility, how do you think this affected patients like Andy Anthony Randall, like in these crazy five years?',
    "I mean, there's two things that are obvious that happen, right? One of them is that there's worse care for these patients for sure. And the second one is reducing trust in the system, especially for marginalized communities that already feel like the system is working against them. It reinforces that and correctly so. A lot of science sometimes tends to be flawed because of who are the people Making the science to begin with and what they care about and their own biases. Right. So those are the two things that come to mind.",
    "Okay, that makes sense. Do you think hospitals or national organizations should be held accountable when algorithms are known to be biased, but they continue to use it? What obligations do you think they have once they become aware of them or aware of the harm? I can maybe also give you an example that I gave to Scott where another example where it was known that technology was biased but still used was in facial recognition technology. Some policing systems would use facial recognition technology to be able to identify a person at the crime scene to a similar face in their database and be able to find out who did the who was there at the crime scene. And it was particularly biased against people that were of darker skin tones because it would more easily just match someone of a darker skin tone to another person of a darker skin tone. So then people would get incarcerated, especially black people would get incarcerated that were completely innocent. And it was kind of known that some of these facial recognition technologies are biased because their sensors are not good enough against darker skin tones, but they would still continue to use it. And this is kind of like an example. They kind of probably knew that it was a bias, but they continue to use it. It could have been maybe they have malicious intent behind it, or maybe it's just easier for the facial recognition technology. As soon as they can put a name to the system, then they can just blame the system if the system was wrong, and they'll just have their person in jail right away. It just makes things a lot faster and easier for them. So it could be the case here, too. So what kind of obligations do you think these hospitals and these national organizations should have once they realize that the system that they're using is unfairly prejudiced against marginalized communities?",
    "Yeah, for sure. Crazy. The first thing is you need to make sure the systems are being checked constantly. If there's an uptick in a particular demographic being targeted, or in this or in the case for the health care, it's like the care got worse for a particular demographic. You need to make sure that those things get monitored, like in real time. It's impossible to think of a system that doesn't do that. The second is, as soon as you find it, you need to address it. You need it to bring it to light. You need to make sure that people know that this is happening and you need to address it. That being said, it's like the first part is really an important one because you can't leave it to chance. You can't just leave it all. Maybe someday somebody's going to do a research and find out that this doesn't work. No, you need to actually be testing it. And there's like a lot of the keys and a lot of the time the distributions that go into the model are. What's the name? Forgot the name. But they change over time. I believe I forgot the name. Embarrassing. And because they change over time, you need to make sure that these models are retrained or any other algorithm is recalibrated. And if you don't have these checks, you never know that their accuracy drop. So the whole thing depends on the first part. Non stationary. Thank you. That's the next.",
    'Okay.',
    'Yeah.',
    "And in Randall's case, the board eventually mandated the removal of these waste adjustments, but it came years after the issue was unknown. Do you think the delay was acceptable? What should have been done differently to keep these companies accountable, these hospitals accountable, to making sure that they use, you know, like software?",
    "This is crazy. What are you talking about? Like, it needs to be like, like we do for bugs. You know, if, if you find a bug and you tell Microsoft, hey, there's a bug in your call and this is causing a security issue, you have 30 days to patch it. Otherwise it's going to be publicly known and that's it. And then it's publicly known. You know, it just gives you some time so that people don't game the system and exploit the bugs. Although I don't even know if there's effects here, but it's a very short timeline. You have to fix it and then it's done.",
    "That's a good analogy. I didn't think of that. And I guess you probably kind of talked about this about like retraining and stuff. I was going to ask how can health systems design clinical algorithms that avoid reinforcing historical and, and systemic inequities, especially those type of race. I've talked about, like how we have non static variables and being able to update them regularly and how they change. For as the population increases 100% the.",
    "Population, the distribution changes. But also it might have been a flaw in the beginning to begin with. So you just need to make sure that there's an active monitoring and that there's a check for the performance across the social demographic data variables and all those other factors. Postal code comes to mind, things like that.",
    'Yeah. And what steps, if any, do you think should be taken to make things like, right for these patients that were harmed by these Algorithms, like adjusting their wait times. I guess that probably happened once they reevaluated their kidneys with the new system. But besides that, what else can they do? Like issuing apologies, maybe offering compensation? Do you have any sort of idea of how they can regain trust with the general public that that was deeply affected by the system?',
    "Yeah, I mean, this person in particular, they're probably owed something because if they were directly affected by this flaw, I mean, the hostel should be liable, right, for, for errors and their agreements. It's their choice to use AI. It's their choice to use. You know, we can just not make organizations accountable for it. So that's one for sure. Like public. We don't do a good job with outreach and actually saying what's going on and being transparent about what kinds of models go into your decision making and all of that. You know, we need to make sure that people understand how things are scored and why they're scored that way so that they can also think about, hey, maybe this could be an issue for me for that reason and they can investigate.",
    "Okay, sounds good. And now we'll move on to the next case. This is our first case now. Almost done. Second, last one. Imagine you're treating a patient with chronic lung disease during a respiratory pandemic. If a pulse 6 ometer shows normal oxygen saturation, but the patient appears basically distressed, what would you do next? These are ideas.",
    "Great question. I mean, you have to check for other symptoms too, right? Not just that, but it is the case that a lot of the time, especially for a pandemic, the data is going to be very simplified. Right. Like you, you don't really get the diversity of case presentation because the incoming data is going to be from just a few places. You get American data from research hospitals that see a particular population and then you say this applies to everybody. So I think you need to see the case and if that person is distressed and investigate it further and see what's really going on.",
    'Okay. How much would you completely rely on the tools that I mentioned, the pulseometers in your clinical decision making, do you think? You know, you mentioned you would do other tests, but would you also ever consider double checking the accuracy of the medical device? Would you ever dealt with it? These are not here. Like, would you ever question the reliability of the technology you were using?',
    "Good question. I don't think I know enough about these, but yeah, I wouldn't expect something like that to be flawed. I think it's more so. What I would think about is more so like what are you doing the information that it comes from it, but yeah, I wouldn't think that.",
    "Okay, sounds good. So now summarize the case and we'll talk about what happened. So during the pandemic, race related biases in medical technology confronted Dr. Noah. She was a family physician and the CEO of Roots Community Health center based in Oakland. In late 2020, one of our patients was an elderly African American gentleman who suffered from chronic lung illness. One of the checks that were done previously in a previous visit, the pulse oxygenation check, revealed that his oxygen saturation levels were high. And now this. On this visit, when she checked, the device showed a relatively normal oxygen level. However, the doctor's clinical instinct was that it indicated that she thought that the patient was more distressed than what the pulse oximeter was showing. She conducted an arterial blood gas test which confirmed her worst fears. The oxygen content in the patient's blood was too low and he needed oxygen. Sometime later, she came across an article in the New England Journal of Medicine that confirmed her hunch. The pulse oximeters that she was using were unable to register low oxygen levels in dark skinned patients as compared to white patients. She and her colleagues were outraged by a device that was supposed to be helping their patients, but it was grossly inaccurate for the dark population. Finally, her clinic participated in a class action lawsuit against manufacturers and sellers of pulse oximeters for more detailed warnings and up to date devices. She didn't stand idle while demanding the FDA take pulse oximeter discrimination towards race very seriously. So it's a kind of example where like she, she doubted the technology she used and then she even took further action to even take lawsuits against them because they didn't have good labels, good warnings, or up to date software that they could have keeping up to date with.",
    'Yeah, I would have never guessed that the tech itself, especially for measuring oxygen levels, would, would be biased. So I guess my ignorance on that too.',
    "Yeah, this is, it's the pulse, pulse oximeters they use at hospitals that, you know, you. To put the thing on your finger, it's those. So those sensors I guess aren't that good for people of darker skin tones and it won't actually accurately get like oxygen levels and stuff like that from it.",
    "That's crazy.",
    "Yeah. In this case, the doctor's clinical judgment. Overruled. The device is reading. What do you think this says about the limitations of relying too heavily on technology without understanding its biases or like the underlying kind of idea behind this algorithm or things that these clinicians have to use?",
    "I mean, it's Good to the clinician. Trust their judgment there. I would say, I think because, you know, I bring my own biases of working tech. Right. But I generally trust technology, and I think I generally trust it more than I trust people in terms of. Because people bring biases too. It's crazy to think that they wouldn't. They bring their own and sometimes worse. But I think we really need to invest in figuring out what this technology is giving us and having the right. You know, if we're talking here, this is probably FDA approved. So FDA failed and they failed in checking how the tool actually works for people that are just not, you know, light skinned and all of that. So that's probably the population they had access to when they were using. They were creating the tool and then they were like, oh, it works for them. It's all good. I mean, that's not enough. So, yeah, I think it was a fail of the FDA in regulating how much testing you actually need.",
    'And pulse oximeters were found to overestimate oxygen levels in patients with darker skin. Why do you think this design law was able to persist for so long despite the risk it posted it posed to patients of color?',
    "Mm, I think for the most part is because generally what they care about is the original approval and then test kind of disappears. No, nobody's really testing anything that's going on now. This is changing because AI is becoming more popular. So we need to test things more. But ongoing tests are kind of like rare. And so I think if, if the original process of approving something is flawed, that problem is going to go undetected for a long time.",
    'Yeah, that makes sense.',
    "And also the power dynamics too, where you're going to have, you know, the medical doctors are going to be on average for a particular group, and if that group is satisfied with the tool because they don't see any visible issue with it, they're unlikely to report.",
    "Okay, yeah, that's a good point. Okay, I think you talked about this one, but I was going to ask, do you believe the FDA and manufacturers haven't done enough to address this issue? What more should regulatory bodies be doing to ensure devices are more accurate across diverse population?",
    "Yeah, I don't think the FDA has done enough, at least as far as I'm concerned. But yeah, we need, we need to make sure that it gets tested across all sorts of different groups. And, you know, and I can see that being like, complex in some cases, but then maybe we need to invest more in like, bringing those tools into the hospitals and actually seeing how they Work for the population they will actually see at a place instead of just like small siloed research groups and then over time provide that approval with the correct checks and in production, let's say.",
    "Right, yeah, that makes sense. Do you think future healthcare providers should be trained to detect and challenge device based bias that they might see in their patients when they're caring for their patients?",
    'Great question.',
    'I mean, you know.',
    'Oh, go ahead.',
    "Sorry. I was gonna say like a lot of times when we're in undergrad, like we use tools exactly how they are, how they are, we don't really question them. Do you think it should be like a good idea to kind of question it sometimes? Especially maybe when it's newer technology or from like companies that maybe aren't super well known in the healthcare space.",
    "I mean being cautious is always a good thing, right in those cases. But I don't know, I think we need better systems for checking these tools and maybe MDs could learn a bit more about them. But I think it's unrealistic to think that MDs are going to become experts to the point they can really question complex tools because they have other stuff they need to learn. So I think that the way we build the systems is that medical care is becoming increasingly complex and we actually need to change the way that these approvals happen to include experts from other fields. But you see a lot of hospitals, decision making ends up being on the hands of MDs alone. And then you have all this tech stack that goes into a hospital that has no balance, no checks, nothing. And we need to actually bring more people that are technical in their own field to these places where they actually get to make decisions and check and all of that. So I think it's a bit of a slightly different approach that I would take personally.",
    "Okay, sounds good. And now for the last case. So I'll ask you, if you purchase a health monitoring device like a smartwatch, like an Apple watch, for example, what level of accuracy and reliability would you expect? Especially when it comes to tracking critical health metrics, for example blood oxygen levels, heart rate, whatever, other things.",
    "For these metrics I would expect a pretty high. If it's something newer, they're detecting cardiac event or something like that, then I think, okay, that's very experimental. But for a metric that I can use I would expect very high.",
    "Okay, so you'd expect like it would be pretty confident in kind of a basic everyday metrics kind of things?",
    "No, like I would hope that it's like accurate but I know that they're flawed, but I would hope that they're accurate.",
    "Okay, and how would you feel if you discovered that, for example, that smartwatch didn't work as accurately for people with your skin tone or demographic?",
    "I think I wouldn't be surprised because I know that they're flawed, although I didn't know that they were differentially flawed for different populations. But it is for sure a problem.",
    "Okay, yeah. So I'll summarize the case now, but the idea behind us kind of, I think, similar to the pulse oximeters, you can't really sense with darker skin tones as well. Okay, so this case is for Alex Morales, a New York resident, and he brought attention to a case of possible racial bias in consumer health device Apple Watch. Morales, who has a darker skin complexion, bought an Apple Watch with the expectation that its blood oxygen sensor would accurately lock his oxygen levels for fitness and health purposes. To surprise, he later found out that the device's oximeter may not work well with people of his from his demographic. In late 2022, Morales initiated a class action lawsuit against Apple for allegedly containing a blood oxygen app that was racially discriminatory and did not function as promised for non white customers. Supported in part by complaints of other studies claiming that more advanced pulse oximetry devices are massively less useful in people with darker skin, Morales asserted that Apple owed the public an explanation. After all, paying smartphone users assumed that the device would be equal for all its users, which was not the case. While the judge dismissed the case in 2023, it did start an important discussion r[] products. This case showed that the show the world that they are in fact indirect biases in everyday medical equipment. This shows us that Alex Morales and the rest of the community are still subjected to discrimination based on race, even in technology they choose to use. Moreover, it demonstrates that a keen eye for responsibility in the tech industry has its place in these situations. Do you think companies like Apple should have an obligation to disclose maybe the limitations of these health sensors, especially if those limitations affect certain racial or skin tone groups?",
    "Yeah, for sure. I mean, if a product's not going to work for you, you should know. I would say yeah.",
    "And do you think it's possible that maybe they already do give this information, but maybe some, like in the terms and conditions, like, you know, the things that no one reads, it's possible it could already be there? Maybe.",
    "Yeah, I wouldn't be surprised if it's already there. But that's not enough. Right? Like that's, that's the whole point nowadays, are we review Reviewing what we mean, like when a, when a person consents to what terms of conditions or, you know, whatever is in the privacy policy. Like, it's not enough. You need to actually show them the pieces that matter to them and see if they agree with that, you know.",
    "Yeah. Why do you think skin tone biases and accelerators and just wearable to health devices hasn't received widespread attention until recently? Maybe. What are some of the biases that you think could be to addressing these concerns? It is in a way similar to the last question I asked for the last case.",
    "Yeah, yeah, yeah, for sure. Oh, I think, you know, a lot of it ends up being the people that are designing it that, you know, when people design things, they don't count for people that don't look like them a lot of the times, I think. And then if a lot of the tech. It's not diverse. It isn't. I mean, maybe more than some fields, but certainly not particularly diverse. Then here you miss out on actually figuring out how to address everybody's needs, I guess.",
    "Yeah. When I was doing the research for this kind of project, we came across like research that said that a lot of times when this clinical, like research happens before they launch products, it's usually done on predominantly white people, maybe some of other demographics, but the predominantly have white people. So then it's always not like fully accurate for every single demographic. Before even release, there was a, there's.",
    "A say in your imaging, I don't remember nowadays that what the knowledge that we have of neuroimaging is the knowledge of American students. You know, because you open a research study and you're like, oh, I'm just gonna open for everybody. And then you open at the university and then who sees it? Our university students. And then they sign up and then they get their scans and then you make copies. Like extrapolated decisions based on the data or very specific group.",
    'Yeah. So in this case, the court dismissed Morales case. But do you think tech companies should be held accountable in other ways? What kind of responsibility should they carry when their products are shown to underperform for our marginalized communities?',
    "Yeah, I mean, I think disclaimers in terms of like, whether the product is going to be useful to you to begin with. You know, because I think, you know, if there's a difficulty in the technology, I think you can make an argument, hey, like this product only works for this group, you know, and then you can make one that's better for people that have darker skin. And you can say, hey, if you have Darker skin. Pick this one instead of that one. I think there's no problems like necessarily with having a product that works in a subgroup. That being said, you need to alert people, hey, this won't work for you, you know, and pick this one instead. And actually put money into figuring out something that works for, for others too.",
    'Yeah, makes sense. Transparency would be nice.',
    'Yeah.',
    'Do you think consumer tech devices that have health features, for example, smartwatches, should be subject to the same, like regulatory scrutiny as medical devices? Why are not?',
    "That's a great question too. I think same level is strong. I think that's a very high bar. And I think the issue with that is that it takes a lot of money to get something approved by the FDA and other regulatory agencies. And what would happen in practice, I think, is that you'd have only these top companies being able to do that Apple wouldn't be able to do. You know what I mean? They have the resources to do it. But then you limit the growth of all sorts of startups and even like you see if, you know there's a subpopulation that's underserved by product, they could bring and create their own. And the moment you say, hey, like the bar is this high now you kill all of that. So it comes with issues, I think, but a higher bar than we have now in coming with the actual metrics, maybe individualized or at least the probability that you work for a particular person a little bit better. I think that's a better than.",
    "Now, the last couple of questions, I think you kind of answered some of them. How can the tech industry ensure that the innovations and health and wellness tools are fairly tested across diverse populations? We should be involved in that process. And I think you kind of mentioned that we should have more diverse people when we're actually developing the tools, testing across different demographics and things like that. And the last question, which I think you also kind of answered is what did this case reveal about the intersection between technology race and access to accurate health information in the digital age? How can companies rebuild or maintain trust with users who may feel excluded or misled by product performance? So I think one thing you mentioned was transparency. But maybe if you want, you could mention anything else you want to discussion[], yeah, I think it's transparency and actually incentivizing, filling the gaps where the products, their products fail. Again, if you know that measuring oxygen levels or heart rate doesn't work as well for our subgroup, then you have to like incentivize research there and even like potentially hire People from that population to work on the project to make sure that their, their needs are met.",
    "You know, and even, like, even if they have to spend the money towards these certain demographics, it might end up being rewarding to them in terms of, like, the profits they get back. Because, like, an example I can think of is, for example, closed captions on movies. It helps all, like, hearingly, hearing impaired people, but a lot of people that can hear also really like reading sometimes, like as a really good invention. So like, other things like that, even if it's meant for a certain demographic, it could end up helping a lot more people than they die.",
    "Yeah, 100%. I actually, I don't think I know anybody that English is not their first language and they don't use those captions. Like, even, like for myself, like, I never have a problem understanding what people are saying, but when I, when I'm watching a movie, I feel like I can't understand anything unless I put it on. So it's super helpful. Yeah.",
    "Yeah. And that's pretty much it for the last case. Now I'll just ask you some overall final reflecting questions and then we're pretty much done.",
    'Okay.',
    'Have you ever encountered or learned about implicit biases in clinical settings? It could be maybe in things that you worked on directly or maybe other things that you heard of, kind of like these cases. And how do you think it affects healthcare decisions?',
    "Yeah, I mean, I've seen, I've seen a lot of different papers. I mean, personally, I don't think I've seen it because I'm not in the business of putting models, you know, in clinical practice right now. But for sure, like, for. There's the, the case with Google and the, the flu prediction. They were trying to predict that and then they figured out that over time that things changed and then their model failed and it was a catastrophe. And then there's the case for facial recognition, like you said. That's a classic one. There's some on law as well that carried on the biases that we know often in the judicial system when trying to judge cases automatically. So all these things, you should take for example, the probability of saying somebody's guilty and then you read the case, and then there's description of the person, etc. And you carried on those vices. You're going to act the same way and not really fix the issue. So, yeah, I heard there's a lot of cases like, yeah.",
    'How can we ensure AI systems in healthcare reflect diversity and complexity of human identity, including transgender and non binary People.',
    'I think the first step. Go ahead. Yeah.',
    "I was gonna say just like, just remind you of the case that we talked about, the transgender man that was pregnant. That could be. It'd be nicer maybe if they had some sort of way to identify in the system, like you have, like, sex assigned at birth and then also gender that they conform to right now or like some other things that can help these AI systems to be able to actually identify that there is a possibility maybe they're pregnant or whatever the case is, rather than reinforcing stereotypes or biases that we have ourselves.",
    "Yeah, 100. I mean, I think the first part is, you know, having those separate variables to better capture the experience of the individual. Right. But I think in research now, we're doing a better job at capturing those variables, too. That's going to be very important because at the end of the day, there's going to be different sets of protective and risk factors for people that are transgender naturally, because their experiences are going to be very different. And then because of that, you're going to have factors and understand more about the biological differences that come with that as well. Can be very important. But I think we're just starting. And as long as we're capturing those variables and trying to understand it by doing my research and keeping. I don't know, the goal is to provide good care for everybody. Right. So I think as long as that's the goal, that we should try to figure it out.",
    'If you were designing an AI tool, how would you balance giving statistically accurate information, but also not downplaying, like those rare and deadly possibilities, for example, like the woman. Statistically, they have less chance of getting heart attack, but if they have an episode, it could very possibly be a case that could be life or death.',
    "Yeah, I know this. Yeah, I know this is a very, very difficult question because it involves also cost. It involves other things. And it might involve. In places like America, might involve individual cost and individual risk because it's not a free service. It's different in each place. And I mean, it goes without saying that high probability and high for the individual predicting of individual, not just probability, statistic for the. For the general population. But if you predict something as high probability and it's high risk, then that should be the first one. What goes after that? It's such a complicated and, you know, you can ask. It's going to be an economical question, going to be a risk management question, epidemiology question. I don't think any. Any individual person has a good answer. For that right now, unfortunately, that's my answer.",
    'And what kind of training or human oversight do you think should go into building and testing these AI? One thing I mentioned was ASMD injectors or like any sort of other AI healthcare kind of stuff to avoid embedded gender and racial biases.',
    "Yeah, I think the, the main one is the monitoring over time. That's the big one. Because in some cases it might be useful to have social demographic information going inside the model. In some cases it might not be useful, but in all cases you should have monitoring of those variables in terms of how they relate to the predictions. And if there's a discrepancy going on, then you need to understand why and see if there's an actual biological or sociological reason for it and rationale that makes sense and applies to clinical care, then you can preserve it. And if in fact doesn't make any sense and it shouldn't be there, then you should fix it. I think it's a case by case, but certainly the systems in place to monitor them over time.",
    "What do you think it feels like for a patient to be denied not because of medical necessity, but because of how an algorithm classifies you? How can future healthcare professionals be trained to recognize and challenge these biases and medical systems or software, and this is kind of like in particular to that the kidney example, where they weren't, it wasn't by need, it was literally because of their race that they were denied for five years. How can we kind of prevent these cases? How can we train future healthcare professionals to kind of challenge these cases when they see them?",
    "I mean, hopefully those cases in the, maybe not in the immediate future, the visible future won't happen. I think we should always try to strive for them not to happen anymore at all. But if, I think if there are those checks for the model performance, they should be very, very minimized. The question of whether a doctor should be trained to identify those is tricky because it's a person that has their own biases and they're going to make a judgment over something that has a different set of biases. And now the question is which bias? Right. In this case in particular, the doctor was correct, but in other cases they might be wrong. So I think it really lies in our ability to make better models and make sure that they're performing well over time.",
    "If you're on a team creating a new clinical algorithm, what are specific safeguards that you'd recommend to ensure that no specific demographic group is affected, even unintentionally?",
    "I mean, it's the same answer throughout. It's. I know it's kind of boring, keep repeating myself. But it literally that without that, there's nothing else. I mean, we can talk about data governance, we can talk about liability, we can talk about all these other things that keep us, you know, accountable, but at the end of the day is looking at the data that's going in, looking at the data that's going out and seeing, hey, is it consistent with what we would expect for this? Is there across these 10 variables, is there one of them that there's an update? And if so, look at it.",
    'Okay, so you say mostly through rigorous analysis of inputs and then outputs and what we expect the outputs to be and try to fine tune the model to become as good as we expect it to be.',
    'Yeah. And monitoring over time.',
    "Sounds good. And yeah, that was my last question. That's all my questions for you today. I really appreciate you taking the time to share your opinions on this topic of how clinical bias, clinical research affects healthcare practices. Do you have any questions for me or any closing remarks you would like to make before we finish?",
    'No, no. Thanks for that. It was very interesting. I learned a lot, too. Pretty cool.',
    "Yeah. Yeah. I was happy to have you. And that's all for today. Thank you again for your time and have a great rest of your day.",
    'Thank.'
  ],
  'scott-davidson': [
    'Okay, so. Hi Scott. Would you first like to start off by introducing yourself? Please tell us about your name, what degree you have, where you currently work, and what your research interests are.',
    "My name's Scott Davidson. I have PhD in molecular biology and bioinformatics and I currently work at SickKids on their cancer sequencing program.",
    "That's great. Thank you for the introduction. So today's topic will be about like, how biased clinical data affects healthcare practices. Our interview consists of six main cases where we talk about specific examples where biased clinical data led to AI systems making bad decisions and specifically stemmed from gender or racial biases. The first three cases we focus on will be focused on gender based biases, whereas the last three cases will focus on racial based biases. And overall the interview will be formatted in such a way that at the beginning of the six cases I'll give you very little information about the case and kind of like bias you in some sort of way. And then in that way you'll see like the point of the first few questions to give you limited information so that you understand kind of how the AI was thinking and how it made the bias predictions that it did. And afterwards I'll give you the full details of what the case and then we'll analyze kind of the impacts of the bias, the AI decision and how we can move forward to prevent such cases from ever occurring again for future patients. Finally, at the very end, once all six cases are done, I'll ask you just some overall urging questions about how biased clinical data affects marginalized communities and what you would suggest overall to include in similar systems if you were tasked with designing one. We'll start with the first case. I'll start with a question. Imagine you're a physician and you're trying to treat a patient with severe abdominal pain and you see no visible injuries. What diagnosis would immediately come to your mind? What kind of additional information would you ask for? And just let me know what you think. And if you don't have anything too specific, that's okay too.",
    "I mean, I don't have anything specific. If somebody said they had abdominal pain and they had no injuries, I'd be thinking in terms of what they had eaten and I don't know any illnesses or other things that is going on with them.",
    'Okay. And if I told you that person was a woman, would you have like some sort of idea maybe then as to what the problem might be?',
    "I guess then you'd have to think of other things, of, I'm not sure, pregnancy, things that are to do with the abdomen? Abdomen that's in a female patient.",
    'Yeah, sounds good. And if it was a man, would you have any sort of inkling towards anything there or would you just be kind of general information?',
    "I think no. If it's a male with that same thing, I'm not sure. I think I just.",
    "Okay, no worries. And then how about if they were transgender person, would, would that affect, like, you know, he had a bias. If it was a woman, it was more towards pregnancy. For a man, it could be any sort of thing. If they're transgender, would you have some sort of inkling towards what it might be or not?",
    'I think that would depend on whether or not as a doctor, I knew they were transgender and as to how that could affect it.',
    "Okay then, now summarize the case and kind of what happened and then we'll kind of talk about the implications of it. So the case was there was a 32 year old transgender man who arrived in an emergency room and it's severe abdominal pain. He informed the staff that he was transgender, but in the hospital's electronic health system record, enlisted him as a male, and as a result, the clinicians failed to consider pregnancy and the Mr. Attributed his symptoms to things like obesity. The patient was in fact pregnant and experiencing labor complications. Due to this delay in recognizing that he needed urgent care. The care was postponed, and tragically, the baby was stillborn. This case highlights how these rigid binary classifications and health records, whether they're male or female, combined with assumptions from the clinicians themselves, can lead to critical misdiagnosis. Standard pregnancy related alerts would have never been triggered in a system because he's treated as a male and there's no indication that he's transgender. So there's no way the system itself could tell. And then the clinicians themselves kind of had bad judgment as well, even though he informed them. And this kind of just highlights the dangers of this bias and the need for more inclusive healthcare systems that reflect the realities of transgender and non trans binary individuals in today's day and age. So I want to ask you, in your opinion, not knowing that the person was transgender and pregnant, what role do you think, like these biases played? How like the system design and other factors you could think of play into like the AI failures or the clinicians failure to identify pregnancy risk in this case?",
    "Not sure. It's kind of. I'm not sure it's a system error. It sounds like a human error because it's about communication. And as humans are not the best at communication, sometimes that's what I think it's like. It's perhaps not the system, it's the humans.",
    'And then if you were kind of responsible for suggesting changes, what would you suggest to changes to either electronic health records, triage systems, or training for these individuals and clinicians to prevent, like, similar outcomes for transgender patients happening in the future?',
    "I mean, I wonder if a simple change would be to be able to mark down both the sex and the gender of someone. Because if that was a contributing factor and that was if the records really are so much to go on compared to someone saying, I'm transgender and that being a communication thing, then, well, maybe the records need to change. But if the communication should be seen as important, then it's being able to talk to people and have the communication be of importance.",
    "Yeah, that makes sense. I agree. Yeah, it makes sense that it should be. It should be some sort of identifier to tell whether just not just like the sex assignment, but also their gender that they identify as. And then also maybe in, like, the training for some of these conditions, they could maybe be a bit more aware that transgender people are not exactly the same as, like, male and female distinction. Even though he was a transgender man, it could still be like the case that he was pregnant. And just being more aware of these different kind of gender identities could help with healthcare systems. Now we'll move on to the second case. I'll start off again with these diagnostic questions and I'll introduce you to the case. Let's imagine a 59 year old patient presents to you with sudden chest pain and nausea without knowing their gender. What would be your top differential diagnosis? What would your next steps be? Just anything you think of.",
    'So can you say the symptoms again? Can you just start?',
    "Yeah, it was a 59 year old patient, and it's having chest pain and.",
    "Nausea and chest pain and nausea. As a, as a non clinician, I hear chest pain and I think heart attack, but with nausea. I don't know if that's part of kind of symptoms for heart attack or not. So I'm not sure what that would add into that makes sense.",
    "That's the right train of thinking. Then my next question is, would you take this problem seriously right away? Because, you know, it's a person with chest pain that's 59 years old. Or would you think it's not that serious?",
    "I would think it's somebody at 59. You take that seriously straight away.",
    "Makes sense. And then now let's imagine that you learned that the patient is woman and you're told statistically women are less likely to have a heart attack. Would this influence your decision? Do you think it should influence your decision?",
    "Is that true? I don't think I knew that if that's true. I thought that men and women would be at the same risk of heart attack if that's the case. I don't know because like 59, still like a decent age and having chest pains. I think that should be taken seriously whether you're male or female.",
    "Okay. And then I was going to ask, this is kind of like the other side that they were a man. Would your sense of urgency change because you know that men have statistically a higher chance of having a heart attack. So would your sense of urgency change or would you kind of treat like whether they're a male or female kind of relatively seriously because they're 59 years old?",
    "I guess I'm not sure how a clinician would think about it because for me I wouldn't be thinking about in that terms because your stats would be off of a population. But you've got a person in front of you who's an n of 1. So I wouldn't think that you would, I don't think you would try to ascribe the population statistics to them to change the immediacy of action. I would just think that this is an older person with chest pains, they need to get checked out.",
    "Yeah, that makes sense. And I'll summarize the case now. So there was an AI powered symptom checker app that provided dangerously different recommendations for a man and woman that had the identical symptoms. They were both 59 year old smokers reporting sudden chest pain and nausea. The only difference between them was gender. And the male patient was warned of potential heart issues including unstable vagina or a heart attack and they were advised to seek BE emergency care. Whereas a female patient was told that her symptoms might be due to depression or anxiety with no urgent care recommended. So you can kind of see like I was trying to ask you the questions like the symptom thought that the man was really at risk because statistically men are more likely to have heart attacks, whereas the woman's case, they didn't take it as seriously because I guess physically that's not kind of machine learning models work. Statistically since it's low, it didn't take it as seriously. And this discrepancy kind of highlights how AI tools can replicate and amplify gentri biases potentially leading to delayed treatment or misdiagnosis. And in this case AI appeared to rely on the statistical trends that underrepresent heart attacks in women. And it overlooked the real and serious risk of cardiac events in female patients. And public concern grew after this example came to light as it revealed how gender stereotypes like women are anxious or men have heart attacks can kind of shape algorithmic decisions. Ultimately. Case highlights a critical example of designing healthcare systems. Healthcare AI systems that recognize and adjust for biases and lifestyle and conditions should be always considered regardless of patient gender statistics, especially when the symptoms are shared. We should always be considering all possibilities, I think. And so then now my question is to you. How should AI triage systems handle based statistical differences? Should it present all kind of serious possibilities regardless of the probabilities of each of the scenarios of happening, or should it always be able to tell if there's especially fatal scenario? This is possible, but maybe it's more likely that it's not that serious. Or how do you think it should try to organize these AI decisions?",
    "I think it depends on the use case. So if this was an AI that was in a hospital to aid doctors to kind of list these are the things that the AI thinks could be a possibility with this patient with these symptoms, then you would have, I would imagine you'd want that AI to produce a list of. These are the things I think, because that's probably what a doctor would be going, we'd be looking at. There's some terms you'd have the common things at the top of the head, the doctor that they see all the time in their specialty or in their clinic that they are used to seeing. And then further down their mind, they would think these are rare things that could still have these symptoms. It's unlikely, but it's still probably there in their mind. So you'd probably want an AI to list them all, all the possibilities that it would think could have those symptoms, regardless of gender or anything. And then in that scenario, you'd imagine the doctor with their expertise and the human eye on it could make a decision that's sensible in that moment. But whether or not that's the use case for that, I don't know. Because if it was like an online symptom chipper check, symptom checker for somebody at home, well, that's not a doctor, that's not a professional. Don't trust it.",
    "Yeah, that makes sense. That's a good point. Of what? The scope kind of depends on whether a doctor, like a patient's directly seeing it. But yeah, I like the idea that it should kind of list Everything. And then maybe like, give us the probabilities of like, this is most likely the case, this is least likely the case, but this is still possible. For example.",
    'Yeah.',
    'And my next question to you is, do you think AI was responsible in this case, given that it followed statistical trends, or should its priority have been over patient safety compared to data driven averages? Where can we draw a line between data and ethical care? And do you believe that it failed in its duty to provide safe recommendations?',
    'For this one example, was it an AI that was in a hospital with helping a doctor, or was it an, like an online resource that somebody at home could use? Like, what was it?',
    'I think it was just an app that anyone could use.',
    "If it's an app that anyone can go on to and use, then it's not something that should be kind of like taken as gospel. If you're having some sort of medical issue and you look at the symptoms that I think for good or bad, if it's saying that there's nothing too serious, maybe still get checked out. If you've got some kind of minor ailment and it's saying that you're going to die in 24 hours, maybe take that, maybe get that checked out. But don't be surprised if it's not the case because it's something that you find online in an app. But if it's something that's within a hospital that's supposed to be there to help a clinician to make a decision, then you'd really have to think about what's went into that model to build it so that the statistics of the likelihood of a heart attack is being given a role to play in that rather than just what's going on with this person standing in front of me with these symptoms.",
    "That makes sense. And then also, I guess we also have to consider, if it's like an app, a lot of people will take it to its word. For example, people go on TikTok and see something and immediately think it's true. So we also do have to be careful about the app. Like if someone looks at it and they think they're fine, they might not see a doctor at all. And then one week later they realize it was something serious too.",
    "Yeah, yeah. I think you need to have a person alongside the app. If you're just trusting the app, that's really not a good thing to do.",
    "Yeah. Okay. And we'll move on to the next case now. And again, I'll start off with the diagnostic questions. So let's say a young patient around 25 years old presents with a lump in their chest. What key factors will guide in your diagnosis and decision to recommend further testing?",
    "Just a 25 year old with a lump in their chest. For me, again, non clinician, I hear lump and I think cancer. So I would think that you'd have to go get that checked out. And so I'm not sure if that would be some sort of referral or what that would be. But you'd imagine you'd want to get that checked out.",
    'And would your approach kind of change if there were a male or female with like the lump in their chest?',
    "I don't think so. I mean, unless there's something, something about it is like, like how deep it is inside or is it some sort of surface skin thing? I'm not sure. It's like, could it be a pimple instead of cancer? I don't know.",
    'And breast cancer is rare in men. Less than 1% of all breast cancers occur in men. Should that rarity affect your decision to test for it? Would you like still test for breast cancer if you knew they were a man? How would you balance like the risk over testing versus missing a critical diagnosis? Just your thoughts on this?',
    "I'm sure you guess it depends on if there's some way to kind of get the idea that you should do the testing from an examination. I'm not sure what a clinician does when they find a lump to think that's benign or something needs to get checked out further, maybe dealt with. I'm not sure what that, what that experience of the doctor is that lets them know that. But for me as a non clinician, if it's there, it should get checked. It shouldn't really matter on your sex.",
    'So you would take the kind of the problem seriously and take similar precautions regardless of whether they were male or female.',
    'I think you have to.',
    "Okay, and now I'll summarize the case for you and then we'll kind of analyze it. So this is a case of Raymond Johnson, what's his name? He was a 26 year old man from South Carolina and he was denied Medicaid coverage, which is like his health insurance coverage for breast cancer treatment, solely because of his gender. Although he was diagnosed with breast cancer, he was ruled ineligible for a federal Medicaid program that covered patients screened through certain government programs and programs that only screen women for breast cancer. As a result, the insurance algorithm automatically excluded men and left Raymond with no coverage for his chemotherapy and the surgery that he originally needed. This case exemplifies how gender biases embedded in policy and algorithmic systems can have life threatening consequences. And the program failed to account for the fact that men can also develop breast cancer, effectively barring male patients from life saving care. Advocacy groups including the ACLU condemned the exclusion and discriminate as discriminatory and push for reform. The case kind of prompted broader conversations about the need for designing healthcare systems and policies that are inclusive of all genders and reflective of real patient populations concerns. And so I want to ask you, what role do you think societal assumptions like breast cancer is like a woman's disease played in the algorithm's decision to annihilate Riemann's coverage?",
    "I think that societal kind of thinking didn't play a part in the algorithm's decision. It played a part in the algorithm's design and writing. So whoever made it had those. The algorithm didn't do that. It was just following what it was supposed to do. What it was supposed to do was wrong. So yeah, I mean it might be the case that a very small percentage of men get breast cancer, but how many men are going to a doctor saying I've got a lump in my chest? Also a very, very small number. But of those that actually go with that complaint, maybe they should be taken seriously because that's not going to happen often. But yeah, that's, I mean the policy of what to do with that, I guess that's down to whoever creates the policy and the insurance providers as to whether or not they cover that. But yeah, it sounds like, sounds like some things were not done properly there.",
    "Yeah. And it's also kind of like unfair that even after like he was denied and like brought up the issue to customer service people, they still said that? No, because, just because like, you know, it's a woman's disease or like it was considered null by the algorithm so they can't provide coverage. So it would have been nice if they still try to acknowledge it even after he brought it out.",
    "Yeah. Just because again, on this side the algorithm says no. It's like just because the computer says no, a person should still be there to think about. Why is it saying no for that?",
    "Yeah, I guess some insurance wants them to save their money. So maybe the study algorithm says no, we're not providing it to you.",
    'Yeah, yeah. This saves our company money.',
    "Yeah. What changes would you suggest to like these healthcare algorithms or policy like algorithms to prevent gender based denial and of coverage in similar cases? How can we ensure equal treatment for all regardless of like, you know, these stereotypes that this might be like only a woman's disease or these other kind of biases.",
    "If an algorithm really is saying as to whether or not something is approved or unapproved, like without human interaction, then the gender shouldn't be taken consideration of it because the clinician that's already diagnosed that has taken that into account.",
    'Yeah.',
    "So why would it be anybody else's need to do it?",
    "So you're saying like, if you have proof of like you did this treatment, you should just be able to get the coverage. You shouldn't need to be worried about whether your gender is or whether you fit the typical demographic for that disease or whatever the case is?",
    "I think so, yeah. If it's something that's health related, if it's life threatening, but somehow you don't fit the usual example of that, why should you be refusing it? Because you don't fit the normal example of that condition. It doesn't make sense.",
    "Yeah, I like that point. And do you think healthcare algorithms should prioritize statistical likelihoods and trends or should they be designed to account for like these rare but serious conditions, especially when there's license stakes? How can we like try to strike this balance, these balances within the system?",
    "I'm not sure. If you do have to take account of statistics and likelihoods, then I think you need something on top of the algorithm, like a clinician, an expert in the field to be able to adjudicate or to nullify what an algorithm says. Or there has to be some sort of way for the rare and edge cases to be allowed through and to be able to be visible.",
    'Yes. Maybe some sort of like auditing or like having someone regularly check on the algorithm would be nice to have.',
    'Yeah. Real time auditing.',
    "Yeah. Okay, so now we'll move on to case four. So those first three cases were the gender based examples. Now we're going to focus on racial based biases. And for the first question that I ask you is imagine you're a healthcare provider and you're evaluating a patient with kidney disease or transplant eligibility. What factors would you kind of consider in assessing disease severity and readiness for transplant? Just like general ideas, you don't have to be like super correct or anything, not an expert.",
    "So eligibility for transplant, I would imagine. I think my brain's going somewhere else. Could for, because to get a transplant you have to like match donors things. So you'd have to be able to have some kind of blood test, immuno test to see that you would match whatever you're getting, whatever organ you're guessing and I guess whoever has the best match and the best chance of maintaining the organ would likely be higher up the list for it. But. So I'm not sure. I think I maybe took that answer a different direction. What your question was, can you ask the question again?",
    "I was asking whether when you're evaluating a patient for kidney disease for transplant eligibility, what clinical factors would you typically consider when you're trying to assess the disease severity, whether they're ready for transplant, maybe like whether you should be taking patient one over patient two, like based on severity. Just any general ideas. Like you said, it was amazing.",
    "Yeah. So I guess in that case you'd be looking at if there's more than one patient, then you'd want to know who is in most dire need. So who's in the most life threatening position at that moment that needs that organ. I guess while at the same time perhaps what I said originally is still the case. Like if someone might really need an organ badly to live, but if they, if their immuno markers don't match, then you wouldn't be giving it to that person anyway. So I guess there's, there's quite a lot of things you'd have to think about to make that decision.",
    "Yeah. And have you ever heard of race based modifiers in kidney function scoring such as egfr? What do you think the tension was behind them, behind including race in those calculations? So I'll just give you, I'll give you a brief summary. So like the system that they used in the case, I won't go into the case entirely yet, but they use a transplant system to be able to kind of tell whether like the list of patients, which one kind of needs the transplant the most. And there was this like race based modifier. So specifically black patients, their kidney scores ended up seeming better than they actually were with this modifier. I think it was just like some sort of way to like since different races will have different kidneys, maybe some of the factors in like the kidney assessment scoring tends to give certain races higher scores. So then the score is kind of modified to make it either higher or lower depending on the racial demographic. And I think the idea was just to modify for those to kind of make it even. But then what ended up happening was it made it a lot worse because the people like black patients that really needed care looked healthy and normal. So then other patients were prioritized over people of these certain races that were kind of deprioritized because they looked healthier than they actually were. So then do you think like, do you think like the idea behind this makes sense? Do you think that the tension behind this makes sense?",
    "Like so I'm trying to wrap my head around that. So would there have been like some kind of lab test to say that in certain genes or so you had certain markers that are perhaps more prevalent in certain ancestries so that they wouldn't have to have their scores adjusted because that would influence how healthy they would seem based on a lab test? Or was it simply a score based on your race to alter your likelihood of getting an organ? Because that second one sounds awful. Like if it's based on data, it's like this is how you, this is how we know more about your situation. That like physiologically, molecularly, this is how we help you better. Fine. But if you're trying to adjust the score just based on what somebody's ancestry is, which they might not fully know of, that could, that's really dangerous. That's not something you would want to, to do.",
    "Yeah, yeah, I agree. Yeah. I don't understand 100 the idea behind this like modifier. But yeah, now I'll summarize the main case and we'll kind of talk about it. So the person was Anthony Rattle Randall. He was a black man from Los Angeles who was on dialysis waiting for kidney transplant for over five years. What he didn't know was that an algorithm from the transplant system incorporated a race based modifier that made black patients kidney scores seem way better than they actually were. This modifier caused Randall's kidneys disease to seem less severe than it truly was, leading to his placement on the national transplant waiting list to be significantly delayed. And like I said, for five years. In mid-2023 he filed a case against the hospital. The hospital was called Cedars Sinai Medical center and the United Network for Organ Sharing. And he also did against the United Network for Organ Sharing. And he alleged that he was being unfairly deprived of a fair chance of getting a transplant because of a racially biased formula. Apparently it was no secret that the algorithm had a bias. The board of the transplant system understood that the modifier resulting in black patients illnesses being severely underestimated. And by early 2023 all hospitals were directed to stop the the usage of the race based adjustment and black patients waiting times were changed to be reflected after the postponement.",
    'So people knew that?',
    "I'm not sure people, but maybe, maybe some people in the research area kind of knew. Maybe it was a complete general knowledge. But I guess like as they as like black patients waiting times go out longer and longer. Maybe there's like a hunch and like an idea that something was like going on behind the scenes there.",
    "That's.",
    "But I guess without like 100 proof you can't always say. But I guess his case like really helped bring it, bring it up to their intention and like for them to be held accountable and take it seriously.",
    "But, but they. So there was a modifier which gave a score that made things look better than they were. That. Yeah, that is, it's hard to think how somebody could not have known that that was going on. Like that's, that's, that's, to me I would think that's probably intentional and that's, that's just pure racism. I guess. That's not nice.",
    "Yeah. I've also seen like other cases in general where like the system is known to have a bias but it'll still be implemented. Like an example was there's racial facial recognition technology that was used in some policing systems to be able to identify people. And it was like known like quite often that for black people it would get it wrong because like black people can't get the features completely correct because the lighting and things like that make it a lot harder to identify a black person. So a lot of times they'll identify a black person wrong and they'll get wrongly incarcerated. And like these facial recognition technologies have been used for a long time like the police, the police policing system is aware but they won't stop using the system until to get enough backlash because it just makes their process easier. Like identify this person. Was this person. Okay. Send them to jail. It just makes like things a lot easier. And then I guess in this case maybe the person was maliciously motivated or people were too lazy to adopt like their own kind of systems rather than using this easy to identify patients. Like I guess it'd be like whole like multitude of reasons.",
    'Yeah.',
    "Kind of crazy. Yeah. Yeah. And so his case highlights how the goal clinical algorithm was good but the exception was not due to the insertion of these race based modifiers which caused black patients to have like receive. Patients receive the transplant way later than they should have. And now we're going to analyze it. So knowing now that this modifier made black patients appear healthier than they were delaying transplant legibility, how do you think this affected patients like Anthony Randall?",
    "Well, I would imagine that some people on the transplant list perhaps never even received a transplant and that might have been fatal. It probably meant the quality of life. Whoever was on the Transplant list before they got a transplant was worsened, perhaps even impacting mental health. For them to, like, be part of that process. That's just awful.",
    'Yeah. And do you think hospitals or national organizations should be held accountable when algorithms are known to be biased but continue to use? What obligations do you think they have once they become aware of these biases and these unfair injustices?',
    "If nobody knew there was bias there and then it was found out and people were told that it needs to be altered immediately, like it needs to be removed, you can't have that happening. I mean, if you're introducing a bias that's based on genetics and moleculars and physiology stuff, stuff that's like, this will give you the best chance we might have to give you this medication. So this medication, because we know this about your biology, fine. That's a bias based on what we know. But if it's a bias based on your race for no good reason, that's ridiculous. Why would that. That should not be kept in any system once it's known to exist.",
    "And. And in Randall's case, the board eventually mandated the removal of the risk adjustments, but it came years after the issue was known. Do you think the delay was acceptable? What do you think could have been done differently to address these concerns, like, a lot faster?",
    "I think once it was known, whoever was in charge of the system should have been responsible for removing it or eliminating the bias. It's on that person's head.",
    "How do you think health systems can design or help design critical algorithms that avoid reinforcing historical or system inequities, especially those tied to race? Maybe, like, should they, like, when they. Whenever they get new algorithms, should they screen them to check whether there's these special modifiers or like, if this. Should they be testing them before even use that, like, across different demographics and seeing that, like, you know, different races are not impacted drastically compared to others or. What do you think in general?",
    "I think they can definitely be tested with historical data, historical examples, to find out what would happen in the algorithm. But imagine that even if you think that an algorithm is okay and you put it into practice, you'd want to have a prospect of auditing of it as it goes to make sure that it's still functioning properly. I mean, we might just be talking about some kind of algorithm in the system. You're just like a score bit getting added. So it's not really AI, but as AI kind of like comes into places, it's a bit more of a black box. You need to check the outcomes on Prospective cases as they're coming in to make sure that it's actually working as intended.",
    "And what steps, if any, do you think should be taken to make things right for patients who are harmed by biased algorithms like, such as adjusting their wait times, issuing apologies, offering some sort of compensation? Do you think there's something they could do to kind of like, repair these relationships between people that were wrongfully waiting for like, multiple years just to get a treatment because they were unfairly prosecuted by the. The system that they were using?",
    "I'm not sure. I'm not a legal expert. I'm not sure how you'd make somebody right after that. I guess the usual course of action, any kind of legal thing, is like a financial. Some sort of financial settlement or winning to see that justice has been done. But if you're. If something's like life threatening, then I'm guessing that sometimes money isn't the answer. But at the same time, if you're on a waiting list for kidneys and this happens, I'm not sure you can just simply go, okay, to make this right, this person goes to the top of the list. Because again, you've got all the other factors to consider of, like, who the organ is going to best match, things like that. So, like, I think in this case, it's difficult to come up with something that's actually going to be of any kind of justice to right the injustice.",
    "Yeah. Okay. And we'll move on to the next case. Now, this is the second last one. We're almost done. So imagine here treating a patient with chronic lung disease during a respiratory pandemic. If a pulse oximeter shows normal oxygen saturation, but the patient appears visibly distressed, what would you do next?",
    "I would check to see if the equipment's working properly. If the machine is saying that the oxygen levels are okay, but you can see with your eyes that something's not right, then check the machine, make sure that your equipment's actually working properly.",
    "Yeah, that kind of ties into my next question. I was going to say, would you completely rely on tools like pulse oximeters and decision making? Do you think you would ever question or double check the accuracy of a medical device if you feel like it doesn't match kind of what the patient looks like they're going through?",
    'I would hope so. Yeah.',
    "That's the right answer. Sometimes, you know, people won't do it. Like, I guess in the example where I talked about, like, the transgender man, I guess they didn't double guess the system there. Like, they just saw they were A man. They didn't try double guessing that they were pregnant, but in this case the. I'll go over the summary now. But the person did double guess equipment and it did really help. So it was during the pandemic. A race related bias in medical technology confronted Dr. Noah Albertia, a family physician and a CEO of Roots Community Health center based in Oakland in late 2020. One of her patients was an elderly African American gentleman who suffered from chronic lung illness. One of the checks done previously, the pulse oximeter check, revealed that his oxygen saturation levels were high, even though the device showed like it was relatively normal oxygen level. Doctor. The doctor's clinical instinct was to check. Sorry. The doctor's clinical instinct indicated that the patient was much more distressed than the system actually showed. And so she conducted a arterial blood gas test which confirmed her worst fears. The oxygen content and the patient's level was too low and he needed oxygen. Sometime later, she came across an article in the New England Journal of Medicine that confirmed her hunch. The exometers were unable to register low levels of oxygen in dark skinned patients as compared to white patients. She and her colleagues were outraged by a device that was supposed to help the patients, but it was grossly inaccurate for the black population. Finally, her clinic participated in a class action lawsuit against the the manufacturers and sellers of false songwriters for more detail warnings and up to date devices. She didn't stand idle while demanding the FDA to take the pulse exometer discrimination towards race very seriously. This is kind of like a case where the doctor really went against the equipment decision when she realized that the patient itself didn't look like they were all right.",
    'Yeah.',
    "So in this case, based off the doctor's clinical judgment and it overruled the device's reading. What do you think this says about the limitation about relying too much, heavily on technology without understanding its biases and like, you know, it's kind of like mislabeled. They didn't tell or keep up to date the technology to inform the doctors about these biases. So what kind of due diligence do you think doctors have to make sure that they question the equipment they use if it seems like it's wrong?",
    "Yeah, yeah. I think if it's something that you're using day in, day out, then you should have not just the training on how to use the machine, but how it actually works. And that's probably what this doctor had. Sounds like she did a great job because you need to know the ins and outs of the tools that you use. And I guess it's as you get further away from that, it's like, how much do you need to know? So you use a tool and you understand how to use it, but you understand how it works. Like what's the principles behind it? But you don't need to go down to do you understand the principles of the CPU and the resistors on a board inside the machine? You don't need to know that level, but you need to know a certain level about the machine and how it works and what you're doing. So it's like anything that you're using with software, whether it's medical devices, you're going to need not just to know how to do it, but how it actually works to a certain level to be, I guess, optimal in your, in your job.",
    "Yeah, that's a good point. Now we'll go to the next question. So pulse exometers were found to overestimate oxygen levels in patients with darker skin. Why do you think this design law persisted for so long despite the risk it posed to patients of color?",
    "So from the name, I don't know what it is, but is that the thing that you put on your finger in the hospital and it kind of measures your pulse and your oxygen from just something being on your finger? Because if that's the thing, then.",
    'I do think so.',
    'Okay, so I wonder like how it works because maybe depending, like how it works is where the bias comes from.',
    "Yeah, probably has something to do with the sensors since maybe you can't like accurately identify pulse through darker skin complexions because the sensor can maybe go through skin properly or whatever the case is.",
    "But yeah, I'm wondering if it works by an electrical current or is it something where it tries to, is it trying to like shine some sort of light through you to be able to pick things up? I'm not sure how it would work, but I would imagine that almost any way you do that there's going to be people that have different impedance to it working properly.",
    'Do you think the FDA and manufacturers have done enough to address this issue? What more should regulatory bodies be doing to ensure devices are accurate across diverse populations? Should they maybe be like performing regular daily checks or like is some way they should be mandating these technologies to make sure that they are up to date and they are meeting the needs and not like being ingestful to certain demographics?',
    "If a machine like that needs FDA approval or some sort of approval to be used, then you should be able to show that it works and only works in one sub population of People, you should be able to show that it works for everyone that a hospital is going to have coming into their care. Because if someone isn't so well versed in the machine, who isn't careful, who just takes the machine at its word, and it doesn't function on all demographics, people, then that's. That could be an issue. Definitely.",
    'And for communities that have been historically underserved or harmed by biased tools, how can medical systems begin to rebuild trust and ensure safer and more equitable care to these people?',
    "You'd have to show that what you're producing actually works. You'd have to show and have some sort of transparency in your quality assurance, I would imagine.",
    'Okay, sounds good. And then now the last case. If you purchased a health monitoring device like a smartwatch and be like an Apple Watch, well, at level of accuracy and reliability would you expect, especially when it comes to tracking critical health metrics like blood oxygen levels and, you know, heart rates and other things like that?',
    "I have a smartwatch and I would not trust it. I don't think anything that you're gonna be building into personal device is gonna have any reliability. I would not trust it.",
    'So you have no trust at all in your store Watch.',
    'It can tell the time.',
    "Yeah. And how would you feel if you discovered that your device didn't work as accurately for people of your skin tone or demographic? Would you feel raised or how would you feel?",
    "The fact I don't trust myself, I wouldn't be too worried if it doesn't work for other people either.",
    "Yeah. So I'll summarize this case and then we'll kind of talk about like the effect they had. So this was a person named Alex Morales. He was a New York resident and he brought attention to a case of possible racial bias in consumer health device Apple watch. Morales, who has a darker skin complexity, bought an Apple Watch with the expectation that its blood oxygen sensor would accurately log his oxygen levels for fitness and health purposes. To his surprise, he later found out that the device's oximeter may not work well with people from his demographic. In late 2022, Morales initiated a class action lawsuit against Apple for allegedly containing a blood oxygen app that was racially discriminatory and did not function as promised for non white customers. Supported in part by complaints of other studies claiming that more advanced pulse oximeter devices are massively less useful in people who of darker skin. Morales asserted that Apple owed a public explanation. And I think this technology is kind of similar to the bias that the last one we had about the pulse Examiners. I think it's related to that where like darker complexities, it's a lot harder to. For the system to figure out the oxygen levels.",
    'Yeah.',
    "And after all, paying smartphone users assume that the devices would be equal for all users, which was not the case. While the judge dismissed the case in 2023, it did start an important discussion r[] products. And this case showed the world that there are in fact direct biases in medical grade equipment. This shows us that Alex Morales and the rest of the community are still subjugated to discrimination based on race, even if the technology they chose was technology they chose to use. And moreover demonstrates that a keen eye for responsibility in tech industry is needed in these situations. Do you think companies like Apple have an obligation to disclose, like disclose these limitations in their health sensors to people, especially if those limitations affect certain skin tones or groups of certain people. What do you think? I don't know.",
    "I think it depends on, on how they marked it. Because when I got my smartwatch I knew that it had like a heart rate monitoring feature in it, but I didn't assume it would be any good. I didn't actually read the documentation that came with the watch to find out is this medical grade? I assumed it wouldn't be. I assumed it's not something to trust. So if Apple is saying that their watches are medical grade equipment, then yeah, that's a problem. They need to live up to that. But if they're not medical grade equipment then I don't think anybody should be relying on them as such. I guess it really depends on how they're marketed. Again for me, I don't think that these watches were marketed. If it was, then it was a part of me buying it. But I, even if it was marketed as a medical grade, I wouldn't have trusted it to be.",
    "So yeah, I just did a quick search. Now it's not considered medical grade device, but it's pretty funny. Like the first link was the Apple watch. Healthcare says Apple Watch can support patients and clinicians across many different aspects of health including health, including heart health, mobility, activity, medications and more. So I guess it does like say it will help you health wise and the clinicians to maybe be able to see that data and help you maybe better.",
    'Yeah.',
    "Why do you think the skin tone bias and exometers and wearable health devices hasn't received the widespread attention until recently? What might be some of the barriers to addressing it?",
    "Probably people thinking the same as me is that just it's a smartwatch, it's not Supposed to. So probably people aren't looking at it. But it makes me wonder because I know that my watch has some sort of. I don't know if it's a laser or light or something on the back of it, which is part of the heart rate monitor. But if the tone of your skin can affect that, then maybe there needs to be more work than that. But at the same time, I mean, if you're simply a hairier person, maybe you're not going to get such a good reading either. It's like, do you need to shave your hair? Some of these things are going to work for some people, not for others. I mean, it's a smartwatch. I don't think it's that much of a issue.",
    'Yeah. So in this case, the court dismissed Morales case. But do you think tech companies in general should be held accountable in other ways what kind of responsibility they should carry when their products are shown to underperform for marginalized communities?',
    "It depends on if it's underperforming based on something, based on a level it is supposed to reach. If it's supposed to meet a standard and it doesn't meet that standard, yes, they absolutely have to be held responsible for that. If they're providing something on their phone, their phone, their watch, whatever, that actually has no standard, but they're saying like, hey, we've got this cool thing on here that kind of does this, then there's no, there can't really be a complaint against it when they've made something that doesn't have any standard to work against. So it depends on whether or not that is there. Like if they're seeing if it's medically read, if they're saying this will actually monitor your heart rate or your oxygen or whatever, then they should be held accountable for it not working. But if they're not saying that, then it's just an us for assuming that it does something when it maybe doesn't.",
    "Yeah, that kind of related to the next question, which I guess kind of answer. I was going to say, do you think consumer tech devices like that have help features like smartwatches should be subject to the same regulatory scrutiny as medical devices? And I guess since you said that it kind of depends on what they're.",
    "Advertising, as if they're marketed that way, then yes. But if they're not, if it's just a smartwatch that has something that might get your heart rate right and fine, because I've looked at my heart rate on my watch and sometimes it looks like the normal Curves that you would expect. And sometimes it looks like a simple up and down, sometimes I'm flatlined. So, you know, yet I'm still here.",
    'Yeah. How do you think that the tech industry can ensure that innovations and health and wellness tools are fairly tested across diverse populations? Who do you think should be involved in the process? Like a government or some sort of regulatory body or.',
    "So if they're, if they're trying to have some sort of sub process and a product that meets a standard, then you need to test it in multiple demographics. You need to understand how the different populations in your society are actually going to play out with that piece of equipment. I think, I think companies should be able to do that by themselves. And if it needs to be regulated, then there needs to be somebody that can make sure that they've done it.",
    'And what do you think this case reveals about like the intersection between technology, race, access to accurate health care information in our digital age? How do you think companies can rebuild or maintain trust with users who may feel misled by biased product performance?',
    "I don't know. In this particular example, if it's a smartwatch, I don't think it tells us. I think it tells us more about what somebody expects from something that they maybe shouldn't be expecting from it.",
    "Yeah, it sounds good. So that was the last case. Now I'll just ask you a couple of final overarching questions based off all these cases and. Yeah, so just be kind of reflecting questions over all the cases now. So first I'll ask you, have you ever encountered or learned about implicit biases in clinical settings? How do you think this affects healthcare decisions? Maybe like biases that you're directly familiar with, the work you do, or maybe just in general examples or anything that you know of, Just anything you want to talk about?",
    "I can't think of anything off the top of my head. No, I can't. Thank you.",
    'No worries. Move on to the next question then. How can we ensure that AI systems and healthcare reflect the diversity, complexity of human identity, including transgender and non binary people?',
    "I mean, with AI, all depends on the model you have, depends on the training. So I think when people are building this, they need to really consider the data that they're putting in and as to how, how the software is going to use that to optimize and try to think ahead as to whether or not any features that you're trying to put in are actually needed.",
    'If you were designing an AI tool, how would you give balance to statistically accurate information, but not downplaying rare but deadly possibilities.',
    "I don't know. I really don't know how I would do that. My first thought is you might actually want more than one AI to want to look at the stats, want to look at the possibility of something being rare and for those to come together. I mean, I don't think if an AI is supposed to be a learning tool, if it is any way similar to a human mind, then you need to be able to consider multiple things in separate ways, which I'm not sure a simple AI does.",
    'Okay, yeah, I like that idea of like having two different perspectives of AIs and then bring them together. What kind of training or human oversight do you think should go into building and testing AI symptom checkers or just like any other AI systems to avoid embedding gender or racial biase?',
    "I think you'd have to have this depend on the data analyst of the data that comes in, the architect of the system, the quality assurance people. It's essentially the whole team dynamic that has to make sure that something works as intended.",
    'What do you think it feels like for a patient to be denied care not because of medical necessity, but because of how an algorithm classifies you? How can future healthcare professionals be trained to recognize and challenge biases like these in medical systems or software?',
    "Well, I still think that medical care should be based on what a physician or a clinician is saying, not what algorithm is saying. I don't think we're at a point where we can trust algorithms. So I think for me, you'd want to have a clinician looking at the output of any algorithm to say, do I agree with this? Am I going to put my name on this?",
    "And if you run a team creating new clinical soft, a new clinical algorithm, what safeguards would you recommend to ensure that it doesn't harm any demographic group, even unintentionally?",
    "I'm not sure. I mean, you'd either want to have your system blind to that information or you'd want to have that information up front and visible at every point so that it's. So that it's known. I'm not sure which way would be better.",
    "That's good though, like those two kind of perspectives. And that's pretty much all my questions for you for today. I really appreciate and thank you for taking the time to share your opinions on this topic of bias clinical research data affecting healthcare practices. Do you have any questions for me or like any closing remarks or anything you would like to say before we finish?",
    'No.',
    "Yeah, sounds great. That's all for today, and thank you again for your time and have a great rest of your day.",
    'Thank you.'
  ]
}
